{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5240e458-2ca9-4cfb-b035-53749246a3b0",
   "metadata": {},
   "source": [
    "Towards 3D Deep Learning for neuropsychiatry: predicting Autism diagnosis using an interpretable Deep Learning pipeline applied to minimally processed structural MRI data, Melanie Garcia, Clare Kelly. medRxiv 2022.10.18.22281196; doi: https://doi.org/10.1101/2022.10.18.22281196\n",
    "\n",
    "Github: https://github.com/garciaml/Autism-3D-CNN-brain-sMRI?tab=readme-ov-file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a580654c-8995-41ac-8a45-b1d3be5d3827",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Activate Virtual Environment and Install Requirements\n",
    "#!python3 -m venv ../pretrainedresnet2\n",
    "#!source ../pretrainedresnet2/bin/activate\n",
    "#!python3 -m ipykernel install --user --name=pretrainedresnet2 --display-name \"Python (pretrainedresnet2)\"\n",
    "#Switch to notebook/virtual environment kernel\n",
    "#!pip install -r requirements.txt # install requirements text in new environment\n",
    "#!pip install \"torchio>=0.19.0\"\n",
    "#!pip install monai\n",
    "#!pip install tensorboard\n",
    "#!pip install torchsummary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b00e43c5-baa8-41b5-b817-35dcbfb76bc5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import shutil\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "adca8835-3e91-422a-9ea8-2ca9eddff78a",
   "metadata": {},
   "source": [
    "# Test Paper Reproducibility"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "bf232768-d708-4975-940a-21a4a1c34239",
   "metadata": {},
   "outputs": [],
   "source": [
    "def copy_with_structure(source_folder, destination_folder):\n",
    "    \"\"\"Copies the contents of source_folder to destination_folder, maintaining the directory structure.\"\"\"\n",
    "\n",
    "    for item in os.listdir(source_folder):\n",
    "        source_path = os.path.join(source_folder, item)\n",
    "        destination_path = os.path.join(destination_folder, item)\n",
    "\n",
    "        if os.path.isfile(source_path):\n",
    "            shutil.copy2(source_path, destination_path)  # copy file with metadata\n",
    "        elif os.path.isdir(source_path):\n",
    "             shutil.copytree(source_path, destination_path, dirs_exist_ok=True) # copy directory and its contents\n",
    "        else:\n",
    "            print(f\"Skipping {source_path}, not a file or directory.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "015997f8-e4a4-4328-aa85-d093a64e9e29",
   "metadata": {},
   "outputs": [],
   "source": [
    "def count_folders(path):\n",
    "    folder_count = 0\n",
    "    for item in os.listdir(path):\n",
    "        if os.path.isdir(os.path.join(path, item)):\n",
    "            folder_count += 1\n",
    "    return folder_count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "b5e980f0-c81c-48bf-aff6-ec53256386f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def count_files(directory_path):\n",
    "    \"\"\"Counts the number of files in a directory.\n",
    "\n",
    "    Args:\n",
    "        directory_path: The path to the directory.\n",
    "\n",
    "    Returns:\n",
    "        The number of files in the directory, or -1 if the directory does not exist.\n",
    "    \"\"\"\n",
    "    if not os.path.exists(directory_path):\n",
    "        return -1\n",
    "    \n",
    "    file_count = 0\n",
    "    for item in os.listdir(directory_path):\n",
    "        item_path = os.path.join(directory_path, item)\n",
    "        if os.path.isfile(item_path):\n",
    "            file_count += 1\n",
    "    return file_count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "9bf20dca-f6b6-4009-8da2-cea37c6de12c",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6\n",
      "3\n",
      "2112\n",
      "2112\n"
     ]
    }
   ],
   "source": [
    "# Model Training - check if can reporduce results from paper (train to 42 epochs with similar split)\n",
    "\n",
    "#Copy ABIDE1 and ABIDE2 images to ABIDE_COMBINED folder in JustBrain_Data folder\n",
    "source_folder1 = \"JustBrain_Data/ABIDE1\"\n",
    "source_folder2 = \"JustBrain_Data/ABIDE2\"\n",
    "destination_folder = \"JustBrain_Data/ABIDE_COMBINED\"\n",
    "\n",
    "copy_with_structure(source_folder1, destination_folder)\n",
    "copy_with_structure(source_folder2, destination_folder)\n",
    "\n",
    "\n",
    "#Combine ABIDEI and ABIDE2 tsv files\n",
    "participants1_tsv = pd.read_csv('JustBrain_Data/ABIDE1/participants.tsv', sep=\"\\t\", dtype=str)\n",
    "participants2_tsv = pd.read_csv('JustBrain_Data/ABIDE2/participants.tsv', sep=\"\\t\", dtype=str)\n",
    "participants_ABIDE = pd.concat([participants1_tsv,participants2_tsv])\n",
    "participants_ABIDE.to_csv('JustBrain_Data/ABIDE_COMBINED/participants.tsv', sep='\\t', index=False, header=True)\n",
    "\n",
    "#Check\n",
    "print(count_folders(source_folder1) + count_folders(source_folder2))\n",
    "print(count_folders(destination_folder))\n",
    "print(participants_ABIDE.shape[0]) # - looks good!\n",
    "\n",
    "\n",
    "#Copy ABIDE1 and ABIDE2 images to ABIDE_COMBINED folder in Preprocessed_Data folder\n",
    "source_folder1 = \"Preprocessed_Data/ABIDE1\"\n",
    "source_folder2 = \"Preprocessed_Data/ABIDE2\"\n",
    "destination_folder = \"Preprocessed_Data/ABIDE_COMBINED\"\n",
    "\n",
    "copy_with_structure(source_folder1, destination_folder)\n",
    "copy_with_structure(source_folder2, destination_folder)\n",
    "\n",
    "#Check\n",
    "test_folder = count_files(\"Preprocessed_Data/ABIDE_COMBINED/test\")\n",
    "train_folder = count_files(\"Preprocessed_Data/ABIDE_COMBINED/train\")\n",
    "val_folder = count_files(\"Preprocessed_Data/ABIDE_COMBINED/val\")\n",
    "print(test_folder + train_folder + val_folder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "da2d921c-63b9-4ebb-8d8b-9168797104b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Start training from scratch\n",
    "# batch size = 4 (6 too large)\n",
    "# learning rate = 0.0001\n",
    "# seed = 42"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "597d68a7-58a1-4ec3-923b-39beeadcee5c",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/sfs/gpfs/tardis/home/ejh2wy/CHMCorr_Autism_Research/../Autism-3D-CNN-brain-sMRI/train_medicalnet.py:29: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
      "  scaler = GradScaler()\n",
      "/sfs/gpfs/tardis/home/ejh2wy/Autism-3D-CNN-brain-sMRI/resnet2.py:174: FutureWarning: `nn.init.kaiming_normal` is now deprecated in favor of `nn.init.kaiming_normal_`.\n",
      "  m.weight = nn.init.kaiming_normal(m.weight, mode='fan_out')\n",
      "Loading pretrained model weights selectively (backbone)\n",
      "/sfs/gpfs/tardis/home/ejh2wy/CHMCorr_Autism_Research/../Autism-3D-CNN-brain-sMRI/train_medicalnet.py:119: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  pretrained_weights = torch.load(pretrain_path, map_location=device)\n",
      "----------\n",
      "epoch 1\n",
      "/sfs/gpfs/tardis/home/ejh2wy/CHMCorr_Autism_Research/../Autism-3D-CNN-brain-sMRI/train_medicalnet.py:159: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast():\n",
      "1/424, train_loss: 0.5798\n",
      "2/424, train_loss: 0.5303\n",
      "3/424, train_loss: 1.1646\n",
      "4/424, train_loss: 1.7541\n",
      "5/424, train_loss: 0.9332\n",
      "6/424, train_loss: 0.6520\n",
      "7/424, train_loss: 2.0708\n",
      "8/424, train_loss: 1.2679\n",
      "9/424, train_loss: 0.7920\n",
      "10/424, train_loss: 0.6910\n",
      "11/424, train_loss: 0.8742\n",
      "12/424, train_loss: 0.8633\n",
      "13/424, train_loss: 0.6171\n",
      "14/424, train_loss: 0.7828\n",
      "15/424, train_loss: 0.5452\n",
      "16/424, train_loss: 0.6900\n",
      "17/424, train_loss: 1.2921\n",
      "18/424, train_loss: 0.8975\n",
      "19/424, train_loss: 1.0314\n",
      "20/424, train_loss: 0.7176\n",
      "21/424, train_loss: 0.8488\n",
      "22/424, train_loss: 1.1240\n",
      "23/424, train_loss: 0.2069\n",
      "24/424, train_loss: 0.8436\n",
      "25/424, train_loss: 1.1680\n",
      "26/424, train_loss: 0.9736\n",
      "27/424, train_loss: 0.7531\n",
      "28/424, train_loss: 0.7423\n",
      "29/424, train_loss: 0.9518\n",
      "30/424, train_loss: 0.5942\n",
      "31/424, train_loss: 0.7916\n",
      "32/424, train_loss: 0.7597\n",
      "33/424, train_loss: 0.6070\n",
      "34/424, train_loss: 0.6987\n",
      "35/424, train_loss: 0.7142\n",
      "36/424, train_loss: 0.7399\n",
      "37/424, train_loss: 0.7051\n",
      "38/424, train_loss: 0.7600\n",
      "39/424, train_loss: 0.5966\n",
      "40/424, train_loss: 0.6166\n",
      "41/424, train_loss: 0.5581\n",
      "42/424, train_loss: 1.0960\n",
      "43/424, train_loss: 0.5471\n",
      "44/424, train_loss: 1.0744\n",
      "45/424, train_loss: 0.7634\n",
      "46/424, train_loss: 0.6705\n",
      "47/424, train_loss: 0.7955\n",
      "48/424, train_loss: 0.6230\n",
      "49/424, train_loss: 0.8048\n",
      "50/424, train_loss: 0.5812\n",
      "51/424, train_loss: 0.6064\n",
      "52/424, train_loss: 0.9363\n",
      "53/424, train_loss: 0.7968\n",
      "54/424, train_loss: 0.6129\n",
      "55/424, train_loss: 0.5727\n",
      "56/424, train_loss: 0.8435\n",
      "57/424, train_loss: 0.8226\n",
      "58/424, train_loss: 0.6838\n",
      "59/424, train_loss: 0.6929\n",
      "60/424, train_loss: 0.5562\n",
      "61/424, train_loss: 0.7440\n",
      "62/424, train_loss: 0.7819\n",
      "63/424, train_loss: 0.8063\n",
      "64/424, train_loss: 0.8185\n",
      "65/424, train_loss: 0.9428\n",
      "66/424, train_loss: 0.6177\n",
      "67/424, train_loss: 0.7028\n",
      "68/424, train_loss: 0.6340\n",
      "69/424, train_loss: 0.5613\n",
      "70/424, train_loss: 0.3439\n",
      "71/424, train_loss: 0.5954\n",
      "72/424, train_loss: 0.9603\n",
      "73/424, train_loss: 0.5943\n",
      "74/424, train_loss: 2.2271\n",
      "75/424, train_loss: 0.7396\n",
      "76/424, train_loss: 0.8572\n",
      "77/424, train_loss: 0.6696\n",
      "78/424, train_loss: 0.7905\n",
      "79/424, train_loss: 0.8478\n",
      "80/424, train_loss: 0.7751\n",
      "81/424, train_loss: 0.4112\n",
      "82/424, train_loss: 1.6484\n",
      "83/424, train_loss: 0.7963\n",
      "84/424, train_loss: 1.2483\n",
      "85/424, train_loss: 0.5273\n",
      "86/424, train_loss: 0.5965\n",
      "87/424, train_loss: 0.4490\n",
      "88/424, train_loss: 0.4870\n",
      "89/424, train_loss: 0.6454\n",
      "90/424, train_loss: 1.9048\n",
      "91/424, train_loss: 0.1117\n",
      "92/424, train_loss: 0.1005\n",
      "93/424, train_loss: 0.8896\n",
      "94/424, train_loss: 0.5163\n",
      "95/424, train_loss: 0.7367\n",
      "96/424, train_loss: 0.1130\n",
      "97/424, train_loss: 0.6788\n",
      "98/424, train_loss: 0.9798\n",
      "99/424, train_loss: 0.8712\n",
      "100/424, train_loss: 0.5947\n",
      "101/424, train_loss: 0.8125\n",
      "102/424, train_loss: 0.7640\n",
      "103/424, train_loss: 0.5637\n",
      "104/424, train_loss: 0.9650\n",
      "105/424, train_loss: 1.0491\n",
      "106/424, train_loss: 0.1779\n",
      "107/424, train_loss: 1.0658\n",
      "108/424, train_loss: 1.7181\n",
      "109/424, train_loss: 1.4792\n",
      "110/424, train_loss: 1.4634\n",
      "111/424, train_loss: 0.7341\n",
      "112/424, train_loss: 0.5490\n",
      "113/424, train_loss: 0.4752\n",
      "114/424, train_loss: 1.4686\n",
      "115/424, train_loss: 0.1173\n",
      "116/424, train_loss: 2.1231\n",
      "117/424, train_loss: 0.5665\n",
      "118/424, train_loss: 1.9740\n",
      "119/424, train_loss: 1.6431\n",
      "120/424, train_loss: 1.6581\n",
      "121/424, train_loss: 1.1597\n",
      "122/424, train_loss: 0.8066\n",
      "123/424, train_loss: 0.7208\n",
      "124/424, train_loss: 1.0826\n",
      "125/424, train_loss: 1.1139\n",
      "126/424, train_loss: 0.8292\n",
      "127/424, train_loss: 0.5589\n",
      "128/424, train_loss: 0.5872\n",
      "129/424, train_loss: 0.7800\n",
      "130/424, train_loss: 0.8895\n",
      "131/424, train_loss: 0.6205\n",
      "132/424, train_loss: 0.6879\n",
      "133/424, train_loss: 0.6488\n",
      "134/424, train_loss: 0.6854\n",
      "135/424, train_loss: 0.6705\n",
      "136/424, train_loss: 0.7311\n",
      "137/424, train_loss: 0.7291\n",
      "138/424, train_loss: 0.6997\n",
      "139/424, train_loss: 0.7075\n",
      "140/424, train_loss: 0.7524\n",
      "141/424, train_loss: 0.6537\n",
      "142/424, train_loss: 0.6946\n",
      "143/424, train_loss: 0.7332\n",
      "144/424, train_loss: 0.7310\n",
      "145/424, train_loss: 0.7825\n",
      "146/424, train_loss: 0.7382\n",
      "147/424, train_loss: 0.7320\n",
      "148/424, train_loss: 0.7613\n",
      "149/424, train_loss: 0.7975\n",
      "150/424, train_loss: 0.8243\n",
      "151/424, train_loss: 0.5637\n",
      "152/424, train_loss: 0.5258\n",
      "153/424, train_loss: 0.7357\n",
      "154/424, train_loss: 0.6737\n",
      "155/424, train_loss: 0.6589\n",
      "156/424, train_loss: 0.5780\n",
      "157/424, train_loss: 0.4008\n",
      "158/424, train_loss: 0.5215\n",
      "159/424, train_loss: 0.8321\n",
      "160/424, train_loss: 1.1146\n",
      "161/424, train_loss: 0.9173\n",
      "162/424, train_loss: 0.8254\n",
      "163/424, train_loss: 0.6918\n",
      "164/424, train_loss: 0.7540\n",
      "165/424, train_loss: 0.6392\n",
      "166/424, train_loss: 0.6749\n",
      "167/424, train_loss: 0.6594\n",
      "168/424, train_loss: 0.6920\n",
      "169/424, train_loss: 0.7026\n",
      "170/424, train_loss: 0.6488\n",
      "171/424, train_loss: 0.8169\n",
      "172/424, train_loss: 0.6351\n",
      "173/424, train_loss: 0.7931\n",
      "174/424, train_loss: 0.7869\n",
      "175/424, train_loss: 0.6752\n",
      "176/424, train_loss: 0.7119\n",
      "177/424, train_loss: 0.7101\n",
      "178/424, train_loss: 0.6801\n",
      "179/424, train_loss: 0.6314\n",
      "180/424, train_loss: 0.6006\n",
      "181/424, train_loss: 0.9021\n",
      "182/424, train_loss: 0.7912\n",
      "183/424, train_loss: 0.9047\n",
      "184/424, train_loss: 0.6763\n",
      "185/424, train_loss: 0.7040\n",
      "186/424, train_loss: 0.8259\n",
      "187/424, train_loss: 0.6381\n",
      "188/424, train_loss: 0.6853\n",
      "189/424, train_loss: 0.6759\n",
      "190/424, train_loss: 0.7107\n",
      "191/424, train_loss: 0.6909\n",
      "192/424, train_loss: 0.7103\n",
      "193/424, train_loss: 0.6310\n",
      "194/424, train_loss: 0.9896\n",
      "195/424, train_loss: 0.5961\n",
      "196/424, train_loss: 1.0061\n",
      "197/424, train_loss: 0.6107\n",
      "198/424, train_loss: 0.6937\n",
      "199/424, train_loss: 0.6149\n",
      "200/424, train_loss: 0.6937\n",
      "201/424, train_loss: 0.6534\n",
      "202/424, train_loss: 0.6967\n",
      "203/424, train_loss: 0.7596\n",
      "204/424, train_loss: 0.6410\n",
      "205/424, train_loss: 0.7528\n",
      "206/424, train_loss: 0.6624\n",
      "207/424, train_loss: 0.6515\n",
      "208/424, train_loss: 0.7295\n",
      "209/424, train_loss: 0.6499\n",
      "210/424, train_loss: 0.8427\n",
      "211/424, train_loss: 0.6532\n",
      "212/424, train_loss: 0.6859\n",
      "213/424, train_loss: 0.6821\n",
      "214/424, train_loss: 0.6902\n",
      "215/424, train_loss: 0.7572\n",
      "216/424, train_loss: 0.6691\n",
      "217/424, train_loss: 0.7261\n",
      "218/424, train_loss: 0.7039\n",
      "219/424, train_loss: 0.8339\n",
      "220/424, train_loss: 0.6282\n",
      "221/424, train_loss: 0.7682\n",
      "222/424, train_loss: 0.6212\n",
      "223/424, train_loss: 0.6091\n",
      "224/424, train_loss: 0.8464\n",
      "225/424, train_loss: 0.4771\n",
      "226/424, train_loss: 0.8594\n",
      "227/424, train_loss: 0.4081\n",
      "228/424, train_loss: 0.7745\n",
      "229/424, train_loss: 1.0226\n",
      "230/424, train_loss: 0.7845\n",
      "231/424, train_loss: 0.6923\n",
      "232/424, train_loss: 0.8130\n",
      "233/424, train_loss: 0.7723\n",
      "234/424, train_loss: 0.6776\n",
      "235/424, train_loss: 0.6366\n",
      "236/424, train_loss: 0.7140\n",
      "237/424, train_loss: 0.9266\n",
      "238/424, train_loss: 0.9570\n",
      "239/424, train_loss: 0.5804\n",
      "240/424, train_loss: 0.4319\n",
      "241/424, train_loss: 0.7451\n",
      "242/424, train_loss: 0.7394\n",
      "243/424, train_loss: 0.7493\n",
      "244/424, train_loss: 0.7225\n",
      "245/424, train_loss: 0.7226\n",
      "246/424, train_loss: 0.7046\n",
      "247/424, train_loss: 0.7896\n",
      "248/424, train_loss: 0.7146\n",
      "249/424, train_loss: 0.8516\n",
      "250/424, train_loss: 0.6373\n",
      "251/424, train_loss: 0.4933\n",
      "252/424, train_loss: 1.0321\n",
      "253/424, train_loss: 1.0532\n",
      "254/424, train_loss: 0.7226\n",
      "255/424, train_loss: 0.7192\n",
      "256/424, train_loss: 0.6992\n",
      "257/424, train_loss: 0.7709\n",
      "258/424, train_loss: 0.6945\n",
      "259/424, train_loss: 0.7177\n",
      "260/424, train_loss: 0.7156\n",
      "261/424, train_loss: 0.6942\n",
      "262/424, train_loss: 0.7192\n",
      "263/424, train_loss: 0.7251\n",
      "264/424, train_loss: 0.6753\n",
      "265/424, train_loss: 0.6946\n",
      "266/424, train_loss: 0.6726\n",
      "267/424, train_loss: 0.6587\n",
      "268/424, train_loss: 0.7351\n",
      "269/424, train_loss: 0.6554\n",
      "270/424, train_loss: 0.7684\n",
      "271/424, train_loss: 0.7076\n",
      "272/424, train_loss: 0.5291\n",
      "273/424, train_loss: 0.8240\n",
      "274/424, train_loss: 0.7376\n",
      "275/424, train_loss: 0.5894\n",
      "276/424, train_loss: 0.6879\n",
      "277/424, train_loss: 0.5363\n",
      "278/424, train_loss: 0.6111\n",
      "279/424, train_loss: 0.5598\n",
      "280/424, train_loss: 0.7835\n",
      "281/424, train_loss: 0.4339\n",
      "282/424, train_loss: 0.6989\n",
      "283/424, train_loss: 0.5338\n",
      "284/424, train_loss: 1.2014\n",
      "285/424, train_loss: 0.5500\n",
      "286/424, train_loss: 0.5468\n",
      "287/424, train_loss: 0.7286\n",
      "288/424, train_loss: 0.5656\n",
      "289/424, train_loss: 0.7391\n",
      "290/424, train_loss: 0.5882\n",
      "291/424, train_loss: 0.8162\n",
      "292/424, train_loss: 0.8850\n",
      "293/424, train_loss: 0.7336\n",
      "294/424, train_loss: 0.5812\n",
      "295/424, train_loss: 0.7814\n",
      "296/424, train_loss: 0.7670\n",
      "297/424, train_loss: 0.7464\n",
      "298/424, train_loss: 1.0134\n",
      "299/424, train_loss: 0.7787\n",
      "300/424, train_loss: 0.6737\n",
      "301/424, train_loss: 0.8433\n",
      "302/424, train_loss: 0.7042\n",
      "303/424, train_loss: 0.6376\n",
      "304/424, train_loss: 0.7227\n",
      "305/424, train_loss: 0.6915\n",
      "306/424, train_loss: 0.7976\n",
      "307/424, train_loss: 0.6909\n",
      "308/424, train_loss: 0.7046\n",
      "309/424, train_loss: 0.7209\n",
      "310/424, train_loss: 0.6648\n",
      "311/424, train_loss: 0.8043\n",
      "312/424, train_loss: 0.7633\n",
      "313/424, train_loss: 0.7078\n",
      "314/424, train_loss: 0.6176\n",
      "315/424, train_loss: 0.7708\n",
      "316/424, train_loss: 0.6184\n",
      "317/424, train_loss: 0.6427\n",
      "318/424, train_loss: 0.8582\n",
      "319/424, train_loss: 0.5792\n",
      "320/424, train_loss: 0.7271\n",
      "321/424, train_loss: 0.6099\n",
      "322/424, train_loss: 0.5693\n",
      "323/424, train_loss: 0.6908\n",
      "324/424, train_loss: 0.7598\n",
      "325/424, train_loss: 0.5901\n",
      "326/424, train_loss: 0.5720\n",
      "327/424, train_loss: 1.0500\n",
      "328/424, train_loss: 0.5532\n",
      "329/424, train_loss: 0.8431\n",
      "330/424, train_loss: 0.7515\n",
      "331/424, train_loss: 0.6672\n",
      "332/424, train_loss: 0.7039\n",
      "333/424, train_loss: 0.7169\n",
      "334/424, train_loss: 0.5984\n",
      "335/424, train_loss: 0.6173\n",
      "336/424, train_loss: 0.5950\n",
      "337/424, train_loss: 0.7509\n",
      "338/424, train_loss: 0.7917\n",
      "339/424, train_loss: 0.3272\n",
      "340/424, train_loss: 0.8295\n",
      "341/424, train_loss: 0.2796\n",
      "342/424, train_loss: 0.9021\n",
      "343/424, train_loss: 0.5487\n",
      "344/424, train_loss: 0.8818\n",
      "345/424, train_loss: 1.1819\n",
      "346/424, train_loss: 1.0922\n",
      "347/424, train_loss: 1.1514\n",
      "348/424, train_loss: 0.5366\n",
      "349/424, train_loss: 0.6556\n",
      "350/424, train_loss: 0.7446\n",
      "351/424, train_loss: 0.6920\n",
      "352/424, train_loss: 0.6353\n",
      "353/424, train_loss: 0.6874\n",
      "354/424, train_loss: 0.7373\n",
      "355/424, train_loss: 0.8477\n",
      "356/424, train_loss: 0.7222\n",
      "357/424, train_loss: 0.7190\n",
      "358/424, train_loss: 0.6351\n",
      "359/424, train_loss: 0.6421\n",
      "360/424, train_loss: 0.7272\n",
      "361/424, train_loss: 0.6722\n",
      "362/424, train_loss: 0.5785\n",
      "363/424, train_loss: 0.7111\n",
      "364/424, train_loss: 0.7228\n",
      "365/424, train_loss: 0.5896\n",
      "366/424, train_loss: 0.8863\n",
      "367/424, train_loss: 0.8333\n",
      "368/424, train_loss: 0.8315\n",
      "369/424, train_loss: 0.6504\n",
      "370/424, train_loss: 0.6899\n",
      "371/424, train_loss: 0.6793\n",
      "372/424, train_loss: 0.7352\n",
      "373/424, train_loss: 0.6906\n",
      "374/424, train_loss: 0.8040\n",
      "375/424, train_loss: 0.6909\n",
      "376/424, train_loss: 0.7958\n",
      "377/424, train_loss: 0.6844\n",
      "378/424, train_loss: 0.6750\n",
      "379/424, train_loss: 0.6001\n",
      "380/424, train_loss: 0.7831\n",
      "381/424, train_loss: 0.9896\n",
      "382/424, train_loss: 0.6326\n",
      "383/424, train_loss: 0.6345\n",
      "384/424, train_loss: 0.5981\n",
      "385/424, train_loss: 0.8680\n",
      "386/424, train_loss: 0.7157\n",
      "387/424, train_loss: 0.5480\n",
      "388/424, train_loss: 0.7830\n",
      "389/424, train_loss: 0.7061\n",
      "390/424, train_loss: 0.5811\n",
      "391/424, train_loss: 0.5573\n",
      "392/424, train_loss: 0.7540\n",
      "393/424, train_loss: 0.6989\n",
      "394/424, train_loss: 0.7280\n",
      "395/424, train_loss: 0.4410\n",
      "396/424, train_loss: 0.7490\n",
      "397/424, train_loss: 0.7468\n",
      "398/424, train_loss: 0.7593\n",
      "399/424, train_loss: 0.5919\n",
      "400/424, train_loss: 1.0957\n",
      "401/424, train_loss: 0.8704\n",
      "402/424, train_loss: 0.5642\n",
      "403/424, train_loss: 0.7518\n",
      "404/424, train_loss: 0.7223\n",
      "405/424, train_loss: 0.6946\n",
      "406/424, train_loss: 0.7209\n",
      "407/424, train_loss: 0.7266\n",
      "408/424, train_loss: 0.7164\n",
      "409/424, train_loss: 0.7588\n",
      "410/424, train_loss: 0.5542\n",
      "411/424, train_loss: 0.7298\n",
      "412/424, train_loss: 0.7080\n",
      "413/424, train_loss: 0.9148\n",
      "414/424, train_loss: 0.7385\n",
      "415/424, train_loss: 0.8525\n",
      "416/424, train_loss: 0.6223\n",
      "417/424, train_loss: 0.6299\n",
      "418/424, train_loss: 0.6915\n",
      "419/424, train_loss: 0.6576\n",
      "420/424, train_loss: 0.7363\n",
      "421/424, train_loss: 0.6802\n",
      "422/424, train_loss: 0.6898\n",
      "423/424, train_loss: 0.6835\n",
      "424/424, train_loss: 0.6891\n",
      "425/424, train_loss: 0.7107\n",
      "epoch 1 average loss: 0.7513\n",
      "Epoch time duration: 568.6233479976654\n",
      "----------\n",
      "epoch 2\n",
      "1/424, train_loss: 0.6943\n",
      "2/424, train_loss: 0.7080\n",
      "3/424, train_loss: 0.6917\n",
      "4/424, train_loss: 0.7086\n",
      "5/424, train_loss: 0.6531\n",
      "6/424, train_loss: 0.6130\n",
      "7/424, train_loss: 0.7944\n",
      "8/424, train_loss: 0.6215\n",
      "9/424, train_loss: 0.7928\n",
      "10/424, train_loss: 0.7018\n",
      "11/424, train_loss: 0.6169\n",
      "12/424, train_loss: 0.5354\n",
      "13/424, train_loss: 0.4971\n",
      "14/424, train_loss: 0.6029\n",
      "15/424, train_loss: 0.9227\n",
      "16/424, train_loss: 0.5715\n",
      "17/424, train_loss: 1.0698\n",
      "18/424, train_loss: 0.7515\n",
      "19/424, train_loss: 0.8261\n",
      "20/424, train_loss: 0.7930\n",
      "21/424, train_loss: 0.6196\n",
      "22/424, train_loss: 0.6633\n",
      "23/424, train_loss: 0.7054\n",
      "24/424, train_loss: 0.7415\n",
      "25/424, train_loss: 0.6501\n",
      "26/424, train_loss: 0.7019\n",
      "27/424, train_loss: 0.6167\n",
      "28/424, train_loss: 0.9711\n",
      "29/424, train_loss: 0.4847\n",
      "30/424, train_loss: 0.7133\n",
      "31/424, train_loss: 0.8735\n",
      "32/424, train_loss: 0.8525\n",
      "33/424, train_loss: 0.7783\n",
      "34/424, train_loss: 0.6401\n",
      "35/424, train_loss: 0.6956\n",
      "36/424, train_loss: 0.7823\n",
      "37/424, train_loss: 0.6058\n",
      "38/424, train_loss: 0.6744\n",
      "39/424, train_loss: 0.7007\n",
      "40/424, train_loss: 0.5737\n",
      "41/424, train_loss: 0.8098\n",
      "42/424, train_loss: 0.5489\n",
      "43/424, train_loss: 0.7958\n",
      "44/424, train_loss: 0.6118\n",
      "45/424, train_loss: 0.6625\n",
      "46/424, train_loss: 0.5740\n",
      "47/424, train_loss: 0.8159\n",
      "48/424, train_loss: 0.7545\n",
      "49/424, train_loss: 0.5964\n",
      "50/424, train_loss: 0.7717\n",
      "51/424, train_loss: 0.7012\n",
      "52/424, train_loss: 0.8130\n",
      "53/424, train_loss: 0.5698\n",
      "54/424, train_loss: 0.5404\n",
      "55/424, train_loss: 0.6231\n",
      "56/424, train_loss: 0.5423\n",
      "57/424, train_loss: 1.2204\n",
      "58/424, train_loss: 1.0812\n",
      "59/424, train_loss: 0.4771\n",
      "60/424, train_loss: 0.7947\n",
      "61/424, train_loss: 0.5488\n",
      "62/424, train_loss: 0.6616\n",
      "63/424, train_loss: 1.6046\n",
      "64/424, train_loss: 1.7166\n",
      "65/424, train_loss: 0.6276\n",
      "66/424, train_loss: 0.6487\n",
      "67/424, train_loss: 0.6544\n",
      "68/424, train_loss: 0.5515\n",
      "69/424, train_loss: 0.5671\n",
      "70/424, train_loss: 0.9739\n",
      "71/424, train_loss: 0.6466\n",
      "72/424, train_loss: 0.5123\n",
      "73/424, train_loss: 1.7354\n",
      "74/424, train_loss: 0.5079\n",
      "75/424, train_loss: 0.6683\n",
      "76/424, train_loss: 0.8297\n",
      "77/424, train_loss: 0.7925\n",
      "78/424, train_loss: 0.8187\n",
      "79/424, train_loss: 1.3315\n",
      "80/424, train_loss: 0.5426\n",
      "81/424, train_loss: 1.2573\n",
      "82/424, train_loss: 0.4184\n",
      "83/424, train_loss: 0.5612\n",
      "84/424, train_loss: 0.6135\n",
      "85/424, train_loss: 0.6941\n",
      "86/424, train_loss: 0.9131\n",
      "87/424, train_loss: 0.6622\n",
      "88/424, train_loss: 0.6730\n",
      "89/424, train_loss: 0.7383\n",
      "90/424, train_loss: 0.7913\n",
      "91/424, train_loss: 0.7534\n",
      "92/424, train_loss: 1.2000\n",
      "93/424, train_loss: 0.8817\n",
      "94/424, train_loss: 0.7330\n",
      "95/424, train_loss: 0.7275\n",
      "96/424, train_loss: 0.6805\n",
      "97/424, train_loss: 0.8126\n",
      "98/424, train_loss: 0.8026\n",
      "99/424, train_loss: 0.5141\n",
      "100/424, train_loss: 0.6725\n",
      "101/424, train_loss: 0.8463\n",
      "102/424, train_loss: 0.7405\n",
      "103/424, train_loss: 0.7797\n",
      "104/424, train_loss: 0.7148\n",
      "105/424, train_loss: 0.5456\n",
      "106/424, train_loss: 0.6430\n",
      "107/424, train_loss: 0.6661\n",
      "108/424, train_loss: 0.7543\n",
      "109/424, train_loss: 0.6669\n",
      "110/424, train_loss: 0.5020\n",
      "111/424, train_loss: 0.5366\n",
      "112/424, train_loss: 0.4968\n",
      "113/424, train_loss: 0.3929\n",
      "114/424, train_loss: 0.7070\n",
      "115/424, train_loss: 1.0274\n",
      "116/424, train_loss: 1.0684\n",
      "117/424, train_loss: 0.6678\n",
      "118/424, train_loss: 0.5504\n",
      "119/424, train_loss: 0.7414\n",
      "120/424, train_loss: 0.5490\n",
      "121/424, train_loss: 0.8857\n",
      "122/424, train_loss: 0.9417\n",
      "123/424, train_loss: 0.6810\n",
      "124/424, train_loss: 0.9027\n",
      "125/424, train_loss: 0.6963\n",
      "126/424, train_loss: 0.5336\n",
      "127/424, train_loss: 0.6370\n",
      "128/424, train_loss: 0.7208\n",
      "129/424, train_loss: 0.8819\n",
      "130/424, train_loss: 0.9913\n",
      "131/424, train_loss: 1.0547\n",
      "132/424, train_loss: 1.4150\n",
      "133/424, train_loss: 0.6797\n",
      "134/424, train_loss: 0.5032\n",
      "135/424, train_loss: 0.7144\n",
      "136/424, train_loss: 0.6587\n",
      "137/424, train_loss: 0.6662\n",
      "138/424, train_loss: 0.8854\n",
      "139/424, train_loss: 0.7651\n",
      "140/424, train_loss: 0.9291\n",
      "141/424, train_loss: 0.7438\n",
      "142/424, train_loss: 0.8237\n",
      "143/424, train_loss: 0.6984\n",
      "144/424, train_loss: 0.6489\n",
      "145/424, train_loss: 0.6421\n",
      "146/424, train_loss: 0.7467\n",
      "147/424, train_loss: 0.8680\n",
      "148/424, train_loss: 0.6729\n",
      "149/424, train_loss: 0.8297\n",
      "150/424, train_loss: 0.6854\n",
      "151/424, train_loss: 0.6370\n",
      "152/424, train_loss: 0.7001\n",
      "153/424, train_loss: 0.7783\n",
      "154/424, train_loss: 0.6478\n",
      "155/424, train_loss: 0.7268\n",
      "156/424, train_loss: 0.5876\n",
      "157/424, train_loss: 0.6825\n",
      "158/424, train_loss: 0.9612\n",
      "159/424, train_loss: 0.7794\n",
      "160/424, train_loss: 0.6889\n",
      "161/424, train_loss: 0.7382\n",
      "162/424, train_loss: 0.4928\n",
      "163/424, train_loss: 0.5003\n",
      "164/424, train_loss: 0.7603\n",
      "165/424, train_loss: 0.5762\n",
      "166/424, train_loss: 0.9980\n",
      "167/424, train_loss: 0.8897\n",
      "168/424, train_loss: 0.5627\n",
      "169/424, train_loss: 0.6843\n",
      "170/424, train_loss: 0.8884\n",
      "171/424, train_loss: 0.8004\n",
      "172/424, train_loss: 0.6780\n",
      "173/424, train_loss: 0.9296\n",
      "174/424, train_loss: 0.7368\n",
      "175/424, train_loss: 0.8901\n",
      "176/424, train_loss: 0.6097\n",
      "177/424, train_loss: 0.5428\n",
      "178/424, train_loss: 0.7677\n",
      "179/424, train_loss: 0.9012\n",
      "180/424, train_loss: 0.8550\n",
      "181/424, train_loss: 0.7997\n",
      "182/424, train_loss: 0.6893\n",
      "183/424, train_loss: 0.7146\n",
      "184/424, train_loss: 0.6571\n",
      "185/424, train_loss: 0.6602\n",
      "186/424, train_loss: 0.6322\n",
      "187/424, train_loss: 0.7516\n",
      "188/424, train_loss: 1.1855\n",
      "189/424, train_loss: 0.3712\n",
      "190/424, train_loss: 0.3800\n",
      "191/424, train_loss: 0.3411\n",
      "192/424, train_loss: 0.5375\n",
      "193/424, train_loss: 1.2510\n",
      "194/424, train_loss: 0.5799\n",
      "195/424, train_loss: 0.6227\n",
      "196/424, train_loss: 1.2141\n",
      "197/424, train_loss: 0.8606\n",
      "198/424, train_loss: 0.7858\n",
      "199/424, train_loss: 0.9144\n",
      "200/424, train_loss: 0.8495\n",
      "201/424, train_loss: 0.6840\n",
      "202/424, train_loss: 0.7682\n",
      "203/424, train_loss: 0.7047\n",
      "204/424, train_loss: 0.8355\n",
      "205/424, train_loss: 0.8437\n",
      "206/424, train_loss: 0.7200\n",
      "207/424, train_loss: 0.5048\n",
      "208/424, train_loss: 0.8056\n",
      "209/424, train_loss: 0.7068\n",
      "210/424, train_loss: 0.6447\n",
      "211/424, train_loss: 0.7642\n",
      "212/424, train_loss: 0.7891\n",
      "213/424, train_loss: 0.7186\n",
      "214/424, train_loss: 0.6387\n",
      "215/424, train_loss: 0.7075\n",
      "216/424, train_loss: 0.4437\n",
      "217/424, train_loss: 0.5647\n",
      "218/424, train_loss: 0.5109\n",
      "219/424, train_loss: 0.9762\n",
      "220/424, train_loss: 0.9840\n",
      "221/424, train_loss: 0.9570\n",
      "222/424, train_loss: 0.6334\n",
      "223/424, train_loss: 0.5950\n",
      "224/424, train_loss: 0.6143\n",
      "225/424, train_loss: 0.8291\n",
      "226/424, train_loss: 0.7771\n",
      "227/424, train_loss: 0.6254\n",
      "228/424, train_loss: 0.8534\n",
      "229/424, train_loss: 1.0659\n",
      "230/424, train_loss: 0.9604\n",
      "231/424, train_loss: 0.7411\n",
      "232/424, train_loss: 0.7006\n",
      "233/424, train_loss: 0.6376\n",
      "234/424, train_loss: 0.7359\n",
      "235/424, train_loss: 0.7584\n",
      "236/424, train_loss: 0.9268\n",
      "237/424, train_loss: 0.6893\n",
      "238/424, train_loss: 0.6829\n",
      "239/424, train_loss: 0.7188\n",
      "240/424, train_loss: 0.7133\n",
      "241/424, train_loss: 0.7175\n",
      "242/424, train_loss: 0.7539\n",
      "243/424, train_loss: 0.7122\n",
      "244/424, train_loss: 0.7208\n",
      "245/424, train_loss: 0.6833\n",
      "246/424, train_loss: 0.6868\n",
      "247/424, train_loss: 0.7478\n",
      "248/424, train_loss: 0.8557\n",
      "249/424, train_loss: 0.8053\n",
      "250/424, train_loss: 0.6783\n",
      "251/424, train_loss: 0.7074\n",
      "252/424, train_loss: 0.6228\n",
      "253/424, train_loss: 0.5495\n",
      "254/424, train_loss: 0.7642\n",
      "255/424, train_loss: 1.1890\n",
      "256/424, train_loss: 0.8032\n",
      "257/424, train_loss: 0.8067\n",
      "258/424, train_loss: 0.7408\n",
      "259/424, train_loss: 0.7550\n",
      "260/424, train_loss: 0.5928\n",
      "261/424, train_loss: 0.7632\n",
      "262/424, train_loss: 0.9118\n",
      "263/424, train_loss: 0.2429\n",
      "264/424, train_loss: 1.3183\n",
      "265/424, train_loss: 0.5878\n",
      "266/424, train_loss: 0.5479\n",
      "267/424, train_loss: 1.1346\n",
      "268/424, train_loss: 0.5394\n",
      "269/424, train_loss: 0.5492\n",
      "270/424, train_loss: 0.8108\n",
      "271/424, train_loss: 0.6947\n",
      "272/424, train_loss: 0.6765\n",
      "273/424, train_loss: 0.7649\n",
      "274/424, train_loss: 0.8704\n",
      "275/424, train_loss: 0.8384\n",
      "276/424, train_loss: 0.6257\n",
      "277/424, train_loss: 0.7085\n",
      "278/424, train_loss: 0.6343\n",
      "279/424, train_loss: 0.8040\n",
      "280/424, train_loss: 0.7256\n",
      "281/424, train_loss: 0.7317\n",
      "282/424, train_loss: 0.6771\n",
      "283/424, train_loss: 0.7048\n",
      "284/424, train_loss: 0.7029\n",
      "285/424, train_loss: 0.7465\n",
      "286/424, train_loss: 0.6573\n",
      "287/424, train_loss: 0.6821\n",
      "288/424, train_loss: 0.7107\n",
      "289/424, train_loss: 0.7263\n",
      "290/424, train_loss: 0.7123\n",
      "291/424, train_loss: 0.6907\n",
      "292/424, train_loss: 0.6366\n",
      "293/424, train_loss: 0.6118\n",
      "294/424, train_loss: 0.7400\n",
      "295/424, train_loss: 0.5988\n",
      "296/424, train_loss: 0.9476\n",
      "297/424, train_loss: 0.5554\n",
      "298/424, train_loss: 0.7610\n",
      "299/424, train_loss: 0.9398\n",
      "300/424, train_loss: 0.7250\n",
      "301/424, train_loss: 0.6930\n",
      "302/424, train_loss: 0.6852\n",
      "303/424, train_loss: 0.7441\n",
      "304/424, train_loss: 0.5839\n",
      "305/424, train_loss: 0.6760\n",
      "306/424, train_loss: 0.5867\n",
      "307/424, train_loss: 0.7422\n",
      "308/424, train_loss: 0.7897\n",
      "309/424, train_loss: 0.8108\n",
      "310/424, train_loss: 0.7749\n",
      "311/424, train_loss: 0.5607\n",
      "312/424, train_loss: 0.5771\n",
      "313/424, train_loss: 0.6910\n",
      "314/424, train_loss: 0.7070\n",
      "315/424, train_loss: 0.7429\n",
      "316/424, train_loss: 0.8721\n",
      "317/424, train_loss: 0.6736\n",
      "318/424, train_loss: 0.7115\n",
      "319/424, train_loss: 0.7018\n",
      "320/424, train_loss: 0.6951\n",
      "321/424, train_loss: 0.9650\n",
      "322/424, train_loss: 0.6875\n",
      "323/424, train_loss: 0.6375\n",
      "324/424, train_loss: 0.6676\n",
      "325/424, train_loss: 0.6989\n",
      "326/424, train_loss: 0.6896\n",
      "327/424, train_loss: 0.6667\n",
      "328/424, train_loss: 0.6755\n",
      "329/424, train_loss: 0.7887\n",
      "330/424, train_loss: 0.7188\n",
      "331/424, train_loss: 0.7729\n",
      "332/424, train_loss: 0.7262\n",
      "333/424, train_loss: 0.6866\n",
      "334/424, train_loss: 0.7245\n",
      "335/424, train_loss: 0.6945\n",
      "336/424, train_loss: 0.6956\n",
      "337/424, train_loss: 0.5989\n",
      "338/424, train_loss: 0.6207\n",
      "339/424, train_loss: 0.8456\n",
      "340/424, train_loss: 0.7034\n",
      "341/424, train_loss: 0.7396\n",
      "342/424, train_loss: 0.6923\n",
      "343/424, train_loss: 0.8555\n",
      "344/424, train_loss: 0.6613\n",
      "345/424, train_loss: 0.7262\n",
      "346/424, train_loss: 0.5586\n",
      "347/424, train_loss: 0.8160\n",
      "348/424, train_loss: 0.9474\n",
      "349/424, train_loss: 0.7238\n",
      "350/424, train_loss: 0.9739\n",
      "351/424, train_loss: 0.5914\n",
      "352/424, train_loss: 0.5557\n",
      "353/424, train_loss: 0.6794\n",
      "354/424, train_loss: 0.4944\n",
      "355/424, train_loss: 0.6450\n",
      "356/424, train_loss: 0.5763\n",
      "357/424, train_loss: 1.0649\n",
      "358/424, train_loss: 0.5533\n",
      "359/424, train_loss: 0.6052\n",
      "360/424, train_loss: 0.5941\n",
      "361/424, train_loss: 0.9093\n",
      "362/424, train_loss: 0.5067\n",
      "363/424, train_loss: 0.7615\n",
      "364/424, train_loss: 0.9453\n",
      "365/424, train_loss: 0.6713\n",
      "366/424, train_loss: 0.7681\n",
      "367/424, train_loss: 0.7468\n",
      "368/424, train_loss: 0.7078\n",
      "369/424, train_loss: 0.7833\n",
      "370/424, train_loss: 0.7440\n",
      "371/424, train_loss: 0.8230\n",
      "372/424, train_loss: 0.7057\n",
      "373/424, train_loss: 0.7164\n",
      "374/424, train_loss: 0.7024\n",
      "375/424, train_loss: 0.6686\n",
      "376/424, train_loss: 0.7855\n",
      "377/424, train_loss: 0.6980\n",
      "378/424, train_loss: 0.6764\n",
      "379/424, train_loss: 0.6472\n",
      "380/424, train_loss: 0.6270\n",
      "381/424, train_loss: 0.6407\n",
      "382/424, train_loss: 0.8813\n",
      "383/424, train_loss: 0.6138\n",
      "384/424, train_loss: 0.6124\n",
      "385/424, train_loss: 0.5233\n",
      "386/424, train_loss: 0.5622\n",
      "387/424, train_loss: 0.7374\n",
      "388/424, train_loss: 0.8154\n",
      "389/424, train_loss: 0.3399\n",
      "390/424, train_loss: 0.9042\n",
      "391/424, train_loss: 0.3396\n",
      "392/424, train_loss: 0.8003\n",
      "393/424, train_loss: 0.5326\n",
      "394/424, train_loss: 0.9066\n",
      "395/424, train_loss: 0.8780\n",
      "396/424, train_loss: 0.9235\n",
      "397/424, train_loss: 0.6362\n",
      "398/424, train_loss: 0.7345\n",
      "399/424, train_loss: 0.6714\n",
      "400/424, train_loss: 0.7245\n",
      "401/424, train_loss: 0.7776\n",
      "402/424, train_loss: 0.7102\n",
      "403/424, train_loss: 0.7191\n",
      "404/424, train_loss: 0.8243\n",
      "405/424, train_loss: 0.8405\n",
      "406/424, train_loss: 0.7849\n",
      "407/424, train_loss: 0.7040\n",
      "408/424, train_loss: 0.6721\n",
      "409/424, train_loss: 0.6796\n",
      "410/424, train_loss: 0.7946\n",
      "411/424, train_loss: 0.8922\n",
      "412/424, train_loss: 0.7524\n",
      "413/424, train_loss: 0.9308\n",
      "414/424, train_loss: 0.6036\n",
      "415/424, train_loss: 0.7028\n",
      "416/424, train_loss: 0.7001\n",
      "417/424, train_loss: 0.6927\n",
      "418/424, train_loss: 0.6631\n",
      "419/424, train_loss: 0.7097\n",
      "420/424, train_loss: 0.6285\n",
      "421/424, train_loss: 0.6370\n",
      "422/424, train_loss: 0.7076\n",
      "423/424, train_loss: 0.6160\n",
      "424/424, train_loss: 0.7330\n",
      "425/424, train_loss: 0.4152\n",
      "epoch 2 average loss: 0.7305\n",
      "Epoch time duration: 567.4744098186493\n",
      "current epoch: 2 current accuracy: 0.4749 best accuracy: 0.4749 at epoch 2\n",
      "----------\n",
      "epoch 3\n",
      "1/424, train_loss: 0.9763\n",
      "2/424, train_loss: 0.7713\n",
      "3/424, train_loss: 1.2334\n",
      "4/424, train_loss: 0.9233\n",
      "5/424, train_loss: 0.5826\n",
      "6/424, train_loss: 0.5219\n",
      "7/424, train_loss: 0.7140\n",
      "8/424, train_loss: 0.6571\n",
      "9/424, train_loss: 0.8002\n",
      "10/424, train_loss: 0.6987\n",
      "11/424, train_loss: 0.7156\n",
      "12/424, train_loss: 0.6416\n",
      "13/424, train_loss: 0.7887\n",
      "14/424, train_loss: 0.5355\n",
      "15/424, train_loss: 0.5859\n",
      "16/424, train_loss: 0.4380\n",
      "17/424, train_loss: 1.1951\n",
      "18/424, train_loss: 0.8337\n",
      "19/424, train_loss: 0.7740\n",
      "20/424, train_loss: 1.1760\n",
      "21/424, train_loss: 1.0778\n",
      "22/424, train_loss: 0.7146\n",
      "23/424, train_loss: 0.7104\n",
      "24/424, train_loss: 0.6377\n",
      "25/424, train_loss: 0.8391\n",
      "26/424, train_loss: 0.8029\n",
      "27/424, train_loss: 0.5274\n",
      "28/424, train_loss: 0.8134\n",
      "29/424, train_loss: 0.5674\n",
      "30/424, train_loss: 0.6347\n",
      "31/424, train_loss: 0.5648\n",
      "32/424, train_loss: 1.2294\n",
      "33/424, train_loss: 0.8781\n",
      "34/424, train_loss: 1.3843\n",
      "35/424, train_loss: 0.5242\n",
      "36/424, train_loss: 0.7541\n",
      "37/424, train_loss: 0.8247\n",
      "38/424, train_loss: 0.6949\n",
      "39/424, train_loss: 0.6931\n",
      "40/424, train_loss: 0.7329\n",
      "41/424, train_loss: 0.7633\n",
      "42/424, train_loss: 0.6041\n",
      "43/424, train_loss: 0.7074\n",
      "44/424, train_loss: 0.5744\n",
      "45/424, train_loss: 0.7456\n",
      "46/424, train_loss: 0.7180\n",
      "47/424, train_loss: 0.7283\n",
      "48/424, train_loss: 0.4277\n",
      "49/424, train_loss: 0.5668\n",
      "50/424, train_loss: 0.7364\n",
      "51/424, train_loss: 0.5879\n",
      "52/424, train_loss: 0.7432\n",
      "53/424, train_loss: 0.7881\n",
      "54/424, train_loss: 0.8047\n",
      "55/424, train_loss: 0.5374\n",
      "56/424, train_loss: 0.8253\n",
      "57/424, train_loss: 0.8851\n",
      "58/424, train_loss: 0.6030\n",
      "59/424, train_loss: 0.6865\n",
      "60/424, train_loss: 0.6119\n",
      "61/424, train_loss: 0.7358\n",
      "62/424, train_loss: 0.6779\n",
      "63/424, train_loss: 0.6411\n",
      "64/424, train_loss: 0.6355\n",
      "65/424, train_loss: 0.6913\n",
      "66/424, train_loss: 0.6884\n",
      "67/424, train_loss: 0.6367\n",
      "68/424, train_loss: 0.8109\n",
      "69/424, train_loss: 0.5576\n",
      "70/424, train_loss: 0.8060\n",
      "71/424, train_loss: 0.5558\n",
      "72/424, train_loss: 0.6439\n",
      "73/424, train_loss: 0.9854\n",
      "74/424, train_loss: 0.5372\n",
      "75/424, train_loss: 0.9192\n",
      "76/424, train_loss: 0.6945\n",
      "77/424, train_loss: 0.7493\n",
      "78/424, train_loss: 0.6932\n",
      "79/424, train_loss: 0.8838\n",
      "80/424, train_loss: 0.6470\n",
      "81/424, train_loss: 0.7390\n",
      "82/424, train_loss: 0.8295\n",
      "83/424, train_loss: 0.8207\n",
      "84/424, train_loss: 0.6824\n",
      "85/424, train_loss: 0.7045\n",
      "86/424, train_loss: 0.6786\n",
      "87/424, train_loss: 0.6827\n",
      "88/424, train_loss: 0.6425\n",
      "89/424, train_loss: 0.6447\n",
      "90/424, train_loss: 0.7463\n",
      "91/424, train_loss: 0.5987\n",
      "92/424, train_loss: 0.7469\n",
      "93/424, train_loss: 0.4080\n",
      "94/424, train_loss: 0.9682\n",
      "95/424, train_loss: 0.7689\n",
      "96/424, train_loss: 0.7859\n",
      "97/424, train_loss: 0.7482\n",
      "98/424, train_loss: 0.5601\n",
      "99/424, train_loss: 0.9434\n",
      "100/424, train_loss: 0.7121\n",
      "101/424, train_loss: 0.8099\n",
      "102/424, train_loss: 0.7036\n",
      "103/424, train_loss: 0.7058\n",
      "104/424, train_loss: 0.6998\n",
      "105/424, train_loss: 0.6897\n",
      "106/424, train_loss: 0.7826\n",
      "107/424, train_loss: 0.6193\n",
      "108/424, train_loss: 0.8253\n",
      "109/424, train_loss: 0.6112\n",
      "110/424, train_loss: 0.8374\n",
      "111/424, train_loss: 0.7224\n",
      "112/424, train_loss: 0.7108\n",
      "113/424, train_loss: 0.6865\n",
      "114/424, train_loss: 0.6788\n",
      "115/424, train_loss: 0.7432\n",
      "116/424, train_loss: 0.6650\n",
      "117/424, train_loss: 0.6389\n",
      "118/424, train_loss: 0.5513\n",
      "119/424, train_loss: 0.7280\n",
      "120/424, train_loss: 0.6704\n",
      "121/424, train_loss: 0.9532\n",
      "122/424, train_loss: 0.7167\n",
      "123/424, train_loss: 0.3907\n",
      "124/424, train_loss: 0.9095\n",
      "125/424, train_loss: 0.7711\n",
      "126/424, train_loss: 0.3933\n",
      "127/424, train_loss: 0.9529\n",
      "128/424, train_loss: 0.7874\n",
      "129/424, train_loss: 0.5597\n",
      "130/424, train_loss: 0.9174\n",
      "131/424, train_loss: 0.8256\n",
      "132/424, train_loss: 0.7789\n",
      "133/424, train_loss: 0.6576\n",
      "134/424, train_loss: 0.7255\n",
      "135/424, train_loss: 0.7108\n",
      "136/424, train_loss: 0.6693\n",
      "137/424, train_loss: 0.6799\n",
      "138/424, train_loss: 0.7180\n",
      "139/424, train_loss: 0.7921\n",
      "140/424, train_loss: 0.5684\n",
      "141/424, train_loss: 0.6958\n",
      "142/424, train_loss: 0.7981\n",
      "143/424, train_loss: 0.6942\n",
      "144/424, train_loss: 0.8556\n",
      "145/424, train_loss: 0.6941\n",
      "146/424, train_loss: 0.6678\n",
      "147/424, train_loss: 0.7102\n",
      "148/424, train_loss: 0.6196\n",
      "149/424, train_loss: 0.5742\n",
      "150/424, train_loss: 0.7032\n",
      "151/424, train_loss: 0.6815\n",
      "152/424, train_loss: 0.6093\n",
      "153/424, train_loss: 0.7250\n",
      "154/424, train_loss: 0.7589\n",
      "155/424, train_loss: 0.5673\n",
      "156/424, train_loss: 0.8102\n",
      "157/424, train_loss: 0.3334\n",
      "158/424, train_loss: 0.8329\n",
      "159/424, train_loss: 0.6190\n",
      "160/424, train_loss: 1.1324\n",
      "161/424, train_loss: 0.5577\n",
      "162/424, train_loss: 0.7551\n",
      "163/424, train_loss: 1.1127\n",
      "164/424, train_loss: 0.6083\n",
      "165/424, train_loss: 0.7848\n",
      "166/424, train_loss: 0.6653\n",
      "167/424, train_loss: 0.6774\n",
      "168/424, train_loss: 0.7196\n",
      "169/424, train_loss: 0.7310\n",
      "170/424, train_loss: 0.5748\n",
      "171/424, train_loss: 0.7023\n",
      "172/424, train_loss: 0.7142\n",
      "173/424, train_loss: 0.8498\n",
      "174/424, train_loss: 0.7462\n",
      "175/424, train_loss: 0.7109\n",
      "176/424, train_loss: 0.8190\n",
      "177/424, train_loss: 0.6904\n",
      "178/424, train_loss: 0.6497\n",
      "179/424, train_loss: 0.7927\n",
      "180/424, train_loss: 0.7201\n",
      "181/424, train_loss: 0.7083\n",
      "182/424, train_loss: 0.7987\n",
      "183/424, train_loss: 0.8019\n",
      "184/424, train_loss: 0.6072\n",
      "185/424, train_loss: 0.7078\n",
      "186/424, train_loss: 0.8722\n",
      "187/424, train_loss: 0.6116\n",
      "188/424, train_loss: 0.5091\n",
      "189/424, train_loss: 0.8134\n",
      "190/424, train_loss: 0.6651\n",
      "191/424, train_loss: 0.9381\n",
      "192/424, train_loss: 0.7435\n",
      "193/424, train_loss: 0.7891\n",
      "194/424, train_loss: 0.7140\n",
      "195/424, train_loss: 0.6912\n",
      "196/424, train_loss: 0.6068\n",
      "197/424, train_loss: 1.0009\n",
      "198/424, train_loss: 0.7079\n",
      "199/424, train_loss: 0.8774\n",
      "200/424, train_loss: 1.0405\n",
      "201/424, train_loss: 0.9552\n",
      "202/424, train_loss: 0.7147\n",
      "203/424, train_loss: 0.6680\n",
      "204/424, train_loss: 0.6710\n",
      "205/424, train_loss: 0.6877\n",
      "206/424, train_loss: 0.8136\n",
      "207/424, train_loss: 0.6715\n",
      "208/424, train_loss: 0.6862\n",
      "209/424, train_loss: 0.6813\n",
      "210/424, train_loss: 0.6265\n",
      "211/424, train_loss: 0.5795\n",
      "212/424, train_loss: 0.5986\n",
      "213/424, train_loss: 0.8207\n",
      "214/424, train_loss: 0.7899\n",
      "215/424, train_loss: 0.5358\n",
      "216/424, train_loss: 0.7377\n",
      "217/424, train_loss: 0.8177\n",
      "218/424, train_loss: 0.8419\n",
      "219/424, train_loss: 0.7154\n",
      "220/424, train_loss: 0.7737\n",
      "221/424, train_loss: 0.7561\n",
      "222/424, train_loss: 0.7153\n",
      "223/424, train_loss: 0.5703\n",
      "224/424, train_loss: 0.7313\n",
      "225/424, train_loss: 0.7518\n",
      "226/424, train_loss: 0.6163\n",
      "227/424, train_loss: 0.8654\n",
      "228/424, train_loss: 0.6716\n",
      "229/424, train_loss: 0.6968\n",
      "230/424, train_loss: 0.7488\n",
      "231/424, train_loss: 0.7006\n",
      "232/424, train_loss: 0.6732\n",
      "233/424, train_loss: 0.6462\n",
      "234/424, train_loss: 0.6770\n",
      "235/424, train_loss: 0.8274\n",
      "236/424, train_loss: 0.7654\n",
      "237/424, train_loss: 0.6907\n",
      "238/424, train_loss: 0.6444\n",
      "239/424, train_loss: 0.7631\n",
      "240/424, train_loss: 0.6781\n",
      "241/424, train_loss: 0.7147\n",
      "242/424, train_loss: 0.7566\n",
      "243/424, train_loss: 0.7068\n",
      "244/424, train_loss: 0.7287\n",
      "245/424, train_loss: 0.7793\n",
      "246/424, train_loss: 0.7712\n",
      "247/424, train_loss: 0.6757\n",
      "248/424, train_loss: 0.7079\n",
      "249/424, train_loss: 0.6810\n",
      "250/424, train_loss: 0.6691\n",
      "251/424, train_loss: 0.6286\n",
      "252/424, train_loss: 0.5973\n",
      "253/424, train_loss: 0.8810\n",
      "254/424, train_loss: 0.7559\n",
      "255/424, train_loss: 0.4377\n",
      "256/424, train_loss: 0.6605\n",
      "257/424, train_loss: 0.5620\n",
      "258/424, train_loss: 0.7050\n",
      "259/424, train_loss: 0.7983\n",
      "260/424, train_loss: 0.6779\n",
      "261/424, train_loss: 0.5334\n",
      "262/424, train_loss: 0.5547\n",
      "263/424, train_loss: 0.5343\n",
      "264/424, train_loss: 0.6279\n",
      "265/424, train_loss: 0.7973\n",
      "266/424, train_loss: 0.8681\n",
      "267/424, train_loss: 1.0141\n",
      "268/424, train_loss: 0.9174\n",
      "269/424, train_loss: 0.7001\n",
      "270/424, train_loss: 0.6652\n",
      "271/424, train_loss: 0.7402\n",
      "272/424, train_loss: 0.7170\n",
      "273/424, train_loss: 0.7367\n",
      "274/424, train_loss: 0.8221\n",
      "275/424, train_loss: 0.6979\n",
      "276/424, train_loss: 0.8335\n",
      "277/424, train_loss: 0.5602\n",
      "278/424, train_loss: 0.8820\n",
      "279/424, train_loss: 0.7328\n",
      "280/424, train_loss: 0.5613\n",
      "281/424, train_loss: 0.6614\n",
      "282/424, train_loss: 0.7059\n",
      "283/424, train_loss: 0.7869\n",
      "284/424, train_loss: 0.6977\n",
      "285/424, train_loss: 0.7004\n",
      "286/424, train_loss: 0.6196\n",
      "287/424, train_loss: 0.8483\n",
      "288/424, train_loss: 0.7623\n",
      "289/424, train_loss: 0.8506\n",
      "290/424, train_loss: 0.6188\n",
      "291/424, train_loss: 0.7378\n",
      "292/424, train_loss: 0.9023\n",
      "293/424, train_loss: 0.7064\n",
      "294/424, train_loss: 0.6526\n",
      "295/424, train_loss: 0.6523\n",
      "296/424, train_loss: 0.6923\n",
      "297/424, train_loss: 0.7538\n",
      "298/424, train_loss: 0.9222\n",
      "299/424, train_loss: 0.7614\n",
      "300/424, train_loss: 0.7585\n",
      "301/424, train_loss: 0.8167\n",
      "302/424, train_loss: 0.6819\n",
      "303/424, train_loss: 0.6421\n",
      "304/424, train_loss: 0.6997\n",
      "305/424, train_loss: 0.4897\n",
      "306/424, train_loss: 0.7916\n",
      "307/424, train_loss: 0.9973\n",
      "308/424, train_loss: 0.7177\n",
      "309/424, train_loss: 0.3363\n",
      "310/424, train_loss: 0.9844\n",
      "311/424, train_loss: 0.4393\n",
      "312/424, train_loss: 0.8167\n",
      "313/424, train_loss: 1.0499\n",
      "314/424, train_loss: 0.5834\n",
      "315/424, train_loss: 0.7994\n",
      "316/424, train_loss: 0.7684\n",
      "317/424, train_loss: 0.8102\n",
      "318/424, train_loss: 0.9457\n",
      "319/424, train_loss: 0.7935\n",
      "320/424, train_loss: 0.6504\n",
      "321/424, train_loss: 0.7010\n",
      "322/424, train_loss: 0.5859\n",
      "323/424, train_loss: 0.8220\n",
      "324/424, train_loss: 0.5733\n",
      "325/424, train_loss: 1.1980\n",
      "326/424, train_loss: 0.5559\n",
      "327/424, train_loss: 0.7964\n",
      "328/424, train_loss: 0.5555\n",
      "329/424, train_loss: 1.0545\n",
      "330/424, train_loss: 0.7979\n",
      "331/424, train_loss: 1.1738\n",
      "332/424, train_loss: 0.4381\n",
      "333/424, train_loss: 0.8467\n",
      "334/424, train_loss: 0.6041\n",
      "335/424, train_loss: 0.5671\n",
      "336/424, train_loss: 0.6603\n",
      "337/424, train_loss: 0.7373\n",
      "338/424, train_loss: 0.6581\n",
      "339/424, train_loss: 0.7581\n",
      "340/424, train_loss: 0.6923\n",
      "341/424, train_loss: 0.6771\n",
      "342/424, train_loss: 0.6240\n",
      "343/424, train_loss: 0.7673\n",
      "344/424, train_loss: 0.6760\n",
      "345/424, train_loss: 0.9681\n",
      "346/424, train_loss: 0.8135\n",
      "347/424, train_loss: 0.6090\n",
      "348/424, train_loss: 0.5537\n",
      "349/424, train_loss: 0.6331\n",
      "350/424, train_loss: 0.7079\n",
      "351/424, train_loss: 0.8442\n",
      "352/424, train_loss: 0.7628\n",
      "353/424, train_loss: 0.6960\n",
      "354/424, train_loss: 0.7377\n",
      "355/424, train_loss: 0.7185\n",
      "356/424, train_loss: 0.6871\n",
      "357/424, train_loss: 0.6932\n",
      "358/424, train_loss: 0.7421\n",
      "359/424, train_loss: 0.5729\n",
      "360/424, train_loss: 0.8098\n",
      "361/424, train_loss: 0.6559\n",
      "362/424, train_loss: 0.6045\n",
      "363/424, train_loss: 0.8943\n",
      "364/424, train_loss: 0.7032\n",
      "365/424, train_loss: 0.9024\n",
      "366/424, train_loss: 0.7190\n",
      "367/424, train_loss: 0.8474\n",
      "368/424, train_loss: 0.7002\n",
      "369/424, train_loss: 0.6556\n",
      "370/424, train_loss: 0.6556\n",
      "371/424, train_loss: 0.6946\n",
      "372/424, train_loss: 0.7035\n",
      "373/424, train_loss: 0.7906\n",
      "374/424, train_loss: 0.7742\n",
      "375/424, train_loss: 0.6647\n",
      "376/424, train_loss: 0.6855\n",
      "377/424, train_loss: 0.7021\n",
      "378/424, train_loss: 0.6832\n",
      "379/424, train_loss: 0.6799\n",
      "380/424, train_loss: 0.6935\n",
      "381/424, train_loss: 0.7151\n",
      "382/424, train_loss: 0.6614\n",
      "383/424, train_loss: 0.6910\n",
      "384/424, train_loss: 0.7114\n",
      "385/424, train_loss: 0.6812\n",
      "386/424, train_loss: 0.7119\n",
      "387/424, train_loss: 0.5957\n",
      "388/424, train_loss: 0.5895\n",
      "389/424, train_loss: 0.7554\n",
      "390/424, train_loss: 0.8162\n",
      "391/424, train_loss: 0.6089\n",
      "392/424, train_loss: 0.6068\n",
      "393/424, train_loss: 0.5275\n",
      "394/424, train_loss: 0.6527\n",
      "395/424, train_loss: 0.9202\n",
      "396/424, train_loss: 1.0796\n",
      "397/424, train_loss: 0.4703\n",
      "398/424, train_loss: 0.4673\n",
      "399/424, train_loss: 0.7578\n",
      "400/424, train_loss: 0.8747\n",
      "401/424, train_loss: 0.7937\n",
      "402/424, train_loss: 0.6086\n",
      "403/424, train_loss: 0.6216\n",
      "404/424, train_loss: 0.7081\n",
      "405/424, train_loss: 0.6422\n",
      "406/424, train_loss: 0.6295\n",
      "407/424, train_loss: 0.6581\n",
      "408/424, train_loss: 0.5635\n",
      "409/424, train_loss: 0.5275\n",
      "410/424, train_loss: 0.6148\n",
      "411/424, train_loss: 0.7144\n",
      "412/424, train_loss: 0.7512\n",
      "413/424, train_loss: 0.9567\n",
      "414/424, train_loss: 0.3953\n",
      "415/424, train_loss: 1.2070\n",
      "416/424, train_loss: 0.6877\n",
      "417/424, train_loss: 0.9382\n",
      "418/424, train_loss: 0.6486\n",
      "419/424, train_loss: 0.9719\n",
      "420/424, train_loss: 0.7126\n",
      "421/424, train_loss: 0.7687\n",
      "422/424, train_loss: 0.6747\n",
      "423/424, train_loss: 0.6759\n",
      "424/424, train_loss: 0.8475\n",
      "425/424, train_loss: 0.4280\n",
      "epoch 3 average loss: 0.7236\n",
      "Epoch time duration: 567.3552601337433\n",
      "----------\n",
      "epoch 4\n",
      "1/424, train_loss: 0.9941\n",
      "2/424, train_loss: 1.0698\n",
      "3/424, train_loss: 0.9855\n",
      "4/424, train_loss: 0.5895\n",
      "5/424, train_loss: 0.5495\n",
      "6/424, train_loss: 0.7247\n",
      "7/424, train_loss: 0.5709\n",
      "8/424, train_loss: 0.7053\n",
      "9/424, train_loss: 0.9033\n",
      "10/424, train_loss: 0.7287\n",
      "11/424, train_loss: 0.6780\n",
      "12/424, train_loss: 0.7161\n",
      "13/424, train_loss: 0.7635\n",
      "14/424, train_loss: 0.6519\n",
      "15/424, train_loss: 0.7186\n",
      "16/424, train_loss: 0.7030\n",
      "17/424, train_loss: 0.6705\n",
      "18/424, train_loss: 0.5893\n",
      "19/424, train_loss: 0.8187\n",
      "20/424, train_loss: 0.7269\n",
      "21/424, train_loss: 0.9581\n",
      "22/424, train_loss: 0.8203\n",
      "23/424, train_loss: 0.7792\n",
      "24/424, train_loss: 0.6848\n",
      "25/424, train_loss: 0.6675\n",
      "26/424, train_loss: 0.6835\n",
      "27/424, train_loss: 0.6620\n",
      "28/424, train_loss: 0.7087\n",
      "29/424, train_loss: 0.6583\n",
      "30/424, train_loss: 0.6907\n",
      "31/424, train_loss: 0.6493\n",
      "32/424, train_loss: 0.7008\n",
      "33/424, train_loss: 0.7775\n",
      "34/424, train_loss: 0.6425\n",
      "35/424, train_loss: 0.5524\n",
      "36/424, train_loss: 0.6425\n",
      "37/424, train_loss: 0.7136\n",
      "38/424, train_loss: 0.6984\n",
      "39/424, train_loss: 1.0112\n",
      "40/424, train_loss: 0.8745\n",
      "41/424, train_loss: 0.6240\n",
      "42/424, train_loss: 0.5671\n",
      "43/424, train_loss: 0.8322\n",
      "44/424, train_loss: 0.7024\n",
      "45/424, train_loss: 0.7708\n",
      "46/424, train_loss: 0.6384\n",
      "47/424, train_loss: 0.6716\n",
      "48/424, train_loss: 0.7349\n",
      "49/424, train_loss: 0.7313\n",
      "50/424, train_loss: 0.7452\n",
      "51/424, train_loss: 0.6226\n",
      "52/424, train_loss: 0.5732\n",
      "53/424, train_loss: 0.7249\n",
      "54/424, train_loss: 0.6880\n",
      "55/424, train_loss: 0.6923\n",
      "56/424, train_loss: 0.7285\n",
      "57/424, train_loss: 0.6841\n",
      "58/424, train_loss: 0.7802\n",
      "59/424, train_loss: 0.6964\n",
      "60/424, train_loss: 0.6506\n",
      "61/424, train_loss: 0.6794\n",
      "62/424, train_loss: 0.7011\n",
      "63/424, train_loss: 0.5684\n",
      "64/424, train_loss: 0.7106\n",
      "65/424, train_loss: 0.7490\n",
      "66/424, train_loss: 0.5988\n",
      "67/424, train_loss: 0.6022\n",
      "68/424, train_loss: 0.8747\n",
      "69/424, train_loss: 0.4491\n",
      "70/424, train_loss: 0.8962\n",
      "71/424, train_loss: 0.4267\n",
      "72/424, train_loss: 0.6945\n",
      "73/424, train_loss: 0.9171\n",
      "74/424, train_loss: 0.4191\n",
      "75/424, train_loss: 0.7427\n",
      "76/424, train_loss: 0.6860\n",
      "77/424, train_loss: 0.7426\n",
      "78/424, train_loss: 0.6024\n",
      "79/424, train_loss: 0.6108\n",
      "80/424, train_loss: 0.6536\n",
      "81/424, train_loss: 0.5913\n",
      "82/424, train_loss: 0.9059\n",
      "83/424, train_loss: 0.7321\n",
      "84/424, train_loss: 0.5958\n",
      "85/424, train_loss: 0.7010\n",
      "86/424, train_loss: 0.6715\n",
      "87/424, train_loss: 0.6088\n",
      "88/424, train_loss: 0.5822\n",
      "89/424, train_loss: 0.7811\n",
      "90/424, train_loss: 0.6541\n",
      "91/424, train_loss: 0.6947\n",
      "92/424, train_loss: 0.6748\n",
      "93/424, train_loss: 0.6088\n",
      "94/424, train_loss: 0.6652\n",
      "95/424, train_loss: 0.6271\n",
      "96/424, train_loss: 0.6946\n",
      "97/424, train_loss: 0.6833\n",
      "98/424, train_loss: 0.5816\n",
      "99/424, train_loss: 0.5289\n",
      "100/424, train_loss: 0.6819\n",
      "101/424, train_loss: 0.7573\n",
      "102/424, train_loss: 0.6939\n",
      "103/424, train_loss: 0.5990\n",
      "104/424, train_loss: 0.3871\n",
      "105/424, train_loss: 0.9620\n",
      "106/424, train_loss: 0.6560\n",
      "107/424, train_loss: 0.8265\n",
      "108/424, train_loss: 0.4321\n",
      "109/424, train_loss: 0.5643\n",
      "110/424, train_loss: 0.5624\n",
      "111/424, train_loss: 0.8349\n",
      "112/424, train_loss: 0.8616\n",
      "113/424, train_loss: 0.4963\n",
      "114/424, train_loss: 0.8790\n",
      "115/424, train_loss: 0.5780\n",
      "116/424, train_loss: 0.6478\n",
      "117/424, train_loss: 0.8123\n",
      "118/424, train_loss: 0.7025\n",
      "119/424, train_loss: 0.7736\n",
      "120/424, train_loss: 0.7046\n",
      "121/424, train_loss: 0.6682\n",
      "122/424, train_loss: 0.6633\n",
      "123/424, train_loss: 0.8228\n",
      "124/424, train_loss: 0.7784\n",
      "125/424, train_loss: 0.6926\n",
      "126/424, train_loss: 0.7880\n",
      "127/424, train_loss: 0.7532\n",
      "128/424, train_loss: 0.7754\n",
      "129/424, train_loss: 0.7601\n",
      "130/424, train_loss: 0.7598\n",
      "131/424, train_loss: 0.7789\n",
      "132/424, train_loss: 0.7211\n",
      "133/424, train_loss: 0.7020\n",
      "134/424, train_loss: 0.8055\n",
      "135/424, train_loss: 0.7256\n",
      "136/424, train_loss: 0.6973\n",
      "137/424, train_loss: 0.6285\n",
      "138/424, train_loss: 0.6886\n",
      "139/424, train_loss: 0.6836\n",
      "140/424, train_loss: 0.6633\n",
      "141/424, train_loss: 0.7661\n",
      "142/424, train_loss: 0.6558\n",
      "143/424, train_loss: 0.6327\n",
      "144/424, train_loss: 0.6501\n",
      "145/424, train_loss: 0.6362\n",
      "146/424, train_loss: 0.7576\n",
      "147/424, train_loss: 0.7443\n",
      "148/424, train_loss: 0.6174\n",
      "149/424, train_loss: 0.7004\n",
      "150/424, train_loss: 0.6505\n",
      "151/424, train_loss: 0.6490\n",
      "152/424, train_loss: 0.7712\n",
      "153/424, train_loss: 0.7676\n",
      "154/424, train_loss: 0.6326\n",
      "155/424, train_loss: 0.6371\n",
      "156/424, train_loss: 0.6978\n",
      "157/424, train_loss: 0.7397\n",
      "158/424, train_loss: 0.6162\n",
      "159/424, train_loss: 0.6844\n",
      "160/424, train_loss: 0.6217\n",
      "161/424, train_loss: 0.7635\n",
      "162/424, train_loss: 0.5885\n",
      "163/424, train_loss: 0.6442\n",
      "164/424, train_loss: 0.5578\n",
      "165/424, train_loss: 0.7016\n",
      "166/424, train_loss: 0.4768\n",
      "167/424, train_loss: 0.6959\n",
      "168/424, train_loss: 0.7535\n",
      "169/424, train_loss: 0.9858\n",
      "170/424, train_loss: 0.6462\n",
      "171/424, train_loss: 0.7723\n",
      "172/424, train_loss: 0.6958\n",
      "173/424, train_loss: 0.7280\n",
      "174/424, train_loss: 0.6909\n",
      "175/424, train_loss: 0.6858\n",
      "176/424, train_loss: 0.6617\n",
      "177/424, train_loss: 0.6409\n",
      "178/424, train_loss: 0.6853\n",
      "179/424, train_loss: 0.5913\n",
      "180/424, train_loss: 0.6044\n",
      "181/424, train_loss: 0.7257\n",
      "182/424, train_loss: 0.8549\n",
      "183/424, train_loss: 0.8130\n",
      "184/424, train_loss: 0.6030\n",
      "185/424, train_loss: 0.7051\n",
      "186/424, train_loss: 0.7548\n",
      "187/424, train_loss: 0.6691\n",
      "188/424, train_loss: 0.6545\n",
      "189/424, train_loss: 0.6249\n",
      "190/424, train_loss: 0.9260\n",
      "191/424, train_loss: 0.5353\n",
      "192/424, train_loss: 0.7294\n",
      "193/424, train_loss: 0.5460\n",
      "194/424, train_loss: 0.7277\n",
      "195/424, train_loss: 1.0506\n",
      "196/424, train_loss: 0.5731\n",
      "197/424, train_loss: 0.5364\n",
      "198/424, train_loss: 0.8790\n",
      "199/424, train_loss: 0.9865\n",
      "200/424, train_loss: 0.6978\n",
      "201/424, train_loss: 0.6323\n",
      "202/424, train_loss: 0.4891\n",
      "203/424, train_loss: 0.8436\n",
      "204/424, train_loss: 0.5684\n",
      "205/424, train_loss: 0.7743\n",
      "206/424, train_loss: 0.5776\n",
      "207/424, train_loss: 0.6969\n",
      "208/424, train_loss: 0.6660\n",
      "209/424, train_loss: 0.5688\n",
      "210/424, train_loss: 0.8086\n",
      "211/424, train_loss: 0.6265\n",
      "212/424, train_loss: 0.6087\n",
      "213/424, train_loss: 0.9834\n",
      "214/424, train_loss: 0.7849\n",
      "215/424, train_loss: 0.6975\n",
      "216/424, train_loss: 0.6111\n",
      "217/424, train_loss: 0.7572\n",
      "218/424, train_loss: 0.6545\n",
      "219/424, train_loss: 0.6981\n",
      "220/424, train_loss: 0.6731\n",
      "221/424, train_loss: 0.6641\n",
      "222/424, train_loss: 0.6727\n",
      "223/424, train_loss: 0.6945\n",
      "224/424, train_loss: 0.7493\n",
      "225/424, train_loss: 0.7362\n",
      "226/424, train_loss: 0.8112\n",
      "227/424, train_loss: 0.6737\n",
      "228/424, train_loss: 0.6698\n",
      "229/424, train_loss: 0.6342\n",
      "230/424, train_loss: 0.6787\n",
      "231/424, train_loss: 0.6145\n",
      "232/424, train_loss: 0.9030\n",
      "233/424, train_loss: 0.5739\n",
      "234/424, train_loss: 0.7281\n",
      "235/424, train_loss: 0.8417\n",
      "236/424, train_loss: 0.5403\n",
      "237/424, train_loss: 0.6680\n",
      "238/424, train_loss: 0.7689\n",
      "239/424, train_loss: 0.7764\n",
      "240/424, train_loss: 0.5944\n",
      "241/424, train_loss: 0.7655\n",
      "242/424, train_loss: 0.6489\n",
      "243/424, train_loss: 0.7416\n",
      "244/424, train_loss: 0.6992\n",
      "245/424, train_loss: 0.7744\n",
      "246/424, train_loss: 0.7926\n",
      "247/424, train_loss: 0.6481\n",
      "248/424, train_loss: 0.7769\n",
      "249/424, train_loss: 0.7058\n",
      "250/424, train_loss: 0.6677\n",
      "251/424, train_loss: 0.8068\n",
      "252/424, train_loss: 0.7600\n",
      "253/424, train_loss: 0.6891\n",
      "254/424, train_loss: 0.6526\n",
      "255/424, train_loss: 0.6163\n",
      "256/424, train_loss: 0.6400\n",
      "257/424, train_loss: 0.7280\n",
      "258/424, train_loss: 0.6877\n",
      "259/424, train_loss: 0.6124\n",
      "260/424, train_loss: 0.5204\n",
      "261/424, train_loss: 0.3321\n",
      "262/424, train_loss: 0.7900\n",
      "263/424, train_loss: 1.1738\n",
      "264/424, train_loss: 1.1151\n",
      "265/424, train_loss: 0.5649\n",
      "266/424, train_loss: 0.5328\n",
      "267/424, train_loss: 0.8137\n",
      "268/424, train_loss: 0.7663\n",
      "269/424, train_loss: 0.5454\n",
      "270/424, train_loss: 0.6448\n",
      "271/424, train_loss: 0.5439\n",
      "272/424, train_loss: 0.6101\n",
      "273/424, train_loss: 0.5466\n",
      "274/424, train_loss: 0.5767\n",
      "275/424, train_loss: 0.8949\n",
      "276/424, train_loss: 0.7969\n",
      "277/424, train_loss: 0.6134\n",
      "278/424, train_loss: 0.6651\n",
      "279/424, train_loss: 0.6825\n",
      "280/424, train_loss: 0.7644\n",
      "281/424, train_loss: 0.6866\n",
      "282/424, train_loss: 0.7115\n",
      "283/424, train_loss: 0.7067\n",
      "284/424, train_loss: 0.6598\n",
      "285/424, train_loss: 0.7729\n",
      "286/424, train_loss: 0.7495\n",
      "287/424, train_loss: 0.6218\n",
      "288/424, train_loss: 0.6049\n",
      "289/424, train_loss: 0.7762\n",
      "290/424, train_loss: 0.6475\n",
      "291/424, train_loss: 0.5742\n",
      "292/424, train_loss: 0.8401\n",
      "293/424, train_loss: 0.8411\n",
      "294/424, train_loss: 0.7010\n",
      "295/424, train_loss: 0.6904\n",
      "296/424, train_loss: 0.8658\n",
      "297/424, train_loss: 0.7426\n",
      "298/424, train_loss: 0.7094\n",
      "299/424, train_loss: 0.6694\n",
      "300/424, train_loss: 0.7030\n",
      "301/424, train_loss: 0.7140\n",
      "302/424, train_loss: 0.5806\n",
      "303/424, train_loss: 0.7489\n",
      "304/424, train_loss: 0.7976\n",
      "305/424, train_loss: 0.5623\n",
      "306/424, train_loss: 0.9872\n",
      "307/424, train_loss: 0.8048\n",
      "308/424, train_loss: 0.8121\n",
      "309/424, train_loss: 0.6403\n",
      "310/424, train_loss: 0.7245\n",
      "311/424, train_loss: 0.6743\n",
      "312/424, train_loss: 0.6949\n",
      "313/424, train_loss: 0.6732\n",
      "314/424, train_loss: 0.6940\n",
      "315/424, train_loss: 0.6200\n",
      "316/424, train_loss: 0.6775\n",
      "317/424, train_loss: 0.7487\n",
      "318/424, train_loss: 0.6927\n",
      "319/424, train_loss: 0.6478\n",
      "320/424, train_loss: 0.6451\n",
      "321/424, train_loss: 0.6718\n",
      "322/424, train_loss: 0.7025\n",
      "323/424, train_loss: 0.6913\n",
      "324/424, train_loss: 0.6761\n",
      "325/424, train_loss: 0.6469\n",
      "326/424, train_loss: 0.6400\n",
      "327/424, train_loss: 0.7789\n",
      "328/424, train_loss: 0.6185\n",
      "329/424, train_loss: 0.8016\n",
      "330/424, train_loss: 0.6276\n",
      "331/424, train_loss: 0.8002\n",
      "332/424, train_loss: 0.7123\n",
      "333/424, train_loss: 0.6757\n",
      "334/424, train_loss: 0.6733\n",
      "335/424, train_loss: 0.6675\n",
      "336/424, train_loss: 0.7079\n",
      "337/424, train_loss: 0.6532\n",
      "338/424, train_loss: 0.6670\n",
      "339/424, train_loss: 0.6929\n",
      "340/424, train_loss: 0.6978\n",
      "341/424, train_loss: 0.7439\n",
      "342/424, train_loss: 0.7197\n",
      "343/424, train_loss: 0.6979\n",
      "344/424, train_loss: 0.7128\n",
      "345/424, train_loss: 0.7074\n",
      "346/424, train_loss: 0.6475\n",
      "347/424, train_loss: 0.7280\n",
      "348/424, train_loss: 0.6888\n",
      "349/424, train_loss: 0.7238\n",
      "350/424, train_loss: 0.6750\n",
      "351/424, train_loss: 0.6426\n",
      "352/424, train_loss: 0.7784\n",
      "353/424, train_loss: 0.7359\n",
      "354/424, train_loss: 0.6521\n",
      "355/424, train_loss: 0.6102\n",
      "356/424, train_loss: 0.5275\n",
      "357/424, train_loss: 0.8528\n",
      "358/424, train_loss: 0.7196\n",
      "359/424, train_loss: 0.6958\n",
      "360/424, train_loss: 0.8668\n",
      "361/424, train_loss: 0.4802\n",
      "362/424, train_loss: 0.6152\n",
      "363/424, train_loss: 0.5954\n",
      "364/424, train_loss: 0.7029\n",
      "365/424, train_loss: 0.8477\n",
      "366/424, train_loss: 0.6838\n",
      "367/424, train_loss: 0.8600\n",
      "368/424, train_loss: 0.6474\n",
      "369/424, train_loss: 0.6409\n",
      "370/424, train_loss: 0.6936\n",
      "371/424, train_loss: 0.7626\n",
      "372/424, train_loss: 0.6702\n",
      "373/424, train_loss: 0.7186\n",
      "374/424, train_loss: 0.7410\n",
      "375/424, train_loss: 0.7183\n",
      "376/424, train_loss: 0.6730\n",
      "377/424, train_loss: 0.6694\n",
      "378/424, train_loss: 0.6509\n",
      "379/424, train_loss: 0.6907\n",
      "380/424, train_loss: 0.6897\n",
      "381/424, train_loss: 0.6613\n",
      "382/424, train_loss: 0.6600\n",
      "383/424, train_loss: 0.7611\n",
      "384/424, train_loss: 0.7388\n",
      "385/424, train_loss: 0.7111\n",
      "386/424, train_loss: 0.6781\n",
      "387/424, train_loss: 0.7454\n",
      "388/424, train_loss: 0.7202\n",
      "389/424, train_loss: 0.6948\n",
      "390/424, train_loss: 0.7246\n",
      "391/424, train_loss: 0.7083\n",
      "392/424, train_loss: 0.6444\n",
      "393/424, train_loss: 0.7361\n",
      "394/424, train_loss: 0.6858\n",
      "395/424, train_loss: 0.7327\n",
      "396/424, train_loss: 0.6671\n",
      "397/424, train_loss: 0.6996\n",
      "398/424, train_loss: 0.7002\n",
      "399/424, train_loss: 0.7588\n",
      "400/424, train_loss: 0.7300\n",
      "401/424, train_loss: 0.6832\n",
      "402/424, train_loss: 0.6793\n",
      "403/424, train_loss: 0.7028\n",
      "404/424, train_loss: 0.7130\n",
      "405/424, train_loss: 0.6787\n",
      "406/424, train_loss: 0.7040\n",
      "407/424, train_loss: 0.6290\n",
      "408/424, train_loss: 0.6997\n",
      "409/424, train_loss: 0.7365\n",
      "410/424, train_loss: 0.7502\n",
      "411/424, train_loss: 0.7235\n",
      "412/424, train_loss: 0.7161\n",
      "413/424, train_loss: 0.7023\n",
      "414/424, train_loss: 0.6766\n",
      "415/424, train_loss: 0.7556\n",
      "416/424, train_loss: 0.8087\n",
      "417/424, train_loss: 0.6437\n",
      "418/424, train_loss: 0.7126\n",
      "419/424, train_loss: 0.6602\n",
      "420/424, train_loss: 0.7378\n",
      "421/424, train_loss: 0.7032\n",
      "422/424, train_loss: 0.7222\n",
      "423/424, train_loss: 0.6732\n",
      "424/424, train_loss: 0.6560\n",
      "425/424, train_loss: 0.6023\n",
      "epoch 4 average loss: 0.6995\n",
      "Epoch time duration: 566.5901916027069\n",
      "current epoch: 4 current accuracy: 0.4749 best accuracy: 0.4749 at epoch 2\n",
      "----------\n",
      "epoch 5\n",
      "1/424, train_loss: 0.6836\n",
      "2/424, train_loss: 0.7306\n",
      "3/424, train_loss: 0.8652\n",
      "4/424, train_loss: 0.8793\n",
      "5/424, train_loss: 0.8404\n",
      "6/424, train_loss: 0.8386\n",
      "7/424, train_loss: 0.6926\n",
      "8/424, train_loss: 0.7964\n",
      "9/424, train_loss: 0.7510\n",
      "10/424, train_loss: 0.6796\n",
      "11/424, train_loss: 0.5829\n",
      "12/424, train_loss: 0.6815\n",
      "13/424, train_loss: 0.7939\n",
      "14/424, train_loss: 0.7258\n",
      "15/424, train_loss: 0.8869\n",
      "16/424, train_loss: 0.3679\n",
      "17/424, train_loss: 0.6085\n",
      "18/424, train_loss: 0.8257\n",
      "19/424, train_loss: 0.6034\n",
      "20/424, train_loss: 0.7062\n",
      "21/424, train_loss: 0.7330\n",
      "22/424, train_loss: 0.5374\n",
      "23/424, train_loss: 1.0383\n",
      "24/424, train_loss: 0.7839\n",
      "25/424, train_loss: 0.8018\n",
      "26/424, train_loss: 0.6127\n",
      "27/424, train_loss: 0.7023\n",
      "28/424, train_loss: 0.6799\n",
      "29/424, train_loss: 0.7042\n",
      "30/424, train_loss: 0.8043\n",
      "31/424, train_loss: 0.7242\n",
      "32/424, train_loss: 0.7020\n",
      "33/424, train_loss: 0.6432\n",
      "34/424, train_loss: 0.7933\n",
      "35/424, train_loss: 0.7669\n",
      "36/424, train_loss: 0.7738\n",
      "37/424, train_loss: 0.8886\n",
      "38/424, train_loss: 0.7396\n",
      "39/424, train_loss: 0.7403\n",
      "40/424, train_loss: 0.6038\n",
      "41/424, train_loss: 0.8721\n",
      "42/424, train_loss: 0.9216\n",
      "43/424, train_loss: 0.6324\n",
      "44/424, train_loss: 0.7185\n",
      "45/424, train_loss: 0.6692\n",
      "46/424, train_loss: 0.7168\n",
      "47/424, train_loss: 0.6953\n",
      "48/424, train_loss: 0.7002\n",
      "49/424, train_loss: 0.6565\n",
      "50/424, train_loss: 0.6923\n",
      "51/424, train_loss: 0.6670\n",
      "52/424, train_loss: 0.6742\n",
      "53/424, train_loss: 0.6663\n",
      "54/424, train_loss: 0.6823\n",
      "55/424, train_loss: 0.6649\n",
      "56/424, train_loss: 0.6591\n",
      "57/424, train_loss: 0.6190\n",
      "58/424, train_loss: 0.7017\n",
      "59/424, train_loss: 0.8042\n",
      "60/424, train_loss: 0.6674\n",
      "61/424, train_loss: 0.7479\n",
      "62/424, train_loss: 0.5910\n",
      "63/424, train_loss: 0.6992\n",
      "64/424, train_loss: 0.7235\n",
      "65/424, train_loss: 0.8396\n",
      "66/424, train_loss: 0.7253\n",
      "67/424, train_loss: 0.6909\n",
      "68/424, train_loss: 0.6710\n",
      "69/424, train_loss: 0.6388\n",
      "70/424, train_loss: 0.6637\n",
      "71/424, train_loss: 0.7667\n",
      "72/424, train_loss: 0.7366\n",
      "73/424, train_loss: 0.6846\n",
      "74/424, train_loss: 0.6246\n",
      "75/424, train_loss: 0.5413\n",
      "76/424, train_loss: 0.5987\n",
      "77/424, train_loss: 0.8964\n",
      "78/424, train_loss: 0.7145\n",
      "79/424, train_loss: 0.8052\n",
      "80/424, train_loss: 0.9897\n",
      "81/424, train_loss: 0.7201\n",
      "82/424, train_loss: 0.7507\n",
      "83/424, train_loss: 0.6941\n",
      "84/424, train_loss: 0.6779\n",
      "85/424, train_loss: 0.6976\n",
      "86/424, train_loss: 0.6764\n",
      "87/424, train_loss: 0.6748\n",
      "88/424, train_loss: 0.6836\n",
      "89/424, train_loss: 0.6533\n",
      "90/424, train_loss: 0.6432\n",
      "91/424, train_loss: 0.7844\n",
      "92/424, train_loss: 0.5914\n",
      "93/424, train_loss: 0.7415\n",
      "94/424, train_loss: 0.7125\n",
      "95/424, train_loss: 0.6361\n",
      "96/424, train_loss: 0.5820\n",
      "97/424, train_loss: 0.9215\n",
      "98/424, train_loss: 0.7748\n",
      "99/424, train_loss: 0.5899\n",
      "100/424, train_loss: 0.5756\n",
      "101/424, train_loss: 0.6786\n",
      "102/424, train_loss: 0.8561\n",
      "103/424, train_loss: 0.8641\n",
      "104/424, train_loss: 0.5886\n",
      "105/424, train_loss: 0.7228\n",
      "106/424, train_loss: 0.5010\n",
      "107/424, train_loss: 0.6327\n",
      "108/424, train_loss: 0.7806\n",
      "109/424, train_loss: 0.8249\n",
      "110/424, train_loss: 0.5773\n",
      "111/424, train_loss: 0.5921\n",
      "112/424, train_loss: 0.7129\n",
      "113/424, train_loss: 0.7497\n",
      "114/424, train_loss: 0.6768\n",
      "115/424, train_loss: 0.6099\n",
      "116/424, train_loss: 0.5616\n",
      "117/424, train_loss: 0.5858\n",
      "118/424, train_loss: 0.6771\n",
      "119/424, train_loss: 0.8694\n",
      "120/424, train_loss: 0.6823\n",
      "121/424, train_loss: 0.5146\n",
      "122/424, train_loss: 0.9100\n",
      "123/424, train_loss: 0.5371\n",
      "124/424, train_loss: 0.6893\n",
      "125/424, train_loss: 0.6378\n",
      "126/424, train_loss: 0.6964\n",
      "127/424, train_loss: 0.7045\n",
      "128/424, train_loss: 0.6341\n",
      "129/424, train_loss: 0.6913\n",
      "130/424, train_loss: 0.8790\n",
      "131/424, train_loss: 0.7299\n",
      "132/424, train_loss: 0.6285\n",
      "133/424, train_loss: 0.7250\n",
      "134/424, train_loss: 0.7156\n",
      "135/424, train_loss: 0.7151\n",
      "136/424, train_loss: 0.6892\n",
      "137/424, train_loss: 0.6570\n",
      "138/424, train_loss: 0.6473\n",
      "139/424, train_loss: 0.7086\n",
      "140/424, train_loss: 0.6868\n",
      "141/424, train_loss: 0.6190\n",
      "142/424, train_loss: 0.5744\n",
      "143/424, train_loss: 0.8535\n",
      "144/424, train_loss: 0.8387\n",
      "145/424, train_loss: 0.6436\n",
      "146/424, train_loss: 0.7073\n",
      "147/424, train_loss: 0.7346\n",
      "148/424, train_loss: 0.6402\n",
      "149/424, train_loss: 0.7850\n",
      "150/424, train_loss: 0.5253\n",
      "151/424, train_loss: 0.5062\n",
      "152/424, train_loss: 0.8235\n",
      "153/424, train_loss: 0.6216\n",
      "154/424, train_loss: 0.4820\n",
      "155/424, train_loss: 1.0022\n",
      "156/424, train_loss: 0.6762\n",
      "157/424, train_loss: 0.6261\n",
      "158/424, train_loss: 0.4784\n",
      "159/424, train_loss: 0.7225\n",
      "160/424, train_loss: 0.4705\n",
      "161/424, train_loss: 0.9116\n",
      "162/424, train_loss: 0.6859\n",
      "163/424, train_loss: 0.6987\n",
      "164/424, train_loss: 0.5339\n",
      "165/424, train_loss: 0.6849\n",
      "166/424, train_loss: 0.8389\n",
      "167/424, train_loss: 0.8308\n",
      "168/424, train_loss: 0.7388\n",
      "169/424, train_loss: 0.5972\n",
      "170/424, train_loss: 0.6714\n",
      "171/424, train_loss: 0.7102\n",
      "172/424, train_loss: 0.6831\n",
      "173/424, train_loss: 0.7137\n",
      "174/424, train_loss: 0.7781\n",
      "175/424, train_loss: 0.7683\n",
      "176/424, train_loss: 0.5403\n",
      "177/424, train_loss: 0.5470\n",
      "178/424, train_loss: 0.7186\n",
      "179/424, train_loss: 0.7127\n",
      "180/424, train_loss: 0.7177\n",
      "181/424, train_loss: 0.4553\n",
      "182/424, train_loss: 0.7547\n",
      "183/424, train_loss: 0.8416\n",
      "184/424, train_loss: 0.9199\n",
      "185/424, train_loss: 0.8865\n",
      "186/424, train_loss: 0.5850\n",
      "187/424, train_loss: 0.5663\n",
      "188/424, train_loss: 0.6445\n",
      "189/424, train_loss: 0.7365\n",
      "190/424, train_loss: 0.5779\n",
      "191/424, train_loss: 0.6141\n",
      "192/424, train_loss: 0.4177\n",
      "193/424, train_loss: 0.7663\n",
      "194/424, train_loss: 0.5105\n",
      "195/424, train_loss: 0.8219\n",
      "196/424, train_loss: 0.6494\n",
      "197/424, train_loss: 0.6589\n",
      "198/424, train_loss: 0.8364\n",
      "199/424, train_loss: 0.6041\n",
      "200/424, train_loss: 0.6583\n",
      "201/424, train_loss: 0.6762\n",
      "202/424, train_loss: 0.8451\n",
      "203/424, train_loss: 0.4994\n",
      "204/424, train_loss: 0.7402\n",
      "205/424, train_loss: 0.7693\n",
      "206/424, train_loss: 1.0056\n",
      "207/424, train_loss: 0.6904\n",
      "208/424, train_loss: 0.6849\n",
      "209/424, train_loss: 0.6252\n",
      "210/424, train_loss: 0.7051\n",
      "211/424, train_loss: 0.6396\n",
      "212/424, train_loss: 0.7172\n",
      "213/424, train_loss: 0.6495\n",
      "214/424, train_loss: 0.6797\n",
      "215/424, train_loss: 0.8083\n",
      "216/424, train_loss: 0.5356\n",
      "217/424, train_loss: 0.7502\n",
      "218/424, train_loss: 0.7114\n",
      "219/424, train_loss: 0.7653\n",
      "220/424, train_loss: 0.6520\n",
      "221/424, train_loss: 0.6096\n",
      "222/424, train_loss: 0.7317\n",
      "223/424, train_loss: 0.6033\n",
      "224/424, train_loss: 0.5115\n",
      "225/424, train_loss: 0.5447\n",
      "226/424, train_loss: 0.4626\n",
      "227/424, train_loss: 0.7205\n",
      "228/424, train_loss: 0.8268\n",
      "229/424, train_loss: 0.6711\n",
      "230/424, train_loss: 0.8864\n",
      "231/424, train_loss: 0.7623\n",
      "232/424, train_loss: 0.5416\n",
      "233/424, train_loss: 1.2297\n",
      "234/424, train_loss: 0.6178\n",
      "235/424, train_loss: 0.8666\n",
      "236/424, train_loss: 0.7142\n",
      "237/424, train_loss: 0.8101\n",
      "238/424, train_loss: 0.5084\n",
      "239/424, train_loss: 0.5271\n",
      "240/424, train_loss: 0.7074\n",
      "241/424, train_loss: 0.5707\n",
      "242/424, train_loss: 0.5790\n",
      "243/424, train_loss: 0.8853\n",
      "244/424, train_loss: 0.8917\n",
      "245/424, train_loss: 0.4831\n",
      "246/424, train_loss: 0.6875\n",
      "247/424, train_loss: 0.5177\n",
      "248/424, train_loss: 0.6116\n",
      "249/424, train_loss: 0.6292\n",
      "250/424, train_loss: 1.0673\n",
      "251/424, train_loss: 0.9819\n",
      "252/424, train_loss: 0.7613\n",
      "253/424, train_loss: 0.5854\n",
      "254/424, train_loss: 0.7352\n",
      "255/424, train_loss: 0.7725\n",
      "256/424, train_loss: 0.6707\n",
      "257/424, train_loss: 0.5386\n",
      "258/424, train_loss: 0.7936\n",
      "259/424, train_loss: 0.6664\n",
      "260/424, train_loss: 0.7206\n",
      "261/424, train_loss: 0.6967\n",
      "262/424, train_loss: 0.5850\n",
      "263/424, train_loss: 0.8112\n",
      "264/424, train_loss: 0.6229\n",
      "265/424, train_loss: 0.7445\n",
      "266/424, train_loss: 0.6833\n",
      "267/424, train_loss: 0.7505\n",
      "268/424, train_loss: 0.5663\n",
      "269/424, train_loss: 0.5872\n",
      "270/424, train_loss: 0.8456\n",
      "271/424, train_loss: 0.5755\n",
      "272/424, train_loss: 0.7529\n",
      "273/424, train_loss: 0.5588\n",
      "274/424, train_loss: 0.6413\n",
      "275/424, train_loss: 0.6602\n",
      "276/424, train_loss: 0.6883\n",
      "277/424, train_loss: 0.6656\n",
      "278/424, train_loss: 0.6212\n",
      "279/424, train_loss: 0.6307\n",
      "280/424, train_loss: 0.9563\n",
      "281/424, train_loss: 0.5286\n",
      "282/424, train_loss: 0.7136\n",
      "283/424, train_loss: 0.6218\n",
      "284/424, train_loss: 0.7842\n",
      "285/424, train_loss: 0.5829\n",
      "286/424, train_loss: 0.5353\n",
      "287/424, train_loss: 0.9038\n",
      "288/424, train_loss: 0.6292\n",
      "289/424, train_loss: 0.5438\n",
      "290/424, train_loss: 0.6115\n",
      "291/424, train_loss: 0.4804\n",
      "292/424, train_loss: 0.5885\n",
      "293/424, train_loss: 0.7906\n",
      "294/424, train_loss: 0.7918\n",
      "295/424, train_loss: 0.5632\n",
      "296/424, train_loss: 0.4457\n",
      "297/424, train_loss: 0.5175\n",
      "298/424, train_loss: 0.8280\n",
      "299/424, train_loss: 0.6790\n",
      "300/424, train_loss: 0.4887\n",
      "301/424, train_loss: 0.8224\n",
      "302/424, train_loss: 0.4905\n",
      "303/424, train_loss: 0.7863\n",
      "304/424, train_loss: 1.0693\n",
      "305/424, train_loss: 0.8649\n",
      "306/424, train_loss: 0.5282\n",
      "307/424, train_loss: 0.8895\n",
      "308/424, train_loss: 0.6223\n",
      "309/424, train_loss: 0.6425\n",
      "310/424, train_loss: 0.7654\n",
      "311/424, train_loss: 0.7555\n",
      "312/424, train_loss: 0.7501\n",
      "313/424, train_loss: 0.7847\n",
      "314/424, train_loss: 0.6770\n",
      "315/424, train_loss: 0.7557\n",
      "316/424, train_loss: 0.6641\n",
      "317/424, train_loss: 0.6498\n",
      "318/424, train_loss: 0.6405\n",
      "319/424, train_loss: 0.6650\n",
      "320/424, train_loss: 0.5867\n",
      "321/424, train_loss: 0.6881\n",
      "322/424, train_loss: 0.9113\n",
      "323/424, train_loss: 0.6836\n",
      "324/424, train_loss: 0.8593\n",
      "325/424, train_loss: 0.5501\n",
      "326/424, train_loss: 0.7039\n",
      "327/424, train_loss: 0.8266\n",
      "328/424, train_loss: 0.8071\n",
      "329/424, train_loss: 0.9048\n",
      "330/424, train_loss: 0.5977\n",
      "331/424, train_loss: 0.6511\n",
      "332/424, train_loss: 0.5652\n",
      "333/424, train_loss: 0.7904\n",
      "334/424, train_loss: 0.7419\n",
      "335/424, train_loss: 0.6550\n",
      "336/424, train_loss: 0.6838\n",
      "337/424, train_loss: 0.7192\n",
      "338/424, train_loss: 0.8088\n",
      "339/424, train_loss: 0.6627\n",
      "340/424, train_loss: 0.7003\n",
      "341/424, train_loss: 0.6672\n",
      "342/424, train_loss: 0.6709\n",
      "343/424, train_loss: 0.6539\n",
      "344/424, train_loss: 0.6373\n",
      "345/424, train_loss: 0.7311\n",
      "346/424, train_loss: 0.6954\n",
      "347/424, train_loss: 0.7206\n",
      "348/424, train_loss: 0.7614\n",
      "349/424, train_loss: 0.7012\n",
      "350/424, train_loss: 0.7135\n",
      "351/424, train_loss: 0.6786\n",
      "352/424, train_loss: 0.6498\n",
      "353/424, train_loss: 0.7263\n",
      "354/424, train_loss: 0.7689\n",
      "355/424, train_loss: 0.7145\n",
      "356/424, train_loss: 0.6282\n",
      "357/424, train_loss: 0.6187\n",
      "358/424, train_loss: 0.8523\n",
      "359/424, train_loss: 0.5653\n",
      "360/424, train_loss: 0.6128\n",
      "361/424, train_loss: 0.8340\n",
      "362/424, train_loss: 0.5853\n",
      "363/424, train_loss: 0.6100\n",
      "364/424, train_loss: 0.5344\n",
      "365/424, train_loss: 0.6615\n",
      "366/424, train_loss: 0.6630\n",
      "367/424, train_loss: 0.8890\n",
      "368/424, train_loss: 0.6741\n",
      "369/424, train_loss: 0.7104\n",
      "370/424, train_loss: 0.8407\n",
      "371/424, train_loss: 0.6119\n",
      "372/424, train_loss: 0.7384\n",
      "373/424, train_loss: 0.5825\n",
      "374/424, train_loss: 0.6484\n",
      "375/424, train_loss: 0.5903\n",
      "376/424, train_loss: 0.6894\n",
      "377/424, train_loss: 0.6176\n",
      "378/424, train_loss: 0.7010\n",
      "379/424, train_loss: 0.6262\n",
      "380/424, train_loss: 0.6909\n",
      "381/424, train_loss: 0.7183\n",
      "382/424, train_loss: 0.5895\n",
      "383/424, train_loss: 0.8385\n",
      "384/424, train_loss: 0.5772\n",
      "385/424, train_loss: 0.7313\n",
      "386/424, train_loss: 0.9253\n",
      "387/424, train_loss: 0.7585\n",
      "388/424, train_loss: 0.7330\n",
      "389/424, train_loss: 0.6630\n",
      "390/424, train_loss: 0.6646\n",
      "391/424, train_loss: 0.7743\n",
      "392/424, train_loss: 0.6655\n",
      "393/424, train_loss: 0.7026\n",
      "394/424, train_loss: 0.7313\n",
      "395/424, train_loss: 0.7795\n",
      "396/424, train_loss: 0.6997\n",
      "397/424, train_loss: 0.6556\n",
      "398/424, train_loss: 0.6741\n",
      "399/424, train_loss: 0.6543\n",
      "400/424, train_loss: 0.6771\n",
      "401/424, train_loss: 0.7205\n",
      "402/424, train_loss: 0.7208\n",
      "403/424, train_loss: 0.6531\n",
      "404/424, train_loss: 0.7185\n",
      "405/424, train_loss: 0.7281\n",
      "406/424, train_loss: 0.7395\n",
      "407/424, train_loss: 0.6405\n",
      "408/424, train_loss: 0.7097\n",
      "409/424, train_loss: 0.6471\n",
      "410/424, train_loss: 0.6823\n",
      "411/424, train_loss: 0.6788\n",
      "412/424, train_loss: 0.7446\n",
      "413/424, train_loss: 0.6259\n",
      "414/424, train_loss: 0.6409\n",
      "415/424, train_loss: 0.5969\n",
      "416/424, train_loss: 0.7275\n",
      "417/424, train_loss: 0.7500\n",
      "418/424, train_loss: 0.5267\n",
      "419/424, train_loss: 0.7112\n",
      "420/424, train_loss: 0.7366\n",
      "421/424, train_loss: 0.5864\n",
      "422/424, train_loss: 0.8374\n",
      "423/424, train_loss: 0.5834\n",
      "424/424, train_loss: 0.6288\n",
      "425/424, train_loss: 0.4552\n",
      "epoch 5 average loss: 0.6955\n",
      "Epoch time duration: 567.8811240196228\n",
      "----------\n",
      "epoch 6\n",
      "1/424, train_loss: 0.5690\n",
      "2/424, train_loss: 0.5462\n",
      "3/424, train_loss: 0.6921\n",
      "4/424, train_loss: 0.5419\n",
      "5/424, train_loss: 1.0101\n",
      "6/424, train_loss: 0.3691\n",
      "7/424, train_loss: 0.3640\n",
      "8/424, train_loss: 0.8235\n",
      "9/424, train_loss: 0.7832\n",
      "10/424, train_loss: 0.5424\n",
      "11/424, train_loss: 0.7919\n",
      "12/424, train_loss: 0.9789\n",
      "13/424, train_loss: 0.7410\n",
      "14/424, train_loss: 0.5752\n",
      "15/424, train_loss: 0.8118\n",
      "16/424, train_loss: 0.7363\n",
      "17/424, train_loss: 0.6095\n",
      "18/424, train_loss: 0.6611\n",
      "19/424, train_loss: 0.6217\n",
      "20/424, train_loss: 0.6687\n",
      "21/424, train_loss: 0.7805\n",
      "22/424, train_loss: 0.6168\n",
      "23/424, train_loss: 0.7529\n",
      "24/424, train_loss: 0.6825\n",
      "25/424, train_loss: 0.6888\n",
      "26/424, train_loss: 0.7588\n",
      "27/424, train_loss: 0.6255\n",
      "28/424, train_loss: 0.7759\n",
      "29/424, train_loss: 0.6492\n",
      "30/424, train_loss: 0.7385\n",
      "31/424, train_loss: 0.7003\n",
      "32/424, train_loss: 0.6768\n",
      "33/424, train_loss: 0.6893\n",
      "34/424, train_loss: 0.8636\n",
      "35/424, train_loss: 0.5893\n",
      "36/424, train_loss: 0.7386\n",
      "37/424, train_loss: 0.7837\n",
      "38/424, train_loss: 0.6478\n",
      "39/424, train_loss: 0.7051\n",
      "40/424, train_loss: 0.6420\n",
      "41/424, train_loss: 0.6685\n",
      "42/424, train_loss: 0.7513\n",
      "43/424, train_loss: 0.7180\n",
      "44/424, train_loss: 0.6085\n",
      "45/424, train_loss: 0.6604\n",
      "46/424, train_loss: 0.6746\n",
      "47/424, train_loss: 0.6460\n",
      "48/424, train_loss: 0.6549\n",
      "49/424, train_loss: 0.7469\n",
      "50/424, train_loss: 0.7315\n",
      "51/424, train_loss: 0.6000\n",
      "52/424, train_loss: 0.7991\n",
      "53/424, train_loss: 0.7382\n",
      "54/424, train_loss: 0.5742\n",
      "55/424, train_loss: 0.7971\n",
      "56/424, train_loss: 0.6793\n",
      "57/424, train_loss: 0.6328\n",
      "58/424, train_loss: 0.7217\n",
      "59/424, train_loss: 0.8011\n",
      "60/424, train_loss: 0.7158\n",
      "61/424, train_loss: 0.6212\n",
      "62/424, train_loss: 0.6479\n",
      "63/424, train_loss: 0.6032\n",
      "64/424, train_loss: 0.6796\n",
      "65/424, train_loss: 0.6141\n",
      "66/424, train_loss: 0.7949\n",
      "67/424, train_loss: 1.1519\n",
      "68/424, train_loss: 0.7916\n",
      "69/424, train_loss: 0.8089\n",
      "70/424, train_loss: 0.7507\n",
      "71/424, train_loss: 0.6713\n",
      "72/424, train_loss: 0.6948\n",
      "73/424, train_loss: 0.7150\n",
      "74/424, train_loss: 0.8925\n",
      "75/424, train_loss: 0.8064\n",
      "76/424, train_loss: 0.6044\n",
      "77/424, train_loss: 0.6455\n",
      "78/424, train_loss: 0.7793\n",
      "79/424, train_loss: 0.6184\n",
      "80/424, train_loss: 0.5963\n",
      "81/424, train_loss: 0.4798\n",
      "82/424, train_loss: 0.5959\n",
      "83/424, train_loss: 0.9550\n",
      "84/424, train_loss: 0.5836\n",
      "85/424, train_loss: 1.2370\n",
      "86/424, train_loss: 0.8231\n",
      "87/424, train_loss: 1.0200\n",
      "88/424, train_loss: 0.7782\n",
      "89/424, train_loss: 0.9091\n",
      "90/424, train_loss: 0.7465\n",
      "91/424, train_loss: 0.8126\n",
      "92/424, train_loss: 0.6561\n",
      "93/424, train_loss: 0.5377\n",
      "94/424, train_loss: 0.6273\n",
      "95/424, train_loss: 0.7371\n",
      "96/424, train_loss: 0.7744\n",
      "97/424, train_loss: 0.7434\n",
      "98/424, train_loss: 0.7089\n",
      "99/424, train_loss: 0.6720\n",
      "100/424, train_loss: 0.6756\n",
      "101/424, train_loss: 0.7618\n",
      "102/424, train_loss: 0.6982\n",
      "103/424, train_loss: 0.6290\n",
      "104/424, train_loss: 0.7016\n",
      "105/424, train_loss: 0.7094\n",
      "106/424, train_loss: 0.6150\n",
      "107/424, train_loss: 0.6676\n",
      "108/424, train_loss: 0.6909\n",
      "109/424, train_loss: 0.7268\n",
      "110/424, train_loss: 0.7611\n",
      "111/424, train_loss: 0.6906\n",
      "112/424, train_loss: 0.6769\n",
      "113/424, train_loss: 0.6443\n",
      "114/424, train_loss: 0.6471\n",
      "115/424, train_loss: 0.7701\n",
      "116/424, train_loss: 0.6525\n",
      "117/424, train_loss: 0.7559\n",
      "118/424, train_loss: 0.6224\n",
      "119/424, train_loss: 0.5687\n",
      "120/424, train_loss: 0.8560\n",
      "121/424, train_loss: 0.6623\n",
      "122/424, train_loss: 0.6214\n",
      "123/424, train_loss: 0.7560\n",
      "124/424, train_loss: 0.6375\n",
      "125/424, train_loss: 0.9032\n",
      "126/424, train_loss: 0.5060\n",
      "127/424, train_loss: 0.7821\n",
      "128/424, train_loss: 0.4342\n",
      "129/424, train_loss: 0.7767\n",
      "130/424, train_loss: 0.7804\n",
      "131/424, train_loss: 0.5597\n",
      "132/424, train_loss: 0.8872\n",
      "133/424, train_loss: 0.5807\n",
      "134/424, train_loss: 0.7760\n",
      "135/424, train_loss: 0.9421\n",
      "136/424, train_loss: 0.6870\n",
      "137/424, train_loss: 0.6071\n",
      "138/424, train_loss: 0.6412\n",
      "139/424, train_loss: 0.6920\n",
      "140/424, train_loss: 0.6262\n",
      "141/424, train_loss: 0.6901\n",
      "142/424, train_loss: 0.6527\n",
      "143/424, train_loss: 0.6277\n",
      "144/424, train_loss: 0.7679\n",
      "145/424, train_loss: 0.5362\n",
      "146/424, train_loss: 0.9554\n",
      "147/424, train_loss: 0.6706\n",
      "148/424, train_loss: 0.8029\n",
      "149/424, train_loss: 0.7917\n",
      "150/424, train_loss: 0.6682\n",
      "151/424, train_loss: 0.6089\n",
      "152/424, train_loss: 0.6860\n",
      "153/424, train_loss: 0.6754\n",
      "154/424, train_loss: 0.6033\n",
      "155/424, train_loss: 0.6143\n",
      "156/424, train_loss: 0.7904\n",
      "157/424, train_loss: 0.6364\n",
      "158/424, train_loss: 0.6210\n",
      "159/424, train_loss: 0.6856\n",
      "160/424, train_loss: 0.6896\n",
      "161/424, train_loss: 0.7507\n",
      "162/424, train_loss: 0.7882\n",
      "163/424, train_loss: 0.5919\n",
      "164/424, train_loss: 0.6461\n",
      "165/424, train_loss: 0.6963\n",
      "166/424, train_loss: 0.6991\n",
      "167/424, train_loss: 0.5667\n",
      "168/424, train_loss: 0.7030\n",
      "169/424, train_loss: 0.6735\n",
      "170/424, train_loss: 0.6744\n",
      "171/424, train_loss: 0.7162\n",
      "172/424, train_loss: 0.6874\n",
      "173/424, train_loss: 0.6888\n",
      "174/424, train_loss: 0.6899\n",
      "175/424, train_loss: 0.5731\n",
      "176/424, train_loss: 0.7123\n",
      "177/424, train_loss: 0.6022\n",
      "178/424, train_loss: 0.6906\n",
      "179/424, train_loss: 0.6738\n",
      "180/424, train_loss: 0.6102\n",
      "181/424, train_loss: 0.6875\n",
      "182/424, train_loss: 0.5647\n",
      "183/424, train_loss: 0.7291\n",
      "184/424, train_loss: 0.4594\n",
      "185/424, train_loss: 0.8275\n",
      "186/424, train_loss: 0.8636\n",
      "187/424, train_loss: 0.7128\n",
      "188/424, train_loss: 0.4728\n",
      "189/424, train_loss: 0.9465\n",
      "190/424, train_loss: 0.8091\n",
      "191/424, train_loss: 0.7507\n",
      "192/424, train_loss: 0.7590\n",
      "193/424, train_loss: 0.7185\n",
      "194/424, train_loss: 0.6060\n",
      "195/424, train_loss: 0.7234\n",
      "196/424, train_loss: 0.9874\n",
      "197/424, train_loss: 0.6337\n",
      "198/424, train_loss: 0.5853\n",
      "199/424, train_loss: 1.0223\n",
      "200/424, train_loss: 0.7882\n",
      "201/424, train_loss: 0.7408\n",
      "202/424, train_loss: 0.5294\n",
      "203/424, train_loss: 0.5886\n",
      "204/424, train_loss: 0.9633\n",
      "205/424, train_loss: 0.6954\n",
      "206/424, train_loss: 1.0856\n",
      "207/424, train_loss: 0.9573\n",
      "208/424, train_loss: 0.8257\n",
      "209/424, train_loss: 0.7111\n",
      "210/424, train_loss: 0.5589\n",
      "211/424, train_loss: 0.6542\n",
      "212/424, train_loss: 0.6731\n",
      "213/424, train_loss: 0.9811\n",
      "214/424, train_loss: 0.8475\n",
      "215/424, train_loss: 0.9548\n",
      "216/424, train_loss: 0.7735\n",
      "217/424, train_loss: 0.5146\n",
      "218/424, train_loss: 0.8650\n",
      "219/424, train_loss: 0.8755\n",
      "220/424, train_loss: 0.5394\n",
      "221/424, train_loss: 0.6383\n",
      "222/424, train_loss: 0.6980\n",
      "223/424, train_loss: 0.6165\n",
      "224/424, train_loss: 0.6134\n",
      "225/424, train_loss: 0.7882\n",
      "226/424, train_loss: 0.6619\n",
      "227/424, train_loss: 0.6874\n",
      "228/424, train_loss: 0.7462\n",
      "229/424, train_loss: 0.7988\n",
      "230/424, train_loss: 0.7440\n",
      "231/424, train_loss: 0.6948\n",
      "232/424, train_loss: 0.6653\n",
      "233/424, train_loss: 0.6705\n",
      "234/424, train_loss: 0.7194\n",
      "235/424, train_loss: 0.7834\n",
      "236/424, train_loss: 0.6868\n",
      "237/424, train_loss: 0.7789\n",
      "238/424, train_loss: 0.7101\n",
      "239/424, train_loss: 0.7665\n",
      "240/424, train_loss: 0.8439\n",
      "241/424, train_loss: 0.6857\n",
      "242/424, train_loss: 0.6588\n",
      "243/424, train_loss: 0.7028\n",
      "244/424, train_loss: 0.5965\n",
      "245/424, train_loss: 0.7717\n",
      "246/424, train_loss: 0.8920\n",
      "247/424, train_loss: 0.5569\n",
      "248/424, train_loss: 0.4796\n",
      "249/424, train_loss: 0.5905\n",
      "250/424, train_loss: 0.5710\n",
      "251/424, train_loss: 0.5554\n",
      "252/424, train_loss: 0.4582\n",
      "253/424, train_loss: 0.8686\n",
      "254/424, train_loss: 0.6500\n",
      "255/424, train_loss: 0.2763\n",
      "256/424, train_loss: 0.6394\n",
      "257/424, train_loss: 0.8779\n",
      "258/424, train_loss: 0.2722\n",
      "259/424, train_loss: 0.5119\n",
      "260/424, train_loss: 0.9995\n",
      "261/424, train_loss: 0.2453\n",
      "262/424, train_loss: 0.8498\n",
      "263/424, train_loss: 0.7860\n",
      "264/424, train_loss: 0.7770\n",
      "265/424, train_loss: 0.5660\n",
      "266/424, train_loss: 1.1013\n",
      "267/424, train_loss: 0.5501\n",
      "268/424, train_loss: 0.9103\n",
      "269/424, train_loss: 0.8458\n",
      "270/424, train_loss: 0.7070\n",
      "271/424, train_loss: 0.8925\n",
      "272/424, train_loss: 0.7850\n",
      "273/424, train_loss: 0.6842\n",
      "274/424, train_loss: 0.6267\n",
      "275/424, train_loss: 0.6815\n",
      "276/424, train_loss: 0.8063\n",
      "277/424, train_loss: 0.5696\n",
      "278/424, train_loss: 0.7473\n",
      "279/424, train_loss: 0.7451\n",
      "280/424, train_loss: 0.8807\n",
      "281/424, train_loss: 0.8837\n",
      "282/424, train_loss: 0.6802\n",
      "283/424, train_loss: 0.5319\n",
      "284/424, train_loss: 0.7767\n",
      "285/424, train_loss: 0.8610\n",
      "286/424, train_loss: 0.7048\n",
      "287/424, train_loss: 0.6926\n",
      "288/424, train_loss: 0.7717\n",
      "289/424, train_loss: 0.7087\n",
      "290/424, train_loss: 0.8160\n",
      "291/424, train_loss: 0.5334\n",
      "292/424, train_loss: 0.5748\n",
      "293/424, train_loss: 0.5910\n",
      "294/424, train_loss: 0.7041\n",
      "295/424, train_loss: 0.3781\n",
      "296/424, train_loss: 0.8525\n",
      "297/424, train_loss: 1.0994\n",
      "298/424, train_loss: 0.6163\n",
      "299/424, train_loss: 0.8938\n",
      "300/424, train_loss: 0.8551\n",
      "301/424, train_loss: 0.7883\n",
      "302/424, train_loss: 0.5586\n",
      "303/424, train_loss: 0.6667\n",
      "304/424, train_loss: 0.9033\n",
      "305/424, train_loss: 0.8730\n",
      "306/424, train_loss: 0.6056\n",
      "307/424, train_loss: 0.7578\n",
      "308/424, train_loss: 0.6935\n",
      "309/424, train_loss: 0.6218\n",
      "310/424, train_loss: 0.7327\n",
      "311/424, train_loss: 0.6144\n",
      "312/424, train_loss: 0.6678\n",
      "313/424, train_loss: 0.7178\n",
      "314/424, train_loss: 0.6892\n",
      "315/424, train_loss: 0.6650\n",
      "316/424, train_loss: 0.7174\n",
      "317/424, train_loss: 0.6495\n",
      "318/424, train_loss: 0.6960\n",
      "319/424, train_loss: 0.6323\n",
      "320/424, train_loss: 0.6620\n",
      "321/424, train_loss: 0.7425\n",
      "322/424, train_loss: 0.6519\n",
      "323/424, train_loss: 0.6771\n",
      "324/424, train_loss: 0.6317\n",
      "325/424, train_loss: 0.8169\n",
      "326/424, train_loss: 0.7023\n",
      "327/424, train_loss: 0.6317\n",
      "328/424, train_loss: 0.5988\n",
      "329/424, train_loss: 0.6984\n",
      "330/424, train_loss: 0.5630\n",
      "331/424, train_loss: 0.6934\n",
      "332/424, train_loss: 0.8604\n",
      "333/424, train_loss: 0.5857\n",
      "334/424, train_loss: 0.7881\n",
      "335/424, train_loss: 0.7797\n",
      "336/424, train_loss: 0.6136\n",
      "337/424, train_loss: 0.6954\n",
      "338/424, train_loss: 0.7164\n",
      "339/424, train_loss: 0.7139\n",
      "340/424, train_loss: 0.7473\n",
      "341/424, train_loss: 0.6517\n",
      "342/424, train_loss: 0.5933\n",
      "343/424, train_loss: 0.6934\n",
      "344/424, train_loss: 0.7869\n",
      "345/424, train_loss: 0.8365\n",
      "346/424, train_loss: 0.6692\n",
      "347/424, train_loss: 0.6522\n",
      "348/424, train_loss: 0.6928\n",
      "349/424, train_loss: 0.7773\n",
      "350/424, train_loss: 0.6260\n",
      "351/424, train_loss: 0.7849\n",
      "352/424, train_loss: 0.7178\n",
      "353/424, train_loss: 0.7336\n",
      "354/424, train_loss: 0.6713\n",
      "355/424, train_loss: 0.5410\n",
      "356/424, train_loss: 0.6154\n",
      "357/424, train_loss: 0.7610\n",
      "358/424, train_loss: 0.7755\n",
      "359/424, train_loss: 0.8521\n",
      "360/424, train_loss: 0.6782\n",
      "361/424, train_loss: 0.6962\n",
      "362/424, train_loss: 0.8652\n",
      "363/424, train_loss: 0.7390\n",
      "364/424, train_loss: 0.6915\n",
      "365/424, train_loss: 0.7583\n",
      "366/424, train_loss: 0.6140\n",
      "367/424, train_loss: 0.5812\n",
      "368/424, train_loss: 0.5496\n",
      "369/424, train_loss: 0.7037\n",
      "370/424, train_loss: 0.5984\n",
      "371/424, train_loss: 0.9879\n",
      "372/424, train_loss: 0.5059\n",
      "373/424, train_loss: 0.2933\n",
      "374/424, train_loss: 1.1308\n",
      "375/424, train_loss: 0.2779\n",
      "376/424, train_loss: 0.9223\n",
      "377/424, train_loss: 0.6756\n",
      "378/424, train_loss: 0.8356\n",
      "379/424, train_loss: 0.7021\n",
      "380/424, train_loss: 0.5056\n",
      "381/424, train_loss: 0.8441\n",
      "382/424, train_loss: 0.7717\n",
      "383/424, train_loss: 0.5399\n",
      "384/424, train_loss: 0.5467\n",
      "385/424, train_loss: 0.5448\n",
      "386/424, train_loss: 0.4953\n",
      "387/424, train_loss: 0.9111\n",
      "388/424, train_loss: 0.6631\n",
      "389/424, train_loss: 0.6992\n",
      "390/424, train_loss: 0.6279\n",
      "391/424, train_loss: 0.6024\n",
      "392/424, train_loss: 0.5714\n",
      "393/424, train_loss: 0.8496\n",
      "394/424, train_loss: 0.6474\n",
      "395/424, train_loss: 0.8204\n",
      "396/424, train_loss: 0.7697\n",
      "397/424, train_loss: 0.5477\n",
      "398/424, train_loss: 0.5682\n",
      "399/424, train_loss: 0.6755\n",
      "400/424, train_loss: 0.6912\n",
      "401/424, train_loss: 0.6598\n",
      "402/424, train_loss: 0.6165\n",
      "403/424, train_loss: 0.7240\n",
      "404/424, train_loss: 0.6510\n",
      "405/424, train_loss: 0.8362\n",
      "406/424, train_loss: 0.6613\n",
      "407/424, train_loss: 0.8594\n",
      "408/424, train_loss: 0.6229\n",
      "409/424, train_loss: 0.5469\n",
      "410/424, train_loss: 0.7962\n",
      "411/424, train_loss: 0.7152\n",
      "412/424, train_loss: 0.7128\n",
      "413/424, train_loss: 0.8001\n",
      "414/424, train_loss: 0.6859\n",
      "415/424, train_loss: 0.6360\n",
      "416/424, train_loss: 0.6025\n",
      "417/424, train_loss: 0.7389\n",
      "418/424, train_loss: 0.7021\n",
      "419/424, train_loss: 0.5790\n",
      "420/424, train_loss: 0.9928\n",
      "421/424, train_loss: 0.7486\n",
      "422/424, train_loss: 0.5531\n",
      "423/424, train_loss: 0.6299\n",
      "424/424, train_loss: 0.6014\n",
      "425/424, train_loss: 0.6785\n",
      "epoch 6 average loss: 0.7022\n",
      "Epoch time duration: 568.7542140483856\n",
      "current epoch: 6 current accuracy: 0.5050 best accuracy: 0.5050 at epoch 6\n",
      "----------\n",
      "epoch 7\n",
      "1/424, train_loss: 0.6097\n",
      "2/424, train_loss: 0.6184\n",
      "3/424, train_loss: 0.6929\n",
      "4/424, train_loss: 0.6062\n",
      "5/424, train_loss: 0.5504\n",
      "6/424, train_loss: 0.5359\n",
      "7/424, train_loss: 0.5756\n",
      "8/424, train_loss: 0.5787\n",
      "9/424, train_loss: 0.4199\n",
      "10/424, train_loss: 0.7893\n",
      "11/424, train_loss: 0.6666\n",
      "12/424, train_loss: 0.8655\n",
      "13/424, train_loss: 1.0386\n",
      "14/424, train_loss: 0.5966\n",
      "15/424, train_loss: 0.3517\n",
      "16/424, train_loss: 0.5355\n",
      "17/424, train_loss: 1.2471\n",
      "18/424, train_loss: 0.9490\n",
      "19/424, train_loss: 0.8597\n",
      "20/424, train_loss: 0.8234\n",
      "21/424, train_loss: 0.7572\n",
      "22/424, train_loss: 0.6951\n",
      "23/424, train_loss: 0.5271\n",
      "24/424, train_loss: 0.8492\n",
      "25/424, train_loss: 0.6221\n",
      "26/424, train_loss: 0.5522\n",
      "27/424, train_loss: 1.0444\n",
      "28/424, train_loss: 0.8819\n",
      "29/424, train_loss: 0.8579\n",
      "30/424, train_loss: 0.9017\n",
      "31/424, train_loss: 0.6422\n",
      "32/424, train_loss: 0.6628\n",
      "33/424, train_loss: 0.6179\n",
      "34/424, train_loss: 0.7787\n",
      "35/424, train_loss: 0.5668\n",
      "36/424, train_loss: 0.6197\n",
      "37/424, train_loss: 0.7682\n",
      "38/424, train_loss: 0.7059\n",
      "39/424, train_loss: 0.8091\n",
      "40/424, train_loss: 0.7385\n",
      "41/424, train_loss: 0.6691\n",
      "42/424, train_loss: 0.6715\n",
      "43/424, train_loss: 0.7096\n",
      "44/424, train_loss: 0.7061\n",
      "45/424, train_loss: 0.6484\n",
      "46/424, train_loss: 0.7965\n",
      "47/424, train_loss: 0.6614\n",
      "48/424, train_loss: 0.4993\n",
      "49/424, train_loss: 0.7900\n",
      "50/424, train_loss: 0.7085\n",
      "51/424, train_loss: 0.7762\n",
      "52/424, train_loss: 0.7532\n",
      "53/424, train_loss: 0.7559\n",
      "54/424, train_loss: 0.7508\n",
      "55/424, train_loss: 0.6242\n",
      "56/424, train_loss: 0.6519\n",
      "57/424, train_loss: 0.5677\n",
      "58/424, train_loss: 0.7119\n",
      "59/424, train_loss: 0.6215\n",
      "60/424, train_loss: 0.6298\n",
      "61/424, train_loss: 0.6158\n",
      "62/424, train_loss: 0.7754\n",
      "63/424, train_loss: 0.7368\n",
      "64/424, train_loss: 0.6312\n",
      "65/424, train_loss: 0.6208\n",
      "66/424, train_loss: 0.6718\n",
      "67/424, train_loss: 0.8188\n",
      "68/424, train_loss: 0.5999\n",
      "69/424, train_loss: 0.8020\n",
      "70/424, train_loss: 0.6763\n",
      "71/424, train_loss: 0.6933\n",
      "72/424, train_loss: 0.6523\n",
      "73/424, train_loss: 0.7317\n",
      "74/424, train_loss: 0.6860\n",
      "75/424, train_loss: 0.7727\n",
      "76/424, train_loss: 0.7911\n",
      "77/424, train_loss: 0.7526\n",
      "78/424, train_loss: 0.6986\n",
      "79/424, train_loss: 0.6496\n",
      "80/424, train_loss: 0.6266\n",
      "81/424, train_loss: 0.6293\n",
      "82/424, train_loss: 0.6719\n",
      "83/424, train_loss: 0.6818\n",
      "84/424, train_loss: 0.7787\n",
      "85/424, train_loss: 0.6788\n",
      "86/424, train_loss: 0.6164\n",
      "87/424, train_loss: 0.5975\n",
      "88/424, train_loss: 0.5941\n",
      "89/424, train_loss: 0.7905\n",
      "90/424, train_loss: 0.6434\n",
      "91/424, train_loss: 0.6667\n",
      "92/424, train_loss: 0.6110\n",
      "93/424, train_loss: 0.6565\n",
      "94/424, train_loss: 0.6887\n",
      "95/424, train_loss: 0.5612\n",
      "96/424, train_loss: 0.6626\n",
      "97/424, train_loss: 0.6716\n",
      "98/424, train_loss: 0.6733\n",
      "99/424, train_loss: 0.9380\n",
      "100/424, train_loss: 0.5218\n",
      "101/424, train_loss: 0.7532\n",
      "102/424, train_loss: 0.4593\n",
      "103/424, train_loss: 0.6414\n",
      "104/424, train_loss: 0.7422\n",
      "105/424, train_loss: 0.7015\n",
      "106/424, train_loss: 0.7280\n",
      "107/424, train_loss: 0.7039\n",
      "108/424, train_loss: 0.5912\n",
      "109/424, train_loss: 0.8770\n",
      "110/424, train_loss: 0.6152\n",
      "111/424, train_loss: 0.6973\n",
      "112/424, train_loss: 0.6954\n",
      "113/424, train_loss: 0.6204\n",
      "114/424, train_loss: 0.7574\n",
      "115/424, train_loss: 0.6370\n",
      "116/424, train_loss: 0.6482\n",
      "117/424, train_loss: 0.6976\n",
      "118/424, train_loss: 0.6854\n",
      "119/424, train_loss: 0.6825\n",
      "120/424, train_loss: 0.7095\n",
      "121/424, train_loss: 0.7087\n",
      "122/424, train_loss: 0.7290\n",
      "123/424, train_loss: 0.6033\n",
      "124/424, train_loss: 0.9602\n",
      "125/424, train_loss: 0.6732\n",
      "126/424, train_loss: 0.6477\n",
      "127/424, train_loss: 0.6628\n",
      "128/424, train_loss: 0.9338\n",
      "129/424, train_loss: 0.8037\n",
      "130/424, train_loss: 0.7466\n",
      "131/424, train_loss: 0.7391\n",
      "132/424, train_loss: 0.8607\n",
      "133/424, train_loss: 0.5870\n",
      "134/424, train_loss: 0.7072\n",
      "135/424, train_loss: 0.5760\n",
      "136/424, train_loss: 0.6007\n",
      "137/424, train_loss: 0.6793\n",
      "138/424, train_loss: 0.6733\n",
      "139/424, train_loss: 0.6464\n",
      "140/424, train_loss: 0.6164\n",
      "141/424, train_loss: 0.7003\n",
      "142/424, train_loss: 0.8145\n",
      "143/424, train_loss: 0.8188\n",
      "144/424, train_loss: 0.7616\n",
      "145/424, train_loss: 0.6376\n",
      "146/424, train_loss: 0.7540\n",
      "147/424, train_loss: 0.7495\n",
      "148/424, train_loss: 0.6099\n",
      "149/424, train_loss: 0.6442\n",
      "150/424, train_loss: 0.6622\n",
      "151/424, train_loss: 0.6503\n",
      "152/424, train_loss: 0.6968\n",
      "153/424, train_loss: 0.6981\n",
      "154/424, train_loss: 0.7345\n",
      "155/424, train_loss: 0.6901\n",
      "156/424, train_loss: 0.7480\n",
      "157/424, train_loss: 0.5981\n",
      "158/424, train_loss: 0.5634\n",
      "159/424, train_loss: 0.8154\n",
      "160/424, train_loss: 0.4797\n",
      "161/424, train_loss: 0.5860\n",
      "162/424, train_loss: 0.6423\n",
      "163/424, train_loss: 0.6272\n",
      "164/424, train_loss: 1.2926\n",
      "165/424, train_loss: 0.6424\n",
      "166/424, train_loss: 0.9930\n",
      "167/424, train_loss: 0.3962\n",
      "168/424, train_loss: 0.5707\n",
      "169/424, train_loss: 0.5974\n",
      "170/424, train_loss: 0.5880\n",
      "171/424, train_loss: 0.8285\n",
      "172/424, train_loss: 1.0428\n",
      "173/424, train_loss: 0.8425\n",
      "174/424, train_loss: 0.5717\n",
      "175/424, train_loss: 0.5106\n",
      "176/424, train_loss: 0.5176\n",
      "177/424, train_loss: 0.7153\n",
      "178/424, train_loss: 0.7759\n",
      "179/424, train_loss: 0.6182\n",
      "180/424, train_loss: 0.6567\n",
      "181/424, train_loss: 0.5955\n",
      "182/424, train_loss: 0.6616\n",
      "183/424, train_loss: 0.5933\n",
      "184/424, train_loss: 0.8547\n",
      "185/424, train_loss: 0.8900\n",
      "186/424, train_loss: 0.7054\n",
      "187/424, train_loss: 0.7041\n",
      "188/424, train_loss: 0.5689\n",
      "189/424, train_loss: 0.8091\n",
      "190/424, train_loss: 0.6389\n",
      "191/424, train_loss: 0.6472\n",
      "192/424, train_loss: 0.7520\n",
      "193/424, train_loss: 0.6478\n",
      "194/424, train_loss: 0.8583\n",
      "195/424, train_loss: 0.4720\n",
      "196/424, train_loss: 0.5265\n",
      "197/424, train_loss: 0.5458\n",
      "198/424, train_loss: 0.8463\n",
      "199/424, train_loss: 0.7375\n",
      "200/424, train_loss: 0.8102\n",
      "201/424, train_loss: 0.4120\n",
      "202/424, train_loss: 0.7360\n",
      "203/424, train_loss: 0.4815\n",
      "204/424, train_loss: 0.8210\n",
      "205/424, train_loss: 0.6612\n",
      "206/424, train_loss: 0.9976\n",
      "207/424, train_loss: 0.6993\n",
      "208/424, train_loss: 0.5247\n",
      "209/424, train_loss: 0.5700\n",
      "210/424, train_loss: 0.5850\n",
      "211/424, train_loss: 0.9308\n",
      "212/424, train_loss: 0.4780\n",
      "213/424, train_loss: 0.6373\n",
      "214/424, train_loss: 0.4322\n",
      "215/424, train_loss: 0.6053\n",
      "216/424, train_loss: 0.6339\n",
      "217/424, train_loss: 0.8019\n",
      "218/424, train_loss: 0.8602\n",
      "219/424, train_loss: 0.8002\n",
      "220/424, train_loss: 0.8164\n",
      "221/424, train_loss: 0.6111\n",
      "222/424, train_loss: 0.6332\n",
      "223/424, train_loss: 0.7820\n",
      "224/424, train_loss: 0.6153\n",
      "225/424, train_loss: 0.6106\n",
      "226/424, train_loss: 0.7253\n",
      "227/424, train_loss: 0.7347\n",
      "228/424, train_loss: 0.9877\n",
      "229/424, train_loss: 0.7016\n",
      "230/424, train_loss: 0.4745\n",
      "231/424, train_loss: 0.7660\n",
      "232/424, train_loss: 0.5098\n",
      "233/424, train_loss: 0.5566\n",
      "234/424, train_loss: 0.4461\n",
      "235/424, train_loss: 0.6234\n",
      "236/424, train_loss: 0.6505\n",
      "237/424, train_loss: 0.7255\n",
      "238/424, train_loss: 0.4710\n",
      "239/424, train_loss: 0.3820\n",
      "240/424, train_loss: 0.8408\n",
      "241/424, train_loss: 0.9476\n",
      "242/424, train_loss: 1.2201\n",
      "243/424, train_loss: 0.6282\n",
      "244/424, train_loss: 0.5183\n",
      "245/424, train_loss: 0.6811\n",
      "246/424, train_loss: 0.9462\n",
      "247/424, train_loss: 0.5345\n",
      "248/424, train_loss: 0.7960\n",
      "249/424, train_loss: 0.7765\n",
      "250/424, train_loss: 0.6313\n",
      "251/424, train_loss: 0.5132\n",
      "252/424, train_loss: 0.4951\n",
      "253/424, train_loss: 1.0232\n",
      "254/424, train_loss: 0.5609\n",
      "255/424, train_loss: 0.6572\n",
      "256/424, train_loss: 0.7805\n",
      "257/424, train_loss: 1.1025\n",
      "258/424, train_loss: 0.4956\n",
      "259/424, train_loss: 0.5498\n",
      "260/424, train_loss: 0.8595\n",
      "261/424, train_loss: 1.0523\n",
      "262/424, train_loss: 0.8029\n",
      "263/424, train_loss: 0.7228\n",
      "264/424, train_loss: 0.8479\n",
      "265/424, train_loss: 0.5770\n",
      "266/424, train_loss: 0.6943\n",
      "267/424, train_loss: 0.6789\n",
      "268/424, train_loss: 0.7491\n",
      "269/424, train_loss: 0.6479\n",
      "270/424, train_loss: 0.9215\n",
      "271/424, train_loss: 0.8228\n",
      "272/424, train_loss: 0.7354\n",
      "273/424, train_loss: 0.6653\n",
      "274/424, train_loss: 0.7457\n",
      "275/424, train_loss: 0.6823\n",
      "276/424, train_loss: 0.7859\n",
      "277/424, train_loss: 0.6801\n",
      "278/424, train_loss: 0.7954\n",
      "279/424, train_loss: 0.7225\n",
      "280/424, train_loss: 0.7286\n",
      "281/424, train_loss: 0.6664\n",
      "282/424, train_loss: 0.6390\n",
      "283/424, train_loss: 0.6445\n",
      "284/424, train_loss: 0.6913\n",
      "285/424, train_loss: 0.6796\n",
      "286/424, train_loss: 0.7318\n",
      "287/424, train_loss: 0.6827\n",
      "288/424, train_loss: 0.6514\n",
      "289/424, train_loss: 0.5528\n",
      "290/424, train_loss: 0.6313\n",
      "291/424, train_loss: 0.6014\n",
      "292/424, train_loss: 0.6028\n",
      "293/424, train_loss: 0.6331\n",
      "294/424, train_loss: 0.6854\n",
      "295/424, train_loss: 0.5379\n",
      "296/424, train_loss: 0.5364\n",
      "297/424, train_loss: 0.6742\n",
      "298/424, train_loss: 0.3707\n",
      "299/424, train_loss: 0.6810\n",
      "300/424, train_loss: 0.4949\n",
      "301/424, train_loss: 0.5894\n",
      "302/424, train_loss: 0.8646\n",
      "303/424, train_loss: 0.6732\n",
      "304/424, train_loss: 0.8420\n",
      "305/424, train_loss: 0.5999\n",
      "306/424, train_loss: 0.7139\n",
      "307/424, train_loss: 1.0688\n",
      "308/424, train_loss: 0.5056\n",
      "309/424, train_loss: 1.0300\n",
      "310/424, train_loss: 0.7901\n",
      "311/424, train_loss: 0.6833\n",
      "312/424, train_loss: 0.5716\n",
      "313/424, train_loss: 0.5898\n",
      "314/424, train_loss: 0.5622\n",
      "315/424, train_loss: 0.6486\n",
      "316/424, train_loss: 0.7046\n",
      "317/424, train_loss: 0.6061\n",
      "318/424, train_loss: 0.9026\n",
      "319/424, train_loss: 0.7350\n",
      "320/424, train_loss: 0.8074\n",
      "321/424, train_loss: 0.7758\n",
      "322/424, train_loss: 0.7922\n",
      "323/424, train_loss: 0.9446\n",
      "324/424, train_loss: 0.7614\n",
      "325/424, train_loss: 0.8169\n",
      "326/424, train_loss: 0.6599\n",
      "327/424, train_loss: 0.7100\n",
      "328/424, train_loss: 0.7216\n",
      "329/424, train_loss: 0.5803\n",
      "330/424, train_loss: 0.7183\n",
      "331/424, train_loss: 0.6276\n",
      "332/424, train_loss: 0.6412\n",
      "333/424, train_loss: 0.7386\n",
      "334/424, train_loss: 0.6228\n",
      "335/424, train_loss: 0.6953\n",
      "336/424, train_loss: 0.5889\n",
      "337/424, train_loss: 0.7233\n",
      "338/424, train_loss: 0.6542\n",
      "339/424, train_loss: 0.8140\n",
      "340/424, train_loss: 0.7274\n",
      "341/424, train_loss: 0.6897\n",
      "342/424, train_loss: 0.6998\n",
      "343/424, train_loss: 0.7778\n",
      "344/424, train_loss: 0.6381\n",
      "345/424, train_loss: 0.6666\n",
      "346/424, train_loss: 0.6927\n",
      "347/424, train_loss: 0.6020\n",
      "348/424, train_loss: 0.6643\n",
      "349/424, train_loss: 0.7351\n",
      "350/424, train_loss: 0.6261\n",
      "351/424, train_loss: 0.6238\n",
      "352/424, train_loss: 0.6863\n",
      "353/424, train_loss: 0.6512\n",
      "354/424, train_loss: 0.7195\n",
      "355/424, train_loss: 0.6605\n",
      "356/424, train_loss: 0.5354\n",
      "357/424, train_loss: 0.6545\n",
      "358/424, train_loss: 0.4872\n",
      "359/424, train_loss: 0.6220\n",
      "360/424, train_loss: 0.9888\n",
      "361/424, train_loss: 1.0606\n",
      "362/424, train_loss: 0.8060\n",
      "363/424, train_loss: 0.3527\n",
      "364/424, train_loss: 0.7421\n",
      "365/424, train_loss: 0.9667\n",
      "366/424, train_loss: 0.6375\n",
      "367/424, train_loss: 0.8550\n",
      "368/424, train_loss: 0.7507\n",
      "369/424, train_loss: 0.5507\n",
      "370/424, train_loss: 0.6096\n",
      "371/424, train_loss: 0.6729\n",
      "372/424, train_loss: 0.7189\n",
      "373/424, train_loss: 0.6414\n",
      "374/424, train_loss: 0.6667\n",
      "375/424, train_loss: 0.6138\n",
      "376/424, train_loss: 0.7576\n",
      "377/424, train_loss: 1.0006\n",
      "378/424, train_loss: 0.7300\n",
      "379/424, train_loss: 0.7141\n",
      "380/424, train_loss: 0.8171\n",
      "381/424, train_loss: 0.7864\n",
      "382/424, train_loss: 0.8790\n",
      "383/424, train_loss: 0.5061\n",
      "384/424, train_loss: 0.6683\n",
      "385/424, train_loss: 0.7406\n",
      "386/424, train_loss: 0.6362\n",
      "387/424, train_loss: 0.7307\n",
      "388/424, train_loss: 0.7318\n",
      "389/424, train_loss: 0.5579\n",
      "390/424, train_loss: 0.5508\n",
      "391/424, train_loss: 0.7479\n",
      "392/424, train_loss: 0.6561\n",
      "393/424, train_loss: 0.6873\n",
      "394/424, train_loss: 0.6469\n",
      "395/424, train_loss: 0.6589\n",
      "396/424, train_loss: 0.7620\n",
      "397/424, train_loss: 0.8831\n",
      "398/424, train_loss: 0.6453\n",
      "399/424, train_loss: 0.5701\n",
      "400/424, train_loss: 0.7683\n",
      "401/424, train_loss: 0.5607\n",
      "402/424, train_loss: 0.5925\n",
      "403/424, train_loss: 0.5427\n",
      "404/424, train_loss: 0.8293\n",
      "405/424, train_loss: 0.7118\n",
      "406/424, train_loss: 0.7163\n",
      "407/424, train_loss: 0.7429\n",
      "408/424, train_loss: 0.6605\n",
      "409/424, train_loss: 0.7943\n",
      "410/424, train_loss: 0.6419\n",
      "411/424, train_loss: 0.6080\n",
      "412/424, train_loss: 0.6696\n",
      "413/424, train_loss: 0.7075\n",
      "414/424, train_loss: 0.8783\n",
      "415/424, train_loss: 0.8281\n",
      "416/424, train_loss: 0.5340\n",
      "417/424, train_loss: 0.4753\n",
      "418/424, train_loss: 0.5247\n",
      "419/424, train_loss: 0.8638\n",
      "420/424, train_loss: 0.8967\n",
      "421/424, train_loss: 0.7429\n",
      "422/424, train_loss: 0.5718\n",
      "423/424, train_loss: 0.7112\n",
      "424/424, train_loss: 0.6176\n",
      "425/424, train_loss: 0.4658\n",
      "epoch 7 average loss: 0.6945\n",
      "Epoch time duration: 567.5266342163086\n",
      "----------\n",
      "epoch 8\n",
      "1/424, train_loss: 1.0243\n",
      "2/424, train_loss: 0.7523\n",
      "3/424, train_loss: 0.8061\n",
      "4/424, train_loss: 0.6744\n",
      "5/424, train_loss: 0.6257\n",
      "6/424, train_loss: 0.6693\n",
      "7/424, train_loss: 0.6863\n",
      "8/424, train_loss: 0.6841\n",
      "9/424, train_loss: 0.7058\n",
      "10/424, train_loss: 0.8400\n",
      "11/424, train_loss: 0.8414\n",
      "12/424, train_loss: 0.6355\n",
      "13/424, train_loss: 0.6687\n",
      "14/424, train_loss: 0.7985\n",
      "15/424, train_loss: 0.9243\n",
      "16/424, train_loss: 0.6263\n",
      "17/424, train_loss: 0.6979\n",
      "18/424, train_loss: 0.7870\n",
      "19/424, train_loss: 0.6279\n",
      "20/424, train_loss: 0.7937\n",
      "21/424, train_loss: 0.6860\n",
      "22/424, train_loss: 0.6298\n",
      "23/424, train_loss: 0.6224\n",
      "24/424, train_loss: 0.6721\n",
      "25/424, train_loss: 0.5524\n",
      "26/424, train_loss: 0.5803\n",
      "27/424, train_loss: 0.7581\n",
      "28/424, train_loss: 0.5075\n",
      "29/424, train_loss: 0.5175\n",
      "30/424, train_loss: 0.4811\n",
      "31/424, train_loss: 1.0856\n",
      "32/424, train_loss: 0.4348\n",
      "33/424, train_loss: 1.1207\n",
      "34/424, train_loss: 0.8243\n",
      "35/424, train_loss: 0.5195\n",
      "36/424, train_loss: 1.0318\n",
      "37/424, train_loss: 1.1293\n",
      "38/424, train_loss: 0.4883\n",
      "39/424, train_loss: 0.6570\n",
      "40/424, train_loss: 0.7698\n",
      "41/424, train_loss: 0.7073\n",
      "42/424, train_loss: 0.7484\n",
      "43/424, train_loss: 0.6159\n",
      "44/424, train_loss: 0.6155\n",
      "45/424, train_loss: 0.6565\n",
      "46/424, train_loss: 0.8738\n",
      "47/424, train_loss: 0.6279\n",
      "48/424, train_loss: 1.1401\n",
      "49/424, train_loss: 0.8983\n",
      "50/424, train_loss: 0.9335\n",
      "51/424, train_loss: 0.4257\n",
      "52/424, train_loss: 0.8724\n",
      "53/424, train_loss: 0.7471\n",
      "54/424, train_loss: 0.7321\n",
      "55/424, train_loss: 0.7650\n",
      "56/424, train_loss: 0.6334\n",
      "57/424, train_loss: 0.7096\n",
      "58/424, train_loss: 0.6613\n",
      "59/424, train_loss: 0.6724\n",
      "60/424, train_loss: 0.6199\n",
      "61/424, train_loss: 0.6182\n",
      "62/424, train_loss: 0.7239\n",
      "63/424, train_loss: 0.6099\n",
      "64/424, train_loss: 0.8752\n",
      "65/424, train_loss: 0.8871\n",
      "66/424, train_loss: 0.8612\n",
      "67/424, train_loss: 0.5057\n",
      "68/424, train_loss: 0.6306\n",
      "69/424, train_loss: 0.7628\n",
      "70/424, train_loss: 0.5599\n",
      "71/424, train_loss: 0.8257\n",
      "72/424, train_loss: 0.6948\n",
      "73/424, train_loss: 0.7861\n",
      "74/424, train_loss: 0.7158\n",
      "75/424, train_loss: 0.7128\n",
      "76/424, train_loss: 0.7451\n",
      "77/424, train_loss: 0.7402\n",
      "78/424, train_loss: 0.6929\n",
      "79/424, train_loss: 0.7666\n",
      "80/424, train_loss: 0.7222\n",
      "81/424, train_loss: 0.6378\n",
      "82/424, train_loss: 0.5559\n",
      "83/424, train_loss: 0.8044\n",
      "84/424, train_loss: 0.6398\n",
      "85/424, train_loss: 0.5323\n",
      "86/424, train_loss: 0.6044\n",
      "87/424, train_loss: 0.8121\n",
      "88/424, train_loss: 0.5932\n",
      "89/424, train_loss: 1.1333\n",
      "90/424, train_loss: 1.1373\n",
      "91/424, train_loss: 0.9525\n",
      "92/424, train_loss: 0.4683\n",
      "93/424, train_loss: 1.0067\n",
      "94/424, train_loss: 0.8229\n",
      "95/424, train_loss: 0.7548\n",
      "96/424, train_loss: 0.6848\n",
      "97/424, train_loss: 0.7042\n",
      "98/424, train_loss: 0.6577\n",
      "99/424, train_loss: 0.6294\n",
      "100/424, train_loss: 0.9003\n",
      "101/424, train_loss: 0.7007\n",
      "102/424, train_loss: 0.6798\n",
      "103/424, train_loss: 0.4190\n",
      "104/424, train_loss: 0.7764\n",
      "105/424, train_loss: 0.9662\n",
      "106/424, train_loss: 0.7638\n",
      "107/424, train_loss: 0.5251\n",
      "108/424, train_loss: 0.7530\n",
      "109/424, train_loss: 0.9108\n",
      "110/424, train_loss: 0.7268\n",
      "111/424, train_loss: 0.8711\n",
      "112/424, train_loss: 0.8396\n",
      "113/424, train_loss: 0.5847\n",
      "114/424, train_loss: 0.8004\n",
      "115/424, train_loss: 0.7177\n",
      "116/424, train_loss: 0.7628\n",
      "117/424, train_loss: 0.6656\n",
      "118/424, train_loss: 0.7261\n",
      "119/424, train_loss: 0.7679\n",
      "120/424, train_loss: 0.8955\n",
      "121/424, train_loss: 0.6638\n",
      "122/424, train_loss: 0.6938\n",
      "123/424, train_loss: 0.6245\n",
      "124/424, train_loss: 0.5842\n",
      "125/424, train_loss: 0.9189\n",
      "126/424, train_loss: 0.5985\n",
      "127/424, train_loss: 0.6956\n",
      "128/424, train_loss: 0.7999\n",
      "129/424, train_loss: 0.5335\n",
      "130/424, train_loss: 0.6023\n",
      "131/424, train_loss: 0.6539\n",
      "132/424, train_loss: 0.7527\n",
      "133/424, train_loss: 0.6289\n",
      "134/424, train_loss: 0.7413\n",
      "135/424, train_loss: 0.6826\n",
      "136/424, train_loss: 0.6511\n",
      "137/424, train_loss: 0.5735\n",
      "138/424, train_loss: 0.6306\n",
      "139/424, train_loss: 0.6461\n",
      "140/424, train_loss: 0.7753\n",
      "141/424, train_loss: 0.7898\n",
      "142/424, train_loss: 0.5604\n",
      "143/424, train_loss: 0.6287\n",
      "144/424, train_loss: 0.7261\n",
      "145/424, train_loss: 0.5435\n",
      "146/424, train_loss: 0.6317\n",
      "147/424, train_loss: 0.5535\n",
      "148/424, train_loss: 0.5494\n",
      "149/424, train_loss: 0.6142\n",
      "150/424, train_loss: 0.7372\n",
      "151/424, train_loss: 0.7161\n",
      "152/424, train_loss: 0.5461\n",
      "153/424, train_loss: 0.5643\n",
      "154/424, train_loss: 0.5704\n",
      "155/424, train_loss: 0.5145\n",
      "156/424, train_loss: 0.5710\n",
      "157/424, train_loss: 0.4875\n",
      "158/424, train_loss: 0.5235\n",
      "159/424, train_loss: 0.8932\n",
      "160/424, train_loss: 0.6048\n",
      "161/424, train_loss: 0.5732\n",
      "162/424, train_loss: 1.0575\n",
      "163/424, train_loss: 1.2296\n",
      "164/424, train_loss: 1.0797\n",
      "165/424, train_loss: 0.5199\n",
      "166/424, train_loss: 0.6623\n",
      "167/424, train_loss: 0.5945\n",
      "168/424, train_loss: 0.5502\n",
      "169/424, train_loss: 0.5051\n",
      "170/424, train_loss: 0.7070\n",
      "171/424, train_loss: 0.5963\n",
      "172/424, train_loss: 0.6490\n",
      "173/424, train_loss: 0.6271\n",
      "174/424, train_loss: 0.5706\n",
      "175/424, train_loss: 0.6509\n",
      "176/424, train_loss: 0.6218\n",
      "177/424, train_loss: 0.8887\n",
      "178/424, train_loss: 0.8743\n",
      "179/424, train_loss: 0.7014\n",
      "180/424, train_loss: 0.5773\n",
      "181/424, train_loss: 0.6468\n",
      "182/424, train_loss: 0.6730\n",
      "183/424, train_loss: 0.6482\n",
      "184/424, train_loss: 0.5933\n",
      "185/424, train_loss: 0.5183\n",
      "186/424, train_loss: 0.7509\n",
      "187/424, train_loss: 0.8885\n",
      "188/424, train_loss: 0.7080\n",
      "189/424, train_loss: 0.5996\n",
      "190/424, train_loss: 0.6707\n",
      "191/424, train_loss: 0.7006\n",
      "192/424, train_loss: 0.6923\n",
      "193/424, train_loss: 0.7632\n",
      "194/424, train_loss: 0.6099\n",
      "195/424, train_loss: 0.7321\n",
      "196/424, train_loss: 0.6631\n",
      "197/424, train_loss: 0.5418\n",
      "198/424, train_loss: 0.6091\n",
      "199/424, train_loss: 0.6741\n",
      "200/424, train_loss: 0.4340\n",
      "201/424, train_loss: 0.7871\n",
      "202/424, train_loss: 0.7926\n",
      "203/424, train_loss: 0.6841\n",
      "204/424, train_loss: 0.6751\n",
      "205/424, train_loss: 0.5099\n",
      "206/424, train_loss: 0.5555\n",
      "207/424, train_loss: 0.5988\n",
      "208/424, train_loss: 0.7800\n",
      "209/424, train_loss: 0.7634\n",
      "210/424, train_loss: 0.8981\n",
      "211/424, train_loss: 0.5372\n",
      "212/424, train_loss: 0.4286\n",
      "213/424, train_loss: 0.4638\n",
      "214/424, train_loss: 0.6147\n",
      "215/424, train_loss: 0.8239\n",
      "216/424, train_loss: 0.7021\n",
      "217/424, train_loss: 0.5183\n",
      "218/424, train_loss: 0.6649\n",
      "219/424, train_loss: 0.7094\n",
      "220/424, train_loss: 0.7751\n",
      "221/424, train_loss: 0.7360\n",
      "222/424, train_loss: 0.7610\n",
      "223/424, train_loss: 0.7049\n",
      "224/424, train_loss: 0.7843\n",
      "225/424, train_loss: 0.7906\n",
      "226/424, train_loss: 0.5394\n",
      "227/424, train_loss: 0.5269\n",
      "228/424, train_loss: 0.7108\n",
      "229/424, train_loss: 0.6990\n",
      "230/424, train_loss: 0.5964\n",
      "231/424, train_loss: 0.6461\n",
      "232/424, train_loss: 0.9460\n",
      "233/424, train_loss: 0.6541\n",
      "234/424, train_loss: 0.7516\n",
      "235/424, train_loss: 0.8130\n",
      "236/424, train_loss: 0.5786\n",
      "237/424, train_loss: 0.6304\n",
      "238/424, train_loss: 0.6511\n",
      "239/424, train_loss: 0.7566\n",
      "240/424, train_loss: 0.7593\n",
      "241/424, train_loss: 0.8069\n",
      "242/424, train_loss: 0.6239\n",
      "243/424, train_loss: 0.5403\n",
      "244/424, train_loss: 0.5599\n",
      "245/424, train_loss: 0.4492\n",
      "246/424, train_loss: 0.6805\n",
      "247/424, train_loss: 1.2688\n",
      "248/424, train_loss: 0.9711\n",
      "249/424, train_loss: 0.6167\n",
      "250/424, train_loss: 1.0778\n",
      "251/424, train_loss: 0.7773\n",
      "252/424, train_loss: 0.6464\n",
      "253/424, train_loss: 0.5532\n",
      "254/424, train_loss: 0.8364\n",
      "255/424, train_loss: 0.5765\n",
      "256/424, train_loss: 0.6427\n",
      "257/424, train_loss: 0.5708\n",
      "258/424, train_loss: 0.7082\n",
      "259/424, train_loss: 0.6552\n",
      "260/424, train_loss: 0.5957\n",
      "261/424, train_loss: 0.8726\n",
      "262/424, train_loss: 0.8646\n",
      "263/424, train_loss: 0.4069\n",
      "264/424, train_loss: 0.3997\n",
      "265/424, train_loss: 0.8428\n",
      "266/424, train_loss: 0.8221\n",
      "267/424, train_loss: 0.8535\n",
      "268/424, train_loss: 0.7469\n",
      "269/424, train_loss: 0.6772\n",
      "270/424, train_loss: 0.5257\n",
      "271/424, train_loss: 0.6050\n",
      "272/424, train_loss: 0.4973\n",
      "273/424, train_loss: 0.8005\n",
      "274/424, train_loss: 0.8816\n",
      "275/424, train_loss: 0.5554\n",
      "276/424, train_loss: 0.8981\n",
      "277/424, train_loss: 0.5855\n",
      "278/424, train_loss: 0.8868\n",
      "279/424, train_loss: 0.7914\n",
      "280/424, train_loss: 0.5775\n",
      "281/424, train_loss: 0.5663\n",
      "282/424, train_loss: 0.5488\n",
      "283/424, train_loss: 0.5971\n",
      "284/424, train_loss: 0.5453\n",
      "285/424, train_loss: 0.7485\n",
      "286/424, train_loss: 0.5647\n",
      "287/424, train_loss: 0.7499\n",
      "288/424, train_loss: 0.7878\n",
      "289/424, train_loss: 0.8710\n",
      "290/424, train_loss: 0.5314\n",
      "291/424, train_loss: 0.8167\n",
      "292/424, train_loss: 0.7003\n",
      "293/424, train_loss: 0.6812\n",
      "294/424, train_loss: 0.5784\n",
      "295/424, train_loss: 0.6809\n",
      "296/424, train_loss: 0.7931\n",
      "297/424, train_loss: 0.7303\n",
      "298/424, train_loss: 0.5975\n",
      "299/424, train_loss: 0.5978\n",
      "300/424, train_loss: 0.6685\n",
      "301/424, train_loss: 0.5775\n",
      "302/424, train_loss: 0.6621\n",
      "303/424, train_loss: 0.5086\n",
      "304/424, train_loss: 0.6737\n",
      "305/424, train_loss: 0.5119\n",
      "306/424, train_loss: 0.5109\n",
      "307/424, train_loss: 0.8530\n",
      "308/424, train_loss: 0.2695\n",
      "309/424, train_loss: 0.5443\n",
      "310/424, train_loss: 0.8094\n",
      "311/424, train_loss: 1.2097\n",
      "312/424, train_loss: 1.2255\n",
      "313/424, train_loss: 0.6809\n",
      "314/424, train_loss: 0.3114\n",
      "315/424, train_loss: 0.4427\n",
      "316/424, train_loss: 0.5966\n",
      "317/424, train_loss: 1.2520\n",
      "318/424, train_loss: 0.8087\n",
      "319/424, train_loss: 0.5870\n",
      "320/424, train_loss: 0.4756\n",
      "321/424, train_loss: 0.6237\n",
      "322/424, train_loss: 0.7001\n",
      "323/424, train_loss: 0.7919\n",
      "324/424, train_loss: 0.5942\n",
      "325/424, train_loss: 0.5378\n",
      "326/424, train_loss: 0.7289\n",
      "327/424, train_loss: 0.5677\n",
      "328/424, train_loss: 0.6628\n",
      "329/424, train_loss: 0.6783\n",
      "330/424, train_loss: 0.6442\n",
      "331/424, train_loss: 0.6110\n",
      "332/424, train_loss: 0.4970\n",
      "333/424, train_loss: 0.7574\n",
      "334/424, train_loss: 0.5934\n",
      "335/424, train_loss: 0.6867\n",
      "336/424, train_loss: 0.6073\n",
      "337/424, train_loss: 0.6189\n",
      "338/424, train_loss: 0.6841\n",
      "339/424, train_loss: 0.5762\n",
      "340/424, train_loss: 0.6687\n",
      "341/424, train_loss: 0.5493\n",
      "342/424, train_loss: 0.6605\n",
      "343/424, train_loss: 0.6126\n",
      "344/424, train_loss: 0.7507\n",
      "345/424, train_loss: 0.5563\n",
      "346/424, train_loss: 0.9414\n",
      "347/424, train_loss: 0.5703\n",
      "348/424, train_loss: 0.7219\n",
      "349/424, train_loss: 0.5836\n",
      "350/424, train_loss: 0.7119\n",
      "351/424, train_loss: 0.7886\n",
      "352/424, train_loss: 0.6623\n",
      "353/424, train_loss: 0.7958\n",
      "354/424, train_loss: 0.6477\n",
      "355/424, train_loss: 0.5851\n",
      "356/424, train_loss: 0.8367\n",
      "357/424, train_loss: 0.5677\n",
      "358/424, train_loss: 0.7633\n",
      "359/424, train_loss: 0.9443\n",
      "360/424, train_loss: 0.6815\n",
      "361/424, train_loss: 0.6624\n",
      "362/424, train_loss: 0.5854\n",
      "363/424, train_loss: 0.7784\n",
      "364/424, train_loss: 0.9989\n",
      "365/424, train_loss: 0.6255\n",
      "366/424, train_loss: 0.9306\n",
      "367/424, train_loss: 0.6412\n",
      "368/424, train_loss: 0.6085\n",
      "369/424, train_loss: 0.8761\n",
      "370/424, train_loss: 0.4672\n",
      "371/424, train_loss: 0.7264\n",
      "372/424, train_loss: 0.5855\n",
      "373/424, train_loss: 0.5212\n",
      "374/424, train_loss: 0.7640\n",
      "375/424, train_loss: 0.7758\n",
      "376/424, train_loss: 0.6410\n",
      "377/424, train_loss: 1.0225\n",
      "378/424, train_loss: 0.7762\n",
      "379/424, train_loss: 0.5984\n",
      "380/424, train_loss: 0.6494\n",
      "381/424, train_loss: 0.7646\n",
      "382/424, train_loss: 0.7344\n",
      "383/424, train_loss: 0.7415\n",
      "384/424, train_loss: 0.9288\n",
      "385/424, train_loss: 0.5108\n",
      "386/424, train_loss: 0.7099\n",
      "387/424, train_loss: 0.6093\n",
      "388/424, train_loss: 0.5364\n",
      "389/424, train_loss: 0.7782\n",
      "390/424, train_loss: 0.8567\n",
      "391/424, train_loss: 0.8959\n",
      "392/424, train_loss: 0.5439\n",
      "393/424, train_loss: 0.7697\n",
      "394/424, train_loss: 0.6992\n",
      "395/424, train_loss: 0.6053\n",
      "396/424, train_loss: 0.6706\n",
      "397/424, train_loss: 0.5477\n",
      "398/424, train_loss: 0.7979\n",
      "399/424, train_loss: 0.6736\n",
      "400/424, train_loss: 0.7196\n",
      "401/424, train_loss: 0.6918\n",
      "402/424, train_loss: 0.5644\n",
      "403/424, train_loss: 0.5941\n",
      "404/424, train_loss: 0.7764\n",
      "405/424, train_loss: 0.6605\n",
      "406/424, train_loss: 0.7311\n",
      "407/424, train_loss: 0.7768\n",
      "408/424, train_loss: 0.7568\n",
      "409/424, train_loss: 0.6100\n",
      "410/424, train_loss: 0.8311\n",
      "411/424, train_loss: 0.6677\n",
      "412/424, train_loss: 0.7277\n",
      "413/424, train_loss: 0.7678\n",
      "414/424, train_loss: 0.5125\n",
      "415/424, train_loss: 0.5927\n",
      "416/424, train_loss: 0.5809\n",
      "417/424, train_loss: 0.5938\n",
      "418/424, train_loss: 0.7534\n",
      "419/424, train_loss: 0.7246\n",
      "420/424, train_loss: 0.6579\n",
      "421/424, train_loss: 0.7479\n",
      "422/424, train_loss: 0.6177\n",
      "423/424, train_loss: 0.7026\n",
      "424/424, train_loss: 0.6520\n",
      "425/424, train_loss: 0.5872\n",
      "epoch 8 average loss: 0.6952\n",
      "Epoch time duration: 566.2985923290253\n",
      "current epoch: 8 current accuracy: 0.4749 best accuracy: 0.5050 at epoch 6\n",
      "----------\n",
      "epoch 9\n",
      "1/424, train_loss: 0.6942\n",
      "2/424, train_loss: 0.5969\n",
      "3/424, train_loss: 0.4730\n",
      "4/424, train_loss: 0.6458\n",
      "5/424, train_loss: 0.6574\n",
      "6/424, train_loss: 0.7365\n",
      "7/424, train_loss: 0.5379\n",
      "8/424, train_loss: 0.6906\n",
      "9/424, train_loss: 0.5936\n",
      "10/424, train_loss: 0.5476\n",
      "11/424, train_loss: 0.7238\n",
      "12/424, train_loss: 0.5745\n",
      "13/424, train_loss: 0.5065\n",
      "14/424, train_loss: 0.5406\n",
      "15/424, train_loss: 0.6118\n",
      "16/424, train_loss: 0.6849\n",
      "17/424, train_loss: 0.6113\n",
      "18/424, train_loss: 0.7670\n",
      "19/424, train_loss: 0.6446\n",
      "20/424, train_loss: 0.5712\n",
      "21/424, train_loss: 0.6746\n",
      "22/424, train_loss: 0.5983\n",
      "23/424, train_loss: 0.6471\n",
      "24/424, train_loss: 0.6469\n",
      "25/424, train_loss: 0.4230\n",
      "26/424, train_loss: 0.4706\n",
      "27/424, train_loss: 0.6068\n",
      "28/424, train_loss: 0.9044\n",
      "29/424, train_loss: 0.6641\n",
      "30/424, train_loss: 0.6019\n",
      "31/424, train_loss: 0.4592\n",
      "32/424, train_loss: 0.5958\n",
      "33/424, train_loss: 0.7819\n",
      "34/424, train_loss: 0.6456\n",
      "35/424, train_loss: 0.6417\n",
      "36/424, train_loss: 0.6575\n",
      "37/424, train_loss: 0.5895\n",
      "38/424, train_loss: 0.5916\n",
      "39/424, train_loss: 0.6572\n",
      "40/424, train_loss: 0.6024\n",
      "41/424, train_loss: 0.4545\n",
      "42/424, train_loss: 0.4037\n",
      "43/424, train_loss: 0.6442\n",
      "44/424, train_loss: 0.5784\n",
      "45/424, train_loss: 0.3559\n",
      "46/424, train_loss: 0.7688\n",
      "47/424, train_loss: 0.5515\n",
      "48/424, train_loss: 0.7290\n",
      "49/424, train_loss: 0.5443\n",
      "50/424, train_loss: 0.6757\n",
      "51/424, train_loss: 0.4475\n",
      "52/424, train_loss: 0.3217\n",
      "53/424, train_loss: 0.6287\n",
      "54/424, train_loss: 0.5848\n",
      "55/424, train_loss: 0.5554\n",
      "56/424, train_loss: 0.8290\n",
      "57/424, train_loss: 0.7253\n",
      "58/424, train_loss: 0.5762\n",
      "59/424, train_loss: 1.4603\n",
      "60/424, train_loss: 0.6630\n",
      "61/424, train_loss: 0.5848\n",
      "62/424, train_loss: 0.5341\n",
      "63/424, train_loss: 0.5862\n",
      "64/424, train_loss: 0.4102\n",
      "65/424, train_loss: 0.5804\n",
      "66/424, train_loss: 0.9358\n",
      "67/424, train_loss: 0.5194\n",
      "68/424, train_loss: 0.5382\n",
      "69/424, train_loss: 0.5185\n",
      "70/424, train_loss: 0.4218\n",
      "71/424, train_loss: 0.4090\n",
      "72/424, train_loss: 0.9372\n",
      "73/424, train_loss: 0.3839\n",
      "74/424, train_loss: 0.9724\n",
      "75/424, train_loss: 0.2377\n",
      "76/424, train_loss: 0.8840\n",
      "77/424, train_loss: 1.1006\n",
      "78/424, train_loss: 1.1668\n",
      "79/424, train_loss: 0.7798\n",
      "80/424, train_loss: 0.6266\n",
      "81/424, train_loss: 0.8194\n",
      "82/424, train_loss: 0.3892\n",
      "83/424, train_loss: 0.5237\n",
      "84/424, train_loss: 0.4831\n",
      "85/424, train_loss: 0.7518\n",
      "86/424, train_loss: 0.6519\n",
      "87/424, train_loss: 0.7070\n",
      "88/424, train_loss: 0.4985\n",
      "89/424, train_loss: 0.6404\n",
      "90/424, train_loss: 0.7424\n",
      "91/424, train_loss: 0.6008\n",
      "92/424, train_loss: 0.6508\n",
      "93/424, train_loss: 0.6783\n",
      "94/424, train_loss: 0.5612\n",
      "95/424, train_loss: 0.7422\n",
      "96/424, train_loss: 0.3942\n",
      "97/424, train_loss: 0.7513\n",
      "98/424, train_loss: 0.6996\n",
      "99/424, train_loss: 0.8644\n",
      "100/424, train_loss: 0.5436\n",
      "101/424, train_loss: 0.8184\n",
      "102/424, train_loss: 0.6805\n",
      "103/424, train_loss: 0.6154\n",
      "104/424, train_loss: 0.5520\n",
      "105/424, train_loss: 0.6678\n",
      "106/424, train_loss: 0.8141\n",
      "107/424, train_loss: 0.6654\n",
      "108/424, train_loss: 0.8600\n",
      "109/424, train_loss: 0.6605\n",
      "110/424, train_loss: 0.6848\n",
      "111/424, train_loss: 0.4948\n",
      "112/424, train_loss: 0.4597\n",
      "113/424, train_loss: 0.9086\n",
      "114/424, train_loss: 0.6733\n",
      "115/424, train_loss: 0.3758\n",
      "116/424, train_loss: 0.8275\n",
      "117/424, train_loss: 0.9374\n",
      "118/424, train_loss: 0.7101\n",
      "119/424, train_loss: 0.6113\n",
      "120/424, train_loss: 0.4727\n",
      "121/424, train_loss: 0.7100\n",
      "122/424, train_loss: 0.7173\n",
      "123/424, train_loss: 0.4708\n",
      "124/424, train_loss: 0.5677\n",
      "125/424, train_loss: 0.4675\n",
      "126/424, train_loss: 0.8192\n",
      "127/424, train_loss: 0.8087\n",
      "128/424, train_loss: 0.5457\n",
      "129/424, train_loss: 0.3713\n",
      "130/424, train_loss: 0.4779\n",
      "131/424, train_loss: 0.6459\n",
      "132/424, train_loss: 0.6697\n",
      "133/424, train_loss: 0.4562\n",
      "134/424, train_loss: 0.5054\n",
      "135/424, train_loss: 0.8066\n",
      "136/424, train_loss: 0.7528\n",
      "137/424, train_loss: 0.4439\n",
      "138/424, train_loss: 0.5993\n",
      "139/424, train_loss: 0.3290\n",
      "140/424, train_loss: 0.7244\n",
      "141/424, train_loss: 0.5445\n",
      "142/424, train_loss: 0.8838\n",
      "143/424, train_loss: 0.8889\n",
      "144/424, train_loss: 0.8865\n",
      "145/424, train_loss: 0.7359\n",
      "146/424, train_loss: 0.8630\n",
      "147/424, train_loss: 0.5123\n",
      "148/424, train_loss: 0.4193\n",
      "149/424, train_loss: 0.8171\n",
      "150/424, train_loss: 0.9418\n",
      "151/424, train_loss: 0.4623\n",
      "152/424, train_loss: 0.6380\n",
      "153/424, train_loss: 0.6537\n",
      "154/424, train_loss: 0.6329\n",
      "155/424, train_loss: 0.5560\n",
      "156/424, train_loss: 0.6274\n",
      "157/424, train_loss: 0.7922\n",
      "158/424, train_loss: 0.8047\n",
      "159/424, train_loss: 0.4596\n",
      "160/424, train_loss: 0.8353\n",
      "161/424, train_loss: 0.5395\n",
      "162/424, train_loss: 0.5320\n",
      "163/424, train_loss: 0.5644\n",
      "164/424, train_loss: 0.8331\n",
      "165/424, train_loss: 0.4158\n",
      "166/424, train_loss: 1.0939\n",
      "167/424, train_loss: 0.9284\n",
      "168/424, train_loss: 0.6049\n",
      "169/424, train_loss: 0.6982\n",
      "170/424, train_loss: 0.6541\n",
      "171/424, train_loss: 0.7846\n",
      "172/424, train_loss: 0.5887\n",
      "173/424, train_loss: 0.5582\n",
      "174/424, train_loss: 0.5634\n",
      "175/424, train_loss: 0.6503\n",
      "176/424, train_loss: 0.7783\n",
      "177/424, train_loss: 0.6454\n",
      "178/424, train_loss: 0.5165\n",
      "179/424, train_loss: 0.6079\n",
      "180/424, train_loss: 0.5670\n",
      "181/424, train_loss: 0.5206\n",
      "182/424, train_loss: 0.5717\n",
      "183/424, train_loss: 0.6957\n",
      "184/424, train_loss: 0.8436\n",
      "185/424, train_loss: 0.7935\n",
      "186/424, train_loss: 0.7847\n",
      "187/424, train_loss: 1.0239\n",
      "188/424, train_loss: 0.9780\n",
      "189/424, train_loss: 0.8657\n",
      "190/424, train_loss: 0.8296\n",
      "191/424, train_loss: 0.5358\n",
      "192/424, train_loss: 0.5356\n",
      "193/424, train_loss: 0.9391\n",
      "194/424, train_loss: 0.5657\n",
      "195/424, train_loss: 0.5007\n",
      "196/424, train_loss: 0.6204\n",
      "197/424, train_loss: 0.4308\n",
      "198/424, train_loss: 0.5812\n",
      "199/424, train_loss: 0.7406\n",
      "200/424, train_loss: 0.7001\n",
      "201/424, train_loss: 0.7964\n",
      "202/424, train_loss: 0.6350\n",
      "203/424, train_loss: 0.7737\n",
      "204/424, train_loss: 0.8102\n",
      "205/424, train_loss: 0.6515\n",
      "206/424, train_loss: 1.3212\n",
      "207/424, train_loss: 1.0265\n",
      "208/424, train_loss: 1.0697\n",
      "209/424, train_loss: 0.5657\n",
      "210/424, train_loss: 0.8192\n",
      "211/424, train_loss: 0.6143\n",
      "212/424, train_loss: 0.4634\n",
      "213/424, train_loss: 0.6802\n",
      "214/424, train_loss: 0.6943\n",
      "215/424, train_loss: 0.4468\n",
      "216/424, train_loss: 0.8647\n",
      "217/424, train_loss: 0.5672\n",
      "218/424, train_loss: 0.5859\n",
      "219/424, train_loss: 0.8158\n",
      "220/424, train_loss: 0.6444\n",
      "221/424, train_loss: 0.6422\n",
      "222/424, train_loss: 0.6744\n",
      "223/424, train_loss: 0.5735\n",
      "224/424, train_loss: 0.6454\n",
      "225/424, train_loss: 0.5305\n",
      "226/424, train_loss: 0.5888\n",
      "227/424, train_loss: 0.9191\n",
      "228/424, train_loss: 0.6412\n",
      "229/424, train_loss: 0.7758\n",
      "230/424, train_loss: 0.9598\n",
      "231/424, train_loss: 0.7004\n",
      "232/424, train_loss: 0.5436\n",
      "233/424, train_loss: 0.6929\n",
      "234/424, train_loss: 0.4895\n",
      "235/424, train_loss: 0.6499\n",
      "236/424, train_loss: 0.4586\n",
      "237/424, train_loss: 0.7928\n",
      "238/424, train_loss: 0.8942\n",
      "239/424, train_loss: 1.1008\n",
      "240/424, train_loss: 0.5877\n",
      "241/424, train_loss: 0.3362\n",
      "242/424, train_loss: 0.3816\n",
      "243/424, train_loss: 0.4627\n",
      "244/424, train_loss: 1.1730\n",
      "245/424, train_loss: 0.6599\n",
      "246/424, train_loss: 0.3877\n",
      "247/424, train_loss: 0.3337\n",
      "248/424, train_loss: 0.6988\n",
      "249/424, train_loss: 0.9959\n",
      "250/424, train_loss: 1.1075\n",
      "251/424, train_loss: 0.9808\n",
      "252/424, train_loss: 1.1663\n",
      "253/424, train_loss: 0.4866\n",
      "254/424, train_loss: 0.4980\n",
      "255/424, train_loss: 0.5802\n",
      "256/424, train_loss: 0.6687\n",
      "257/424, train_loss: 0.4985\n",
      "258/424, train_loss: 0.6228\n",
      "259/424, train_loss: 0.6365\n",
      "260/424, train_loss: 0.5023\n",
      "261/424, train_loss: 0.5264\n",
      "262/424, train_loss: 0.8007\n",
      "263/424, train_loss: 0.6441\n",
      "264/424, train_loss: 0.5244\n",
      "265/424, train_loss: 0.6057\n",
      "266/424, train_loss: 0.6155\n",
      "267/424, train_loss: 0.3992\n",
      "268/424, train_loss: 0.9036\n",
      "269/424, train_loss: 0.8251\n",
      "270/424, train_loss: 0.6814\n",
      "271/424, train_loss: 0.8226\n",
      "272/424, train_loss: 0.7000\n",
      "273/424, train_loss: 0.8412\n",
      "274/424, train_loss: 0.6329\n",
      "275/424, train_loss: 0.7635\n",
      "276/424, train_loss: 0.7153\n",
      "277/424, train_loss: 0.4437\n",
      "278/424, train_loss: 0.6024\n",
      "279/424, train_loss: 0.8186\n",
      "280/424, train_loss: 0.6917\n",
      "281/424, train_loss: 0.5753\n",
      "282/424, train_loss: 0.4501\n",
      "283/424, train_loss: 0.5499\n",
      "284/424, train_loss: 0.6979\n",
      "285/424, train_loss: 0.5964\n",
      "286/424, train_loss: 0.7611\n",
      "287/424, train_loss: 0.9575\n",
      "288/424, train_loss: 1.0349\n",
      "289/424, train_loss: 0.7328\n",
      "290/424, train_loss: 0.5349\n",
      "291/424, train_loss: 0.5345\n",
      "292/424, train_loss: 0.4694\n",
      "293/424, train_loss: 0.5382\n",
      "294/424, train_loss: 0.7870\n",
      "295/424, train_loss: 0.7917\n",
      "296/424, train_loss: 0.5751\n",
      "297/424, train_loss: 0.6776\n",
      "298/424, train_loss: 0.6228\n",
      "299/424, train_loss: 0.4950\n",
      "300/424, train_loss: 0.7738\n",
      "301/424, train_loss: 0.6784\n",
      "302/424, train_loss: 0.8646\n",
      "303/424, train_loss: 0.5993\n",
      "304/424, train_loss: 0.9846\n",
      "305/424, train_loss: 0.4853\n",
      "306/424, train_loss: 0.6423\n",
      "307/424, train_loss: 0.5344\n",
      "308/424, train_loss: 0.6105\n",
      "309/424, train_loss: 0.4608\n",
      "310/424, train_loss: 0.5608\n",
      "311/424, train_loss: 0.8479\n",
      "312/424, train_loss: 0.8711\n",
      "313/424, train_loss: 0.6749\n",
      "314/424, train_loss: 0.4779\n",
      "315/424, train_loss: 0.6129\n",
      "316/424, train_loss: 0.7226\n",
      "317/424, train_loss: 0.6354\n",
      "318/424, train_loss: 0.7267\n",
      "319/424, train_loss: 1.0232\n",
      "320/424, train_loss: 0.5839\n",
      "321/424, train_loss: 0.5443\n",
      "322/424, train_loss: 0.3782\n",
      "323/424, train_loss: 0.8170\n",
      "324/424, train_loss: 0.8404\n",
      "325/424, train_loss: 1.2139\n",
      "326/424, train_loss: 0.5716\n",
      "327/424, train_loss: 0.4081\n",
      "328/424, train_loss: 1.1638\n",
      "329/424, train_loss: 0.5616\n",
      "330/424, train_loss: 0.4657\n",
      "331/424, train_loss: 0.4349\n",
      "332/424, train_loss: 0.7707\n",
      "333/424, train_loss: 0.5385\n",
      "334/424, train_loss: 0.5756\n",
      "335/424, train_loss: 0.7229\n",
      "336/424, train_loss: 0.6238\n",
      "337/424, train_loss: 0.6554\n",
      "338/424, train_loss: 0.8315\n",
      "339/424, train_loss: 0.7052\n",
      "340/424, train_loss: 0.5878\n",
      "341/424, train_loss: 0.7192\n",
      "342/424, train_loss: 0.7206\n",
      "343/424, train_loss: 0.6397\n",
      "344/424, train_loss: 0.7620\n",
      "345/424, train_loss: 0.8973\n",
      "346/424, train_loss: 0.4551\n",
      "347/424, train_loss: 0.8600\n",
      "348/424, train_loss: 0.7820\n",
      "349/424, train_loss: 0.9843\n",
      "350/424, train_loss: 0.6677\n",
      "351/424, train_loss: 0.6815\n",
      "352/424, train_loss: 0.4902\n",
      "353/424, train_loss: 0.6109\n",
      "354/424, train_loss: 0.5342\n",
      "355/424, train_loss: 0.7412\n",
      "356/424, train_loss: 0.4095\n",
      "357/424, train_loss: 0.8256\n",
      "358/424, train_loss: 0.8517\n",
      "359/424, train_loss: 0.5511\n",
      "360/424, train_loss: 0.5714\n",
      "361/424, train_loss: 0.6151\n",
      "362/424, train_loss: 0.7649\n",
      "363/424, train_loss: 0.5438\n",
      "364/424, train_loss: 0.6776\n",
      "365/424, train_loss: 0.5947\n",
      "366/424, train_loss: 0.5095\n",
      "367/424, train_loss: 0.4838\n",
      "368/424, train_loss: 0.5040\n",
      "369/424, train_loss: 0.6422\n",
      "370/424, train_loss: 0.3925\n",
      "371/424, train_loss: 0.4647\n",
      "372/424, train_loss: 0.6375\n",
      "373/424, train_loss: 0.5147\n",
      "374/424, train_loss: 0.7542\n",
      "375/424, train_loss: 0.8370\n",
      "376/424, train_loss: 0.4558\n",
      "377/424, train_loss: 0.5367\n",
      "378/424, train_loss: 0.6688\n",
      "379/424, train_loss: 0.8003\n",
      "380/424, train_loss: 0.5118\n",
      "381/424, train_loss: 0.5315\n",
      "382/424, train_loss: 0.7826\n",
      "383/424, train_loss: 0.7988\n",
      "384/424, train_loss: 0.4601\n",
      "385/424, train_loss: 0.5461\n",
      "386/424, train_loss: 0.9998\n",
      "387/424, train_loss: 0.7034\n",
      "388/424, train_loss: 0.6175\n",
      "389/424, train_loss: 0.4043\n",
      "390/424, train_loss: 0.5164\n",
      "391/424, train_loss: 0.8373\n",
      "392/424, train_loss: 1.0433\n",
      "393/424, train_loss: 0.6348\n",
      "394/424, train_loss: 1.1110\n",
      "395/424, train_loss: 0.7275\n",
      "396/424, train_loss: 0.5158\n",
      "397/424, train_loss: 0.7104\n",
      "398/424, train_loss: 0.4810\n",
      "399/424, train_loss: 0.4344\n",
      "400/424, train_loss: 0.7229\n",
      "401/424, train_loss: 0.5406\n",
      "402/424, train_loss: 0.5546\n",
      "403/424, train_loss: 0.7581\n",
      "404/424, train_loss: 0.3553\n",
      "405/424, train_loss: 0.4743\n",
      "406/424, train_loss: 0.5461\n",
      "407/424, train_loss: 0.6361\n",
      "408/424, train_loss: 0.4324\n",
      "409/424, train_loss: 0.5571\n",
      "410/424, train_loss: 0.5104\n",
      "411/424, train_loss: 0.5730\n",
      "412/424, train_loss: 0.8600\n",
      "413/424, train_loss: 0.6824\n",
      "414/424, train_loss: 0.4453\n",
      "415/424, train_loss: 0.8054\n",
      "416/424, train_loss: 0.9385\n",
      "417/424, train_loss: 0.4965\n",
      "418/424, train_loss: 0.9314\n",
      "419/424, train_loss: 0.5400\n",
      "420/424, train_loss: 0.6293\n",
      "421/424, train_loss: 0.6062\n",
      "422/424, train_loss: 0.7111\n",
      "423/424, train_loss: 0.6270\n",
      "424/424, train_loss: 0.9594\n",
      "425/424, train_loss: 0.5110\n",
      "epoch 9 average loss: 0.6594\n",
      "Epoch time duration: 566.8635358810425\n",
      "----------\n",
      "epoch 10\n",
      "1/424, train_loss: 0.5767\n",
      "2/424, train_loss: 0.6508\n",
      "3/424, train_loss: 0.8307\n",
      "4/424, train_loss: 0.7044\n",
      "5/424, train_loss: 0.5338\n",
      "6/424, train_loss: 0.7002\n",
      "7/424, train_loss: 0.9185\n",
      "8/424, train_loss: 0.7738\n",
      "9/424, train_loss: 0.4143\n",
      "10/424, train_loss: 0.9467\n",
      "11/424, train_loss: 0.8460\n",
      "12/424, train_loss: 0.8017\n",
      "13/424, train_loss: 0.9833\n",
      "14/424, train_loss: 0.3877\n",
      "15/424, train_loss: 0.5779\n",
      "16/424, train_loss: 0.5769\n",
      "17/424, train_loss: 0.6984\n",
      "18/424, train_loss: 0.4146\n",
      "19/424, train_loss: 0.8978\n",
      "20/424, train_loss: 0.6409\n",
      "21/424, train_loss: 0.6266\n",
      "22/424, train_loss: 0.5392\n",
      "23/424, train_loss: 0.4476\n",
      "24/424, train_loss: 0.6613\n",
      "25/424, train_loss: 0.5423\n",
      "26/424, train_loss: 0.5814\n",
      "27/424, train_loss: 0.5452\n",
      "28/424, train_loss: 0.8052\n",
      "29/424, train_loss: 1.0900\n",
      "30/424, train_loss: 0.5370\n",
      "31/424, train_loss: 0.3987\n",
      "32/424, train_loss: 0.6229\n",
      "33/424, train_loss: 0.6597\n",
      "34/424, train_loss: 0.7483\n",
      "35/424, train_loss: 0.6304\n",
      "36/424, train_loss: 0.6379\n",
      "37/424, train_loss: 0.7582\n",
      "38/424, train_loss: 0.7351\n",
      "39/424, train_loss: 0.7270\n",
      "40/424, train_loss: 0.4441\n",
      "41/424, train_loss: 0.7927\n",
      "42/424, train_loss: 0.8140\n",
      "43/424, train_loss: 0.4769\n",
      "44/424, train_loss: 0.6102\n",
      "45/424, train_loss: 0.5032\n",
      "46/424, train_loss: 0.6360\n",
      "47/424, train_loss: 0.5991\n",
      "48/424, train_loss: 0.3229\n",
      "49/424, train_loss: 0.4826\n",
      "50/424, train_loss: 0.8818\n",
      "51/424, train_loss: 0.4880\n",
      "52/424, train_loss: 0.9084\n",
      "53/424, train_loss: 0.7665\n",
      "54/424, train_loss: 0.6966\n",
      "55/424, train_loss: 0.7968\n",
      "56/424, train_loss: 0.7279\n",
      "57/424, train_loss: 0.5157\n",
      "58/424, train_loss: 0.8434\n",
      "59/424, train_loss: 0.6155\n",
      "60/424, train_loss: 0.8647\n",
      "61/424, train_loss: 0.5157\n",
      "62/424, train_loss: 0.5159\n",
      "63/424, train_loss: 0.4181\n",
      "64/424, train_loss: 0.8352\n",
      "65/424, train_loss: 0.7449\n",
      "66/424, train_loss: 0.4826\n",
      "67/424, train_loss: 0.5636\n",
      "68/424, train_loss: 0.8727\n",
      "69/424, train_loss: 0.6347\n",
      "70/424, train_loss: 0.6179\n",
      "71/424, train_loss: 0.6985\n",
      "72/424, train_loss: 0.9174\n",
      "73/424, train_loss: 0.6096\n",
      "74/424, train_loss: 0.6411\n",
      "75/424, train_loss: 0.3464\n",
      "76/424, train_loss: 0.7020\n",
      "77/424, train_loss: 1.0244\n",
      "78/424, train_loss: 0.4019\n",
      "79/424, train_loss: 0.8611\n",
      "80/424, train_loss: 0.5740\n",
      "81/424, train_loss: 0.5367\n",
      "82/424, train_loss: 0.7255\n",
      "83/424, train_loss: 0.9042\n",
      "84/424, train_loss: 0.5261\n",
      "85/424, train_loss: 0.4874\n",
      "86/424, train_loss: 0.4409\n",
      "87/424, train_loss: 0.8630\n",
      "88/424, train_loss: 0.5964\n",
      "89/424, train_loss: 0.7640\n",
      "90/424, train_loss: 0.9728\n",
      "91/424, train_loss: 0.6987\n",
      "92/424, train_loss: 0.6665\n",
      "93/424, train_loss: 0.7438\n",
      "94/424, train_loss: 0.7616\n",
      "95/424, train_loss: 0.6073\n",
      "96/424, train_loss: 0.7251\n",
      "97/424, train_loss: 0.7508\n",
      "98/424, train_loss: 0.6344\n",
      "99/424, train_loss: 0.9706\n",
      "100/424, train_loss: 0.6378\n",
      "101/424, train_loss: 0.3890\n",
      "102/424, train_loss: 0.6107\n",
      "103/424, train_loss: 0.6307\n",
      "104/424, train_loss: 0.4958\n",
      "105/424, train_loss: 0.6555\n",
      "106/424, train_loss: 0.4809\n",
      "107/424, train_loss: 0.5808\n",
      "108/424, train_loss: 0.5770\n",
      "109/424, train_loss: 0.6222\n",
      "110/424, train_loss: 0.7378\n",
      "111/424, train_loss: 0.5187\n",
      "112/424, train_loss: 0.4720\n",
      "113/424, train_loss: 0.5506\n",
      "114/424, train_loss: 0.5178\n",
      "115/424, train_loss: 0.5359\n",
      "116/424, train_loss: 0.4958\n",
      "117/424, train_loss: 0.4134\n",
      "118/424, train_loss: 0.2775\n",
      "119/424, train_loss: 0.6306\n",
      "120/424, train_loss: 0.5728\n",
      "121/424, train_loss: 0.4629\n",
      "122/424, train_loss: 0.7975\n",
      "123/424, train_loss: 0.5023\n",
      "124/424, train_loss: 0.6456\n",
      "125/424, train_loss: 0.5193\n",
      "126/424, train_loss: 0.7352\n",
      "127/424, train_loss: 0.4771\n",
      "128/424, train_loss: 0.6010\n",
      "129/424, train_loss: 1.0852\n",
      "130/424, train_loss: 0.3000\n",
      "131/424, train_loss: 0.5920\n",
      "132/424, train_loss: 0.5242\n",
      "133/424, train_loss: 0.5496\n",
      "134/424, train_loss: 0.7192\n",
      "135/424, train_loss: 0.6699\n",
      "136/424, train_loss: 0.6200\n",
      "137/424, train_loss: 0.3877\n",
      "138/424, train_loss: 0.6391\n",
      "139/424, train_loss: 0.5707\n",
      "140/424, train_loss: 0.2950\n",
      "141/424, train_loss: 0.9674\n",
      "142/424, train_loss: 0.9771\n",
      "143/424, train_loss: 0.5768\n",
      "144/424, train_loss: 0.6402\n",
      "145/424, train_loss: 0.3216\n",
      "146/424, train_loss: 0.4841\n",
      "147/424, train_loss: 0.5454\n",
      "148/424, train_loss: 0.9371\n",
      "149/424, train_loss: 0.7260\n",
      "150/424, train_loss: 0.6031\n",
      "151/424, train_loss: 0.3773\n",
      "152/424, train_loss: 0.6907\n",
      "153/424, train_loss: 0.3877\n",
      "154/424, train_loss: 0.4595\n",
      "155/424, train_loss: 0.4492\n",
      "156/424, train_loss: 0.5156\n",
      "157/424, train_loss: 0.5409\n",
      "158/424, train_loss: 0.4012\n",
      "159/424, train_loss: 0.3700\n",
      "160/424, train_loss: 0.6647\n",
      "161/424, train_loss: 0.8150\n",
      "162/424, train_loss: 1.0909\n",
      "163/424, train_loss: 0.3794\n",
      "164/424, train_loss: 1.1000\n",
      "165/424, train_loss: 1.0659\n",
      "166/424, train_loss: 0.9268\n",
      "167/424, train_loss: 0.4081\n",
      "168/424, train_loss: 0.4479\n",
      "169/424, train_loss: 0.5428\n",
      "170/424, train_loss: 0.4212\n",
      "171/424, train_loss: 0.5307\n",
      "172/424, train_loss: 0.5276\n",
      "173/424, train_loss: 0.4476\n",
      "174/424, train_loss: 0.8724\n",
      "175/424, train_loss: 0.4877\n",
      "176/424, train_loss: 0.4005\n",
      "177/424, train_loss: 0.5449\n",
      "178/424, train_loss: 0.5695\n",
      "179/424, train_loss: 0.4371\n",
      "180/424, train_loss: 0.3095\n",
      "181/424, train_loss: 0.8909\n",
      "182/424, train_loss: 0.8467\n",
      "183/424, train_loss: 1.0679\n",
      "184/424, train_loss: 0.4729\n",
      "185/424, train_loss: 0.5930\n",
      "186/424, train_loss: 0.7294\n",
      "187/424, train_loss: 0.7802\n",
      "188/424, train_loss: 0.8241\n",
      "189/424, train_loss: 0.6957\n",
      "190/424, train_loss: 0.4538\n",
      "191/424, train_loss: 0.5339\n",
      "192/424, train_loss: 0.3927\n",
      "193/424, train_loss: 0.4948\n",
      "194/424, train_loss: 0.4905\n",
      "195/424, train_loss: 1.0698\n",
      "196/424, train_loss: 0.8125\n",
      "197/424, train_loss: 0.8522\n",
      "198/424, train_loss: 1.1487\n",
      "199/424, train_loss: 0.7578\n",
      "200/424, train_loss: 0.3747\n",
      "201/424, train_loss: 0.5515\n",
      "202/424, train_loss: 0.5606\n",
      "203/424, train_loss: 0.9483\n",
      "204/424, train_loss: 0.6641\n",
      "205/424, train_loss: 0.5210\n",
      "206/424, train_loss: 0.4225\n",
      "207/424, train_loss: 0.8126\n",
      "208/424, train_loss: 0.3546\n",
      "209/424, train_loss: 0.4908\n",
      "210/424, train_loss: 0.6177\n",
      "211/424, train_loss: 0.9188\n",
      "212/424, train_loss: 0.5377\n",
      "213/424, train_loss: 0.9899\n",
      "214/424, train_loss: 0.5015\n",
      "215/424, train_loss: 0.2816\n",
      "216/424, train_loss: 0.5923\n",
      "217/424, train_loss: 0.7583\n",
      "218/424, train_loss: 0.4288\n",
      "219/424, train_loss: 0.7099\n",
      "220/424, train_loss: 0.6453\n",
      "221/424, train_loss: 0.7153\n",
      "222/424, train_loss: 0.3715\n",
      "223/424, train_loss: 0.6416\n",
      "224/424, train_loss: 0.6480\n",
      "225/424, train_loss: 0.4872\n",
      "226/424, train_loss: 0.4650\n",
      "227/424, train_loss: 0.5158\n",
      "228/424, train_loss: 0.4642\n",
      "229/424, train_loss: 0.8110\n",
      "230/424, train_loss: 0.6186\n",
      "231/424, train_loss: 0.6974\n",
      "232/424, train_loss: 0.4471\n",
      "233/424, train_loss: 0.6094\n",
      "234/424, train_loss: 0.8139\n",
      "235/424, train_loss: 0.7522\n",
      "236/424, train_loss: 0.3058\n",
      "237/424, train_loss: 1.5784\n",
      "238/424, train_loss: 0.7255\n",
      "239/424, train_loss: 0.8802\n",
      "240/424, train_loss: 0.7997\n",
      "241/424, train_loss: 1.1643\n",
      "242/424, train_loss: 1.0486\n",
      "243/424, train_loss: 0.4442\n",
      "244/424, train_loss: 0.8287\n",
      "245/424, train_loss: 0.5303\n",
      "246/424, train_loss: 0.4511\n",
      "247/424, train_loss: 0.5033\n",
      "248/424, train_loss: 0.7794\n",
      "249/424, train_loss: 0.6633\n",
      "250/424, train_loss: 0.8060\n",
      "251/424, train_loss: 0.9402\n",
      "252/424, train_loss: 0.7010\n",
      "253/424, train_loss: 0.6516\n",
      "254/424, train_loss: 0.8148\n",
      "255/424, train_loss: 0.3713\n",
      "256/424, train_loss: 0.5477\n",
      "257/424, train_loss: 0.9322\n",
      "258/424, train_loss: 0.4952\n",
      "259/424, train_loss: 0.6138\n",
      "260/424, train_loss: 0.4922\n",
      "261/424, train_loss: 0.9341\n",
      "262/424, train_loss: 0.4839\n",
      "263/424, train_loss: 0.8561\n",
      "264/424, train_loss: 0.6920\n",
      "265/424, train_loss: 0.5956\n",
      "266/424, train_loss: 0.5055\n",
      "267/424, train_loss: 0.5390\n",
      "268/424, train_loss: 0.5751\n",
      "269/424, train_loss: 0.7533\n",
      "270/424, train_loss: 0.8002\n",
      "271/424, train_loss: 0.5737\n",
      "272/424, train_loss: 0.6624\n",
      "273/424, train_loss: 1.0002\n",
      "274/424, train_loss: 0.6194\n",
      "275/424, train_loss: 0.4346\n",
      "276/424, train_loss: 0.4684\n",
      "277/424, train_loss: 0.3602\n",
      "278/424, train_loss: 0.7590\n",
      "279/424, train_loss: 0.5773\n",
      "280/424, train_loss: 0.5028\n",
      "281/424, train_loss: 0.3547\n",
      "282/424, train_loss: 0.9537\n",
      "283/424, train_loss: 0.5371\n",
      "284/424, train_loss: 0.6819\n",
      "285/424, train_loss: 0.5234\n",
      "286/424, train_loss: 0.9933\n",
      "287/424, train_loss: 0.7838\n",
      "288/424, train_loss: 0.4949\n",
      "289/424, train_loss: 0.4699\n",
      "290/424, train_loss: 0.4269\n",
      "291/424, train_loss: 0.5998\n",
      "292/424, train_loss: 0.5391\n",
      "293/424, train_loss: 0.9482\n",
      "294/424, train_loss: 0.4264\n",
      "295/424, train_loss: 0.8986\n",
      "296/424, train_loss: 0.6512\n",
      "297/424, train_loss: 0.5589\n",
      "298/424, train_loss: 0.5897\n",
      "299/424, train_loss: 1.0107\n",
      "300/424, train_loss: 0.4505\n",
      "301/424, train_loss: 0.5577\n",
      "302/424, train_loss: 0.6370\n",
      "303/424, train_loss: 0.6202\n",
      "304/424, train_loss: 0.5494\n",
      "305/424, train_loss: 0.5995\n",
      "306/424, train_loss: 0.5858\n",
      "307/424, train_loss: 0.5861\n",
      "308/424, train_loss: 0.5349\n",
      "309/424, train_loss: 0.7962\n",
      "310/424, train_loss: 0.7208\n",
      "311/424, train_loss: 0.6782\n",
      "312/424, train_loss: 0.7209\n",
      "313/424, train_loss: 0.9047\n",
      "314/424, train_loss: 0.3359\n",
      "315/424, train_loss: 0.5637\n",
      "316/424, train_loss: 0.4898\n",
      "317/424, train_loss: 0.6297\n",
      "318/424, train_loss: 0.6973\n",
      "319/424, train_loss: 0.6252\n",
      "320/424, train_loss: 0.3014\n",
      "321/424, train_loss: 0.5344\n",
      "322/424, train_loss: 0.8113\n",
      "323/424, train_loss: 0.4766\n",
      "324/424, train_loss: 0.5386\n",
      "325/424, train_loss: 0.6777\n",
      "326/424, train_loss: 0.3409\n",
      "327/424, train_loss: 0.3351\n",
      "328/424, train_loss: 0.4128\n",
      "329/424, train_loss: 0.5244\n",
      "330/424, train_loss: 0.5866\n",
      "331/424, train_loss: 0.5917\n",
      "332/424, train_loss: 1.0292\n",
      "333/424, train_loss: 0.6891\n",
      "334/424, train_loss: 0.3314\n",
      "335/424, train_loss: 0.9126\n",
      "336/424, train_loss: 0.9265\n",
      "337/424, train_loss: 0.5931\n",
      "338/424, train_loss: 0.6620\n",
      "339/424, train_loss: 0.7511\n",
      "340/424, train_loss: 0.3240\n",
      "341/424, train_loss: 0.6270\n",
      "342/424, train_loss: 1.0723\n",
      "343/424, train_loss: 0.5848\n",
      "344/424, train_loss: 0.5134\n",
      "345/424, train_loss: 0.5800\n",
      "346/424, train_loss: 0.6400\n",
      "347/424, train_loss: 0.4793\n",
      "348/424, train_loss: 0.8303\n",
      "349/424, train_loss: 0.6693\n",
      "350/424, train_loss: 0.5336\n",
      "351/424, train_loss: 0.5955\n",
      "352/424, train_loss: 0.4080\n",
      "353/424, train_loss: 0.6406\n",
      "354/424, train_loss: 0.4141\n",
      "355/424, train_loss: 0.6902\n",
      "356/424, train_loss: 0.9775\n",
      "357/424, train_loss: 0.5479\n",
      "358/424, train_loss: 0.5151\n",
      "359/424, train_loss: 0.7677\n",
      "360/424, train_loss: 0.3952\n",
      "361/424, train_loss: 0.3675\n",
      "362/424, train_loss: 0.6702\n",
      "363/424, train_loss: 0.6830\n",
      "364/424, train_loss: 0.7800\n",
      "365/424, train_loss: 1.1780\n",
      "366/424, train_loss: 0.4739\n",
      "367/424, train_loss: 0.8330\n",
      "368/424, train_loss: 0.5040\n",
      "369/424, train_loss: 0.5665\n",
      "370/424, train_loss: 0.4448\n",
      "371/424, train_loss: 0.7549\n",
      "372/424, train_loss: 0.3943\n",
      "373/424, train_loss: 0.5350\n",
      "374/424, train_loss: 0.3589\n",
      "375/424, train_loss: 0.4349\n",
      "376/424, train_loss: 0.2883\n",
      "377/424, train_loss: 0.7776\n",
      "378/424, train_loss: 0.9600\n",
      "379/424, train_loss: 0.4114\n",
      "380/424, train_loss: 0.6247\n",
      "381/424, train_loss: 0.5969\n",
      "382/424, train_loss: 0.7473\n",
      "383/424, train_loss: 0.2967\n",
      "384/424, train_loss: 0.5110\n",
      "385/424, train_loss: 0.4789\n",
      "386/424, train_loss: 0.4644\n",
      "387/424, train_loss: 0.5991\n",
      "388/424, train_loss: 0.9346\n",
      "389/424, train_loss: 0.6200\n",
      "390/424, train_loss: 0.4416\n",
      "391/424, train_loss: 0.3959\n",
      "392/424, train_loss: 0.4948\n",
      "393/424, train_loss: 0.8546\n",
      "394/424, train_loss: 0.6900\n",
      "395/424, train_loss: 0.5142\n",
      "396/424, train_loss: 0.4892\n",
      "397/424, train_loss: 0.5550\n",
      "398/424, train_loss: 1.1106\n",
      "399/424, train_loss: 0.5375\n",
      "400/424, train_loss: 0.6237\n",
      "401/424, train_loss: 0.6180\n",
      "402/424, train_loss: 0.8739\n",
      "403/424, train_loss: 0.6227\n",
      "404/424, train_loss: 0.5673\n",
      "405/424, train_loss: 0.4438\n",
      "406/424, train_loss: 0.7701\n",
      "407/424, train_loss: 0.8254\n",
      "408/424, train_loss: 0.5801\n",
      "409/424, train_loss: 0.6143\n",
      "410/424, train_loss: 0.4365\n",
      "411/424, train_loss: 0.5215\n",
      "412/424, train_loss: 0.5041\n",
      "413/424, train_loss: 0.6574\n",
      "414/424, train_loss: 0.8149\n",
      "415/424, train_loss: 0.9480\n",
      "416/424, train_loss: 0.6024\n",
      "417/424, train_loss: 0.9185\n",
      "418/424, train_loss: 0.8614\n",
      "419/424, train_loss: 0.6012\n",
      "420/424, train_loss: 0.4888\n",
      "421/424, train_loss: 0.8221\n",
      "422/424, train_loss: 0.6643\n",
      "423/424, train_loss: 0.5175\n",
      "424/424, train_loss: 0.4850\n",
      "425/424, train_loss: 0.6624\n",
      "epoch 10 average loss: 0.6347\n",
      "Epoch time duration: 566.2762863636017\n",
      "current epoch: 10 current accuracy: 0.5251 best accuracy: 0.5251 at epoch 10\n",
      "----------\n",
      "epoch 11\n",
      "1/424, train_loss: 0.7486\n",
      "2/424, train_loss: 0.5806\n",
      "3/424, train_loss: 0.6861\n",
      "4/424, train_loss: 0.8054\n",
      "5/424, train_loss: 0.4746\n",
      "6/424, train_loss: 0.9490\n",
      "7/424, train_loss: 1.0164\n",
      "8/424, train_loss: 0.6567\n",
      "9/424, train_loss: 0.5539\n",
      "10/424, train_loss: 0.8002\n",
      "11/424, train_loss: 0.7738\n",
      "12/424, train_loss: 0.5262\n",
      "13/424, train_loss: 0.5370\n",
      "14/424, train_loss: 0.4424\n",
      "15/424, train_loss: 0.7421\n",
      "16/424, train_loss: 0.4772\n",
      "17/424, train_loss: 0.4337\n",
      "18/424, train_loss: 0.7482\n",
      "19/424, train_loss: 0.4350\n",
      "20/424, train_loss: 0.3720\n",
      "21/424, train_loss: 0.4883\n",
      "22/424, train_loss: 0.9143\n",
      "23/424, train_loss: 0.3551\n",
      "24/424, train_loss: 0.8728\n",
      "25/424, train_loss: 0.8382\n",
      "26/424, train_loss: 0.5764\n",
      "27/424, train_loss: 0.5141\n",
      "28/424, train_loss: 0.4325\n",
      "29/424, train_loss: 0.4589\n",
      "30/424, train_loss: 0.5315\n",
      "31/424, train_loss: 0.3631\n",
      "32/424, train_loss: 0.7391\n",
      "33/424, train_loss: 0.5646\n",
      "34/424, train_loss: 0.8435\n",
      "35/424, train_loss: 0.4566\n",
      "36/424, train_loss: 0.6053\n",
      "37/424, train_loss: 0.5268\n",
      "38/424, train_loss: 0.3643\n",
      "39/424, train_loss: 0.9797\n",
      "40/424, train_loss: 0.4879\n",
      "41/424, train_loss: 0.3035\n",
      "42/424, train_loss: 0.4824\n",
      "43/424, train_loss: 0.4218\n",
      "44/424, train_loss: 0.7531\n",
      "45/424, train_loss: 0.8174\n",
      "46/424, train_loss: 0.7762\n",
      "47/424, train_loss: 0.6723\n",
      "48/424, train_loss: 0.4565\n",
      "49/424, train_loss: 0.4580\n",
      "50/424, train_loss: 0.7594\n",
      "51/424, train_loss: 0.3718\n",
      "52/424, train_loss: 0.6562\n",
      "53/424, train_loss: 0.3459\n",
      "54/424, train_loss: 0.9249\n",
      "55/424, train_loss: 0.7079\n",
      "56/424, train_loss: 0.5316\n",
      "57/424, train_loss: 0.7265\n",
      "58/424, train_loss: 0.5899\n",
      "59/424, train_loss: 0.8272\n",
      "60/424, train_loss: 0.6731\n",
      "61/424, train_loss: 0.7710\n",
      "62/424, train_loss: 0.2958\n",
      "63/424, train_loss: 0.2214\n",
      "64/424, train_loss: 0.4808\n",
      "65/424, train_loss: 0.4362\n",
      "66/424, train_loss: 0.3377\n",
      "67/424, train_loss: 0.7507\n",
      "68/424, train_loss: 0.7731\n",
      "69/424, train_loss: 0.6516\n",
      "70/424, train_loss: 0.5709\n",
      "71/424, train_loss: 0.3222\n",
      "72/424, train_loss: 0.6901\n",
      "73/424, train_loss: 0.7336\n",
      "74/424, train_loss: 0.6393\n",
      "75/424, train_loss: 0.3805\n",
      "76/424, train_loss: 0.6974\n",
      "77/424, train_loss: 1.0526\n",
      "78/424, train_loss: 0.4788\n",
      "79/424, train_loss: 0.4592\n",
      "80/424, train_loss: 1.1077\n",
      "81/424, train_loss: 0.5041\n",
      "82/424, train_loss: 0.6180\n",
      "83/424, train_loss: 0.6577\n",
      "84/424, train_loss: 0.6539\n",
      "85/424, train_loss: 0.7390\n",
      "86/424, train_loss: 0.6847\n",
      "87/424, train_loss: 0.6044\n",
      "88/424, train_loss: 0.4546\n",
      "89/424, train_loss: 0.7673\n",
      "90/424, train_loss: 0.5967\n",
      "91/424, train_loss: 0.7371\n",
      "92/424, train_loss: 0.3675\n",
      "93/424, train_loss: 0.5693\n",
      "94/424, train_loss: 0.5139\n",
      "95/424, train_loss: 0.5916\n",
      "96/424, train_loss: 0.4691\n",
      "97/424, train_loss: 0.4489\n",
      "98/424, train_loss: 0.4304\n",
      "99/424, train_loss: 0.4918\n",
      "100/424, train_loss: 0.4232\n",
      "101/424, train_loss: 0.5350\n",
      "102/424, train_loss: 0.5354\n",
      "103/424, train_loss: 0.9420\n",
      "104/424, train_loss: 0.9086\n",
      "105/424, train_loss: 0.5545\n",
      "106/424, train_loss: 0.6006\n",
      "107/424, train_loss: 0.3735\n",
      "108/424, train_loss: 0.7343\n",
      "109/424, train_loss: 0.7732\n",
      "110/424, train_loss: 0.7042\n",
      "111/424, train_loss: 0.6981\n",
      "112/424, train_loss: 0.7384\n",
      "113/424, train_loss: 0.6762\n",
      "114/424, train_loss: 0.5036\n",
      "115/424, train_loss: 0.5018\n",
      "116/424, train_loss: 0.3648\n",
      "117/424, train_loss: 0.4932\n",
      "118/424, train_loss: 0.3783\n",
      "119/424, train_loss: 0.6331\n",
      "120/424, train_loss: 0.5615\n",
      "121/424, train_loss: 0.4766\n",
      "122/424, train_loss: 1.0846\n",
      "123/424, train_loss: 0.4828\n",
      "124/424, train_loss: 0.9561\n",
      "125/424, train_loss: 0.4330\n",
      "126/424, train_loss: 0.6018\n",
      "127/424, train_loss: 0.5183\n",
      "128/424, train_loss: 0.4996\n",
      "129/424, train_loss: 0.5349\n",
      "130/424, train_loss: 0.3110\n",
      "131/424, train_loss: 0.3175\n",
      "132/424, train_loss: 0.8056\n",
      "133/424, train_loss: 0.8333\n",
      "134/424, train_loss: 0.7209\n",
      "135/424, train_loss: 0.7338\n",
      "136/424, train_loss: 0.5019\n",
      "137/424, train_loss: 0.9908\n",
      "138/424, train_loss: 0.5285\n",
      "139/424, train_loss: 0.6335\n",
      "140/424, train_loss: 0.5255\n",
      "141/424, train_loss: 0.8176\n",
      "142/424, train_loss: 1.1528\n",
      "143/424, train_loss: 0.8067\n",
      "144/424, train_loss: 0.7001\n",
      "145/424, train_loss: 1.0268\n",
      "146/424, train_loss: 0.5662\n",
      "147/424, train_loss: 0.3258\n",
      "148/424, train_loss: 0.7453\n",
      "149/424, train_loss: 0.4142\n",
      "150/424, train_loss: 0.5791\n",
      "151/424, train_loss: 0.6185\n",
      "152/424, train_loss: 1.1049\n",
      "153/424, train_loss: 0.3844\n",
      "154/424, train_loss: 0.9052\n",
      "155/424, train_loss: 0.5085\n",
      "156/424, train_loss: 0.5084\n",
      "157/424, train_loss: 0.4056\n",
      "158/424, train_loss: 0.5429\n",
      "159/424, train_loss: 0.4406\n",
      "160/424, train_loss: 0.6657\n",
      "161/424, train_loss: 0.4703\n",
      "162/424, train_loss: 0.6081\n",
      "163/424, train_loss: 0.4709\n",
      "164/424, train_loss: 0.4847\n",
      "165/424, train_loss: 0.6347\n",
      "166/424, train_loss: 0.5526\n",
      "167/424, train_loss: 0.4668\n",
      "168/424, train_loss: 0.4897\n",
      "169/424, train_loss: 0.6703\n",
      "170/424, train_loss: 0.6067\n",
      "171/424, train_loss: 1.1302\n",
      "172/424, train_loss: 1.0524\n",
      "173/424, train_loss: 0.4138\n",
      "174/424, train_loss: 1.0411\n",
      "175/424, train_loss: 0.6265\n",
      "176/424, train_loss: 1.0314\n",
      "177/424, train_loss: 0.7781\n",
      "178/424, train_loss: 0.6858\n",
      "179/424, train_loss: 0.5121\n",
      "180/424, train_loss: 0.8228\n",
      "181/424, train_loss: 1.0604\n",
      "182/424, train_loss: 0.6010\n",
      "183/424, train_loss: 0.4385\n",
      "184/424, train_loss: 0.4236\n",
      "185/424, train_loss: 0.2777\n",
      "186/424, train_loss: 0.8038\n",
      "187/424, train_loss: 0.6119\n",
      "188/424, train_loss: 0.4608\n",
      "189/424, train_loss: 0.5042\n",
      "190/424, train_loss: 0.3825\n",
      "191/424, train_loss: 0.3468\n",
      "192/424, train_loss: 0.6397\n",
      "193/424, train_loss: 0.4068\n",
      "194/424, train_loss: 0.5886\n",
      "195/424, train_loss: 0.5668\n",
      "196/424, train_loss: 0.4695\n",
      "197/424, train_loss: 0.5897\n",
      "198/424, train_loss: 0.4725\n",
      "199/424, train_loss: 0.7266\n",
      "200/424, train_loss: 0.4034\n",
      "201/424, train_loss: 0.6019\n",
      "202/424, train_loss: 0.4611\n",
      "203/424, train_loss: 0.5259\n",
      "204/424, train_loss: 0.5247\n",
      "205/424, train_loss: 0.5793\n",
      "206/424, train_loss: 0.5140\n",
      "207/424, train_loss: 0.5703\n",
      "208/424, train_loss: 0.5506\n",
      "209/424, train_loss: 0.8047\n",
      "210/424, train_loss: 0.4285\n",
      "211/424, train_loss: 0.4067\n",
      "212/424, train_loss: 0.9739\n",
      "213/424, train_loss: 0.9558\n",
      "214/424, train_loss: 0.6878\n",
      "215/424, train_loss: 1.0271\n",
      "216/424, train_loss: 0.5503\n",
      "217/424, train_loss: 0.7967\n",
      "218/424, train_loss: 0.5556\n",
      "219/424, train_loss: 0.5380\n",
      "220/424, train_loss: 0.5879\n",
      "221/424, train_loss: 0.3741\n",
      "222/424, train_loss: 0.5417\n",
      "223/424, train_loss: 0.6070\n",
      "224/424, train_loss: 0.4541\n",
      "225/424, train_loss: 0.8194\n",
      "226/424, train_loss: 0.9879\n",
      "227/424, train_loss: 0.9468\n",
      "228/424, train_loss: 0.4677\n",
      "229/424, train_loss: 0.6638\n",
      "230/424, train_loss: 1.0342\n",
      "231/424, train_loss: 0.7128\n",
      "232/424, train_loss: 1.2292\n",
      "233/424, train_loss: 0.3758\n",
      "234/424, train_loss: 0.4615\n",
      "235/424, train_loss: 0.3222\n",
      "236/424, train_loss: 0.4592\n",
      "237/424, train_loss: 0.4543\n",
      "238/424, train_loss: 0.4156\n",
      "239/424, train_loss: 0.5621\n",
      "240/424, train_loss: 0.5591\n",
      "241/424, train_loss: 0.7006\n",
      "242/424, train_loss: 0.6509\n",
      "243/424, train_loss: 0.7014\n",
      "244/424, train_loss: 0.4949\n",
      "245/424, train_loss: 0.5397\n",
      "246/424, train_loss: 0.7478\n",
      "247/424, train_loss: 0.6611\n",
      "248/424, train_loss: 0.6600\n",
      "249/424, train_loss: 0.5559\n",
      "250/424, train_loss: 0.4777\n",
      "251/424, train_loss: 0.6196\n",
      "252/424, train_loss: 0.6052\n",
      "253/424, train_loss: 0.4377\n",
      "254/424, train_loss: 0.7105\n",
      "255/424, train_loss: 0.5493\n",
      "256/424, train_loss: 0.7132\n",
      "257/424, train_loss: 1.1281\n",
      "258/424, train_loss: 0.6199\n",
      "259/424, train_loss: 0.7642\n",
      "260/424, train_loss: 0.5353\n",
      "261/424, train_loss: 0.5221\n",
      "262/424, train_loss: 0.5404\n",
      "263/424, train_loss: 0.4526\n",
      "264/424, train_loss: 0.5507\n",
      "265/424, train_loss: 0.4949\n",
      "266/424, train_loss: 0.4951\n",
      "267/424, train_loss: 0.6453\n",
      "268/424, train_loss: 0.5921\n",
      "269/424, train_loss: 0.5359\n",
      "270/424, train_loss: 0.6663\n",
      "271/424, train_loss: 0.8428\n",
      "272/424, train_loss: 0.5447\n",
      "273/424, train_loss: 0.3660\n",
      "274/424, train_loss: 1.1425\n",
      "275/424, train_loss: 0.6623\n",
      "276/424, train_loss: 0.4033\n",
      "277/424, train_loss: 0.5319\n",
      "278/424, train_loss: 0.3803\n",
      "279/424, train_loss: 0.9028\n",
      "280/424, train_loss: 0.2635\n",
      "281/424, train_loss: 0.6353\n",
      "282/424, train_loss: 0.8066\n",
      "283/424, train_loss: 0.6537\n",
      "284/424, train_loss: 0.4551\n",
      "285/424, train_loss: 0.7184\n",
      "286/424, train_loss: 0.3781\n",
      "287/424, train_loss: 0.5934\n",
      "288/424, train_loss: 0.4840\n",
      "289/424, train_loss: 0.5975\n",
      "290/424, train_loss: 0.3860\n",
      "291/424, train_loss: 0.5681\n",
      "292/424, train_loss: 0.6507\n",
      "293/424, train_loss: 0.8376\n",
      "294/424, train_loss: 0.5894\n",
      "295/424, train_loss: 0.4540\n",
      "296/424, train_loss: 0.3317\n",
      "297/424, train_loss: 0.2901\n"
     ]
    }
   ],
   "source": [
    "!python ../Autism-3D-CNN-brain-sMRI/train_medicalnet.py 'JustBrain_Data/ABIDE_COMBINED' 'Preprocessed_Data/ABIDE_COMBINED' './outputs/Resnet50/ABIDE_Combined' '../Autism-3D-CNN-brain-sMRI/resnet_training/resnet_50.pth'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2993877a-8db0-45f1-a64b-f949df2eb724",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Resume training from saved model\n",
    "#!python ../Autism-3D-CNN-brain-sMRI/train_medicalnet.py 'JustBrain_Data/ABIDE_COMBINED' 'Preprocessed_Data/ABIDE_COMBINED' './outputs/Resnet50/ABIDE_Combined' 'outputs/Resnet50/ABIDE_Combined/checkpoint_1.pth' --resume"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8654e748-5af5-4e97-8e72-94e4925eed18",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"outputs/Resnet50/ABIDE_Combined/training.log\", \"r\") as infile, open(\"outputs/Resnet50/ABIDE_Combined/filtered.log\", \"w\") as outfile:\n",
    "    for line in infile:\n",
    "        if line.startswith(\"epoch \") and \"average loss\" in line:\n",
    "            outfile.write(line)\n",
    "        #elif line.startswith(\"Epoch time duration:\"):\n",
    "            #outfile.write(line)\n",
    "        elif line.startswith(\"current epoch:\"):\n",
    "            outfile.write(line)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1343affc-7c63-48a6-9312-159e93768c1d",
   "metadata": {},
   "source": [
    "# ABIDEII"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9d8499e-064d-401b-bb7c-215be375d59c",
   "metadata": {},
   "source": [
    "## Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "75547e36-4142-43eb-89aa-296adb7b1861",
   "metadata": {},
   "outputs": [],
   "source": [
    "participants_tsv = pd.read_csv('JustBrain_Data/ABIDE_COMBINED/participants.tsv', sep=\"\\t\", dtype=str)\n",
    "participants_tsv.rename(columns={\"participant_id\" : \"SUB_ID\"}, inplace=True)\n",
    "participants_tsv = participants_tsv[participants_tsv.dataset == 'test']\n",
    "participants_tsv.to_csv('outputs/Resnet50/ABIDE_Combined/test/subjects.csv', index = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d3dae81f-48e7-4cea-8b32-32cdec2fca5e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/sfs/gpfs/tardis/home/ejh2wy/Autism-3D-CNN-brain-sMRI/resnet2.py:174: FutureWarning: `nn.init.kaiming_normal` is now deprecated in favor of `nn.init.kaiming_normal_`.\n",
      "  m.weight = nn.init.kaiming_normal(m.weight, mode='fan_out')\n",
      "/sfs/gpfs/tardis/home/ejh2wy/CHMCorr_Autism_Research/../Autism-3D-CNN-brain-sMRI/predict_medicalnet_subids.py:104: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  pretrain = torch.load(pretrain_path)\n",
      "/home/ejh2wy/.local/lib/python3.11/site-packages/torchio/data/image.py:248: UserWarning: Using TorchIO images without a torchio.SubjectsLoader in PyTorch >= 2.3 might have unexpected consequences, e.g., the collated batches will be instances of torchio.Subject with 5D images. Replace your PyTorch DataLoader with a torchio.SubjectsLoader so that the collated batch becomes a dictionary, as expected. See https://github.com/TorchIO-project/torchio/issues/1179 for more context about this issue.\n",
      "  warnings.warn(message, stacklevel=1)\n",
      "evaluation metric: 0.48695652173913045\n"
     ]
    }
   ],
   "source": [
    "# Predictions - 18 epochs (best accuracy) - overfit - predicts 0 for everything\n",
    "!python ../Autism-3D-CNN-brain-sMRI/predict_medicalnet_subids.py 'Preprocessed_Data/ABIDE_COMBINED/test' 'outputs/Resnet50/ABIDE_Combined/test/subjects.csv' './outputs/Resnet50/ABIDE_Combined/checkpoint_18.pth' './outputs/Resnet50/ABIDE_Combined/test'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "2ab0961b-f52b-4b88-a5c9-8f5a470a31cc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/sfs/gpfs/tardis/home/ejh2wy/Autism-3D-CNN-brain-sMRI/resnet2.py:174: FutureWarning: `nn.init.kaiming_normal` is now deprecated in favor of `nn.init.kaiming_normal_`.\n",
      "  m.weight = nn.init.kaiming_normal(m.weight, mode='fan_out')\n",
      "/sfs/gpfs/tardis/home/ejh2wy/CHMCorr_Autism_Research/../Autism-3D-CNN-brain-sMRI/predict_medicalnet_subids.py:104: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  pretrain = torch.load(pretrain_path)\n",
      "/home/ejh2wy/.local/lib/python3.11/site-packages/torchio/data/image.py:248: UserWarning: Using TorchIO images without a torchio.SubjectsLoader in PyTorch >= 2.3 might have unexpected consequences, e.g., the collated batches will be instances of torchio.Subject with 5D images. Replace your PyTorch DataLoader with a torchio.SubjectsLoader so that the collated batch becomes a dictionary, as expected. See https://github.com/TorchIO-project/torchio/issues/1179 for more context about this issue.\n",
      "  warnings.warn(message, stacklevel=1)\n",
      "evaluation metric: 0.5130434782608696\n"
     ]
    }
   ],
   "source": [
    "# Predictions - 12 epochs (second best accuracy) - predicts 1 for everything\n",
    "!python ../Autism-3D-CNN-brain-sMRI/predict_medicalnet_subids.py 'Preprocessed_Data/ABIDE_COMBINED/test' 'outputs/Resnet50/ABIDE_Combined/test/subjects.csv' './outputs/Resnet50/ABIDE_Combined/checkpoint_12.pth' './outputs/Resnet50/ABIDE_Combined/test'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "852ab040-6f42-42ae-bdde-43f6b43257c0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/sfs/gpfs/tardis/home/ejh2wy/Autism-3D-CNN-brain-sMRI/resnet2.py:174: FutureWarning: `nn.init.kaiming_normal` is now deprecated in favor of `nn.init.kaiming_normal_`.\n",
      "  m.weight = nn.init.kaiming_normal(m.weight, mode='fan_out')\n",
      "/sfs/gpfs/tardis/home/ejh2wy/CHMCorr_Autism_Research/../Autism-3D-CNN-brain-sMRI/predict_medicalnet_subids.py:104: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  pretrain = torch.load(pretrain_path)\n",
      "/home/ejh2wy/.local/lib/python3.11/site-packages/torchio/data/image.py:248: UserWarning: Using TorchIO images without a torchio.SubjectsLoader in PyTorch >= 2.3 might have unexpected consequences, e.g., the collated batches will be instances of torchio.Subject with 5D images. Replace your PyTorch DataLoader with a torchio.SubjectsLoader so that the collated batch becomes a dictionary, as expected. See https://github.com/TorchIO-project/torchio/issues/1179 for more context about this issue.\n",
      "  warnings.warn(message, stacklevel=1)\n",
      "evaluation metric: 0.5130434782608696\n"
     ]
    }
   ],
   "source": [
    "# Predictions - 10 epochs (third best accuracy) - predicts 1 for everything\n",
    "!python ../Autism-3D-CNN-brain-sMRI/predict_medicalnet_subids.py 'Preprocessed_Data/ABIDE_COMBINED/test' 'outputs/Resnet50/ABIDE_Combined/test/subjects.csv' './outputs/Resnet50/ABIDE_Combined/checkpoint_10.pth' './outputs/Resnet50/ABIDE_Combined/test'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "6ca9061b-7f4e-4c50-b93d-b546bc2aeed5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/sfs/gpfs/tardis/home/ejh2wy/Autism-3D-CNN-brain-sMRI/resnet2.py:174: FutureWarning: `nn.init.kaiming_normal` is now deprecated in favor of `nn.init.kaiming_normal_`.\n",
      "  m.weight = nn.init.kaiming_normal(m.weight, mode='fan_out')\n",
      "/sfs/gpfs/tardis/home/ejh2wy/CHMCorr_Autism_Research/../Autism-3D-CNN-brain-sMRI/predict_medicalnet_subids.py:104: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  pretrain = torch.load(pretrain_path)\n",
      "/home/ejh2wy/.local/lib/python3.11/site-packages/torchio/data/image.py:248: UserWarning: Using TorchIO images without a torchio.SubjectsLoader in PyTorch >= 2.3 might have unexpected consequences, e.g., the collated batches will be instances of torchio.Subject with 5D images. Replace your PyTorch DataLoader with a torchio.SubjectsLoader so that the collated batch becomes a dictionary, as expected. See https://github.com/TorchIO-project/torchio/issues/1179 for more context about this issue.\n",
      "  warnings.warn(message, stacklevel=1)\n",
      "evaluation metric: 0.5130434782608696\n"
     ]
    }
   ],
   "source": [
    "# Predictions - 6 epochs (fourth best accuracy)\n",
    "!python ../Autism-3D-CNN-brain-sMRI/predict_medicalnet_subids.py 'Preprocessed_Data/ABIDE_COMBINED/test' 'outputs/Resnet50/ABIDE_Combined/test/subjects.csv' './outputs/Resnet50/ABIDE_Combined/checkpoint_6.pth' './outputs/Resnet50/ABIDE_Combined/test'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a820afc-d2d9-4766-9d87-742fee1f72d4",
   "metadata": {},
   "source": [
    "## Val"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "dc3ec65f-8e20-4e35-9ac4-9620001b4941",
   "metadata": {},
   "outputs": [],
   "source": [
    "participants_tsv = pd.read_csv('JustBrain_Data/ABIDE_COMBINED/participants.tsv', sep=\"\\t\", dtype=str)\n",
    "participants_tsv.rename(columns={\"participant_id\" : \"SUB_ID\"}, inplace=True)\n",
    "participants_tsv = participants_tsv[participants_tsv.dataset == 'val']\n",
    "participants_tsv.to_csv('outputs/Resnet50/ABIDE_Combined/validation/subjects.csv', index = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af0cb6d4-8eb4-4906-a70c-ea24d2a72576",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Predictions - 36 epochs (predictions all 0) - overfit\n",
    "!python ../Autism-3D-CNN-brain-sMRI/predict_medicalnet_subids.py 'Preprocessed_Data/ABIDE_COMBINED/val' 'outputs/Resnet50/ABIDE_Combined/validation/subjects.csv' './outputs/Resnet50/ABIDE_Combined/checkpoint_36.pth' './outputs/Resnet50/ABIDE_Combined/validation'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eaf8787f-2a50-4e27-a83e-c1ec01db9291",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Predictions - 12 epochs\n",
    "!python ../Autism-3D-CNN-brain-sMRI/predict_medicalnet_subids.py 'Preprocessed_Data/ABIDE_COMBINED/val' 'outputs/Resnet50/ABIDE_Combined/validation/subjects.csv' './outputs/Resnet50/ABIDE_Combined/checkpoint_12.pth' './outputs/Resnet50/ABIDE_Combined/validation'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e75b94b-163d-49a2-ac9e-1760de5b7051",
   "metadata": {},
   "source": [
    "## Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2267488-6795-4724-92d7-c5ea9d648c22",
   "metadata": {},
   "outputs": [],
   "source": [
    "participants_tsv = pd.read_csv('JustBrain_Data/ABIDE_COMBINED/participants.tsv', sep=\"\\t\", dtype=str)\n",
    "participants_tsv.rename(columns={\"participant_id\" : \"SUB_ID\"}, inplace=True)\n",
    "participants_tsv = participants_tsv[participants_tsv.dataset == 'train']\n",
    "participants_tsv.to_csv('outputs/Resnet50/ABIDE_Combined/train/subjects.csv', index = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "7d5ce33b-b322-4708-ba77-397ecce85708",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/sfs/gpfs/tardis/home/ejh2wy/Autism-3D-CNN-brain-sMRI/resnet2.py:174: FutureWarning: `nn.init.kaiming_normal` is now deprecated in favor of `nn.init.kaiming_normal_`.\n",
      "  m.weight = nn.init.kaiming_normal(m.weight, mode='fan_out')\n",
      "/sfs/gpfs/tardis/home/ejh2wy/CHMCorr_Autism_Research/../Autism-3D-CNN-brain-sMRI/predict_medicalnet_subids.py:104: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  pretrain = torch.load(pretrain_path)\n",
      "/home/ejh2wy/.local/lib/python3.11/site-packages/torchio/data/image.py:248: UserWarning: Using TorchIO images without a torchio.SubjectsLoader in PyTorch >= 2.3 might have unexpected consequences, e.g., the collated batches will be instances of torchio.Subject with 5D images. Replace your PyTorch DataLoader with a torchio.SubjectsLoader so that the collated batch becomes a dictionary, as expected. See https://github.com/TorchIO-project/torchio/issues/1179 for more context about this issue.\n",
      "  warnings.warn(message, stacklevel=1)\n",
      "evaluation metric: 0.5294464075382803\n"
     ]
    }
   ],
   "source": [
    "# Predictions - 36 epochs (predictions all 0) - overfit\n",
    "!python ../Autism-3D-CNN-brain-sMRI/predict_medicalnet_subids.py 'Preprocessed_Data/ABIDE_COMBINED/train' 'outputs/Resnet50/ABIDE_Combined/train/subjects.csv' './outputs/Resnet50/ABIDE_Combined/checkpoint_36.pth' './outputs/Resnet50/ABIDE_Combined/train'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8cb5a5a5-e742-46eb-bdd9-33da80889328",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Predictions - 12 epochs\n",
    "!python ../Autism-3D-CNN-brain-sMRI/predict_medicalnet_subids.py 'Preprocessed_Data/ABIDE_COMBINED/train' 'outputs/Resnet50/ABIDE_Combined/train/subjects.csv' './outputs/Resnet50/ABIDE_Combined/checkpoint_12.pth' './outputs/Resnet50/ABIDE_Combined/train'"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (pretrainedresnet2)",
   "language": "python",
   "name": "pretrainedresnet2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
