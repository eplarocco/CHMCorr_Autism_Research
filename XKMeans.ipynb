{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "145818e9-c533-40e4-86e0-0cd6275ef841",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Forked repo and deleted 2 lines in cut_finder.pyx so it would work: #if CYTHON_USE_PYLONG_INTERNALS\n",
    "  #include \"longintrepr.h\"\n",
    "#!pip install git+https://username:password@github.com/eplarocco/ExKMC.git\n",
    "\n",
    "#!pip install graphviz\n",
    "\n",
    "# Clear Trash and Kernels - disk space needed\n",
    "#!jupyter kernelspec uninstall chmenv1\n",
    "#!rm -r -f /home/ejh2wy/.local/share/Trash/files/*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b54035d5-8f44-4a03-b09c-bd1a1e9bf06f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from pathlib import Path\n",
    "import numpy as np\n",
    "import nibabel as nib\n",
    "from ExKMC.Tree import Tree\n",
    "\n",
    "train_dir = Path.cwd() / \"data/train\"\n",
    "test_dir = Path.cwd() / \"data/test\"\n",
    "val_dir = Path.cwd() / \"data/val\"\n",
    "train_n = len([f for f in os.listdir(train_dir) if os.path.isfile(os.path.join(train_dir, f))])\n",
    "test_n = len([f for f in os.listdir(test_dir) if os.path.isfile(os.path.join(test_dir, f))])\n",
    "val_n = len([f for f in os.listdir(val_dir) if os.path.isfile(os.path.join(val_dir, f))])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ed74e206-7b86-457b-937d-4aaf0890bec5",
   "metadata": {},
   "outputs": [
    {
     "ename": "IndexError",
     "evalue": "index 745 is out of bounds for axis 0 with size 745",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[2], line 25\u001b[0m\n\u001b[1;32m     22\u001b[0m data \u001b[38;5;241m=\u001b[39m img\u001b[38;5;241m.\u001b[39mget_fdata(dtype\u001b[38;5;241m=\u001b[39mnp\u001b[38;5;241m.\u001b[39mfloat32)\u001b[38;5;241m.\u001b[39mflatten()\n\u001b[1;32m     24\u001b[0m \u001b[38;5;66;03m# Store voxel data\u001b[39;00m\n\u001b[0;32m---> 25\u001b[0m \u001b[43mmmapped_array\u001b[49m\u001b[43m[\u001b[49m\u001b[43midx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m:\u001b[49m\u001b[43m]\u001b[49m \u001b[38;5;241m=\u001b[39m data\n\u001b[1;32m     27\u001b[0m \u001b[38;5;66;03m# Store diagnosis ID\u001b[39;00m\n\u001b[1;32m     28\u001b[0m labels_array[idx] \u001b[38;5;241m=\u001b[39m diagnosis_id\n",
      "File \u001b[0;32m~/.local/lib/python3.11/site-packages/numpy/_core/memmap.py:358\u001b[0m, in \u001b[0;36mmemmap.__getitem__\u001b[0;34m(self, index)\u001b[0m\n\u001b[1;32m    357\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21m__getitem__\u001b[39m(\u001b[38;5;28mself\u001b[39m, index):\n\u001b[0;32m--> 358\u001b[0m     res \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[38;5;21;43m__getitem__\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mindex\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    359\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mtype\u001b[39m(res) \u001b[38;5;129;01mis\u001b[39;00m memmap \u001b[38;5;129;01mand\u001b[39;00m res\u001b[38;5;241m.\u001b[39m_mmap \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    360\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m res\u001b[38;5;241m.\u001b[39mview(\u001b[38;5;28mtype\u001b[39m\u001b[38;5;241m=\u001b[39mndarray)\n",
      "\u001b[0;31mIndexError\u001b[0m: index 745 is out of bounds for axis 0 with size 745"
     ]
    }
   ],
   "source": [
    "# Define paths and parameters\n",
    "output_memmap = \"outputs/train_image_1D_list.dat\"\n",
    "output_labels = \"outputs/train_diagnosis_labels.dat\"\n",
    "input_folder = train_dir\n",
    "shape = (train_n, 256 * 256 * 256) #train_n +1 ?\n",
    "\n",
    "# Create a memory-mapped file for voxel data\n",
    "mmapped_array = np.memmap(output_memmap, dtype=np.float32, mode='w+', shape=shape)\n",
    "\n",
    "# Create a memory-mapped file for diagnosis labels\n",
    "labels_array = np.memmap(output_labels, dtype=np.int32, mode='w+', shape=(train_n,))\n",
    "\n",
    "for idx, filename in enumerate(sorted(os.listdir(input_folder))):\n",
    "    if filename.endswith(\".mgz\"):\n",
    "        filepath = os.path.join(input_folder, filename)\n",
    "        \n",
    "        # Extract diagnosis_id from filename\n",
    "        diagnosis_id = int(filename.split('_')[-1].replace('.mgz', ''))  # Convert to int\n",
    "        \n",
    "        # Load .mgz file\n",
    "        img = nib.load(filepath)\n",
    "        data = img.get_fdata(dtype=np.float32).flatten()\n",
    "        \n",
    "        # Store voxel data\n",
    "        mmapped_array[idx, :] = data\n",
    "        \n",
    "        # Store diagnosis ID\n",
    "        labels_array[idx] = diagnosis_id\n",
    "\n",
    "# Flush to disk\n",
    "mmapped_array.flush()\n",
    "labels_array.flush()\n",
    "\n",
    "print(f\"Saved memory-mapped voxel data to {output_memmap}\")\n",
    "print(f\"Saved diagnosis labels to {output_labels}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c2e2e9ae-b55c-4d2f-92bc-37a76e6bd436",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Number of Images in Trian Dataset: 745\n"
     ]
    }
   ],
   "source": [
    "print(f'Total Number of Images in Trian Dataset: {len([f for f in os.listdir(train_dir) if os.path.isfile(os.path.join(train_dir, f))])}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f2b387fa-22b0-442f-8b44-886674c3d8d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reopen the memory-mapped file\n",
    "mmapped_array = np.memmap(\"outputs/train_image_1D_list.dat\", dtype=np.float32, mode=\"r\", shape=shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "79c784a6-4c1a-40b3-b112-411e1aa0228e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/bin/xdg-open: line 862: www-browser: command not found\n",
      "/usr/bin/xdg-open: line 862: links2: command not found\n",
      "/usr/bin/xdg-open: line 862: elinks: command not found\n",
      "/usr/bin/xdg-open: line 862: links: command not found\n",
      "/usr/bin/xdg-open: line 862: lynx: command not found\n",
      "/usr/bin/xdg-open: line 862: w3m: command not found\n",
      "xdg-open: no method available for opening 'outputs/xkmeans_tree.gv.png'\n"
     ]
    }
   ],
   "source": [
    "# Initialize tree with up to 6 leaves, predicting 2 clusters\n",
    "k = 2\n",
    "\n",
    "tree = Tree(k=k, max_leaves=4*k) #didn't change the # of leaves when I updated from 4 to 8 max\n",
    "\n",
    "# Construct the tree, and return cluster labels\n",
    "prediction = tree.fit_predict(mmapped_array) #fit and predict on training set\n",
    "\n",
    "# Tree plot saved to filename\n",
    "tree.plot('outputs/xkmeans_tree')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "0d51f783-9c3a-4629-850e-a1805198480b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved prediction array to outputs/predictions.dat\n"
     ]
    }
   ],
   "source": [
    "# Define output file and shape\n",
    "output_file = \"outputs/predictions.dat\"\n",
    "shape = prediction.shape  # Store original shape\n",
    "dtype = np.int32  # Turn to int\n",
    "\n",
    "# Create a memory-mapped file\n",
    "mmapped_array = np.memmap(output_file, dtype=dtype, mode='w+', shape=shape)\n",
    "\n",
    "# Store data\n",
    "mmapped_array[:] = prediction  # Copy data into memmap\n",
    "\n",
    "# Flush to disk\n",
    "mmapped_array.flush()\n",
    "\n",
    "print(f\"Saved prediction array to {output_file}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "88ac882a-c351-4e10-81b4-ee40d18de4ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reopen the memory-mapped file\n",
    "predictions = np.memmap(\"outputs/predictions.dat\", dtype=np.int32, mode=\"r\", shape=shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "8c736a1c-8e90-4b98-a58f-02362c936c09",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "memmap([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "        0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "        0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 0, 0,\n",
       "        0, 0, 0, 1, 0, 0, 1, 1, 0, 0, 0, 0, 1, 1, 1, 1, 0, 1, 1, 0, 0, 1,\n",
       "        1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1,\n",
       "        1, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0,\n",
       "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "        0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "        0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "        0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "        1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "        1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0,\n",
       "        0, 0, 1, 1, 0, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1,\n",
       "        0, 0, 1, 1, 1, 1, 0, 0, 1, 0, 0, 0, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1,\n",
       "        0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "        0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "        1, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "        0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
       "       dtype=int32)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "09f20664-4e4d-46b1-a93d-3d963f7338ee",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Updated memmap file with transformed values.\n"
     ]
    }
   ],
   "source": [
    "mmapped_array_labels = np.memmap(\"outputs/train_diagnosis_labels.dat\", dtype=np.int32, mode=\"r+\", shape=(train_n,))\n",
    "\n",
    "# Apply transformation: Change 1s to 0s, 2s to 1s\n",
    "mmapped_array_labels[:] = (mmapped_array_labels == 2).astype(np.int32)  # 1 → 0, 2 → 1\n",
    "\n",
    "# Flush changes to disk\n",
    "mmapped_array_labels.flush()\n",
    "\n",
    "print(\"Updated memmap file with transformed values.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "da0a5047-d5b9-4c4d-a6f6-aa6673b9e9d2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "memmap([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "        1, 1, 1, 1, 1, 0, 1, 0, 1, 0, 1, 1, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0,\n",
       "        1, 1, 0, 0, 1, 0, 1, 1, 0, 1, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0,\n",
       "        0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 0, 0, 0, 0, 1, 1, 0, 0, 0, 1, 1,\n",
       "        0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 1, 1, 0, 1,\n",
       "        0, 1, 1, 1, 0, 1, 0, 0, 1, 1, 1, 0, 0, 0, 1, 1, 0, 1, 1, 0, 0, 1,\n",
       "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1,\n",
       "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "        1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1,\n",
       "        1, 1, 0, 1, 1, 1, 0, 1, 0, 0, 0, 0, 1, 1, 1, 0, 1, 0, 0, 1, 0, 0,\n",
       "        1, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "        1, 0, 1, 1, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1,\n",
       "        1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "        1, 1, 1, 1, 0, 0, 1, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1,\n",
       "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "        1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "        0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 0, 0, 1,\n",
       "        1, 0, 0, 0, 1, 0, 0, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0,\n",
       "        1, 1, 0, 0, 0, 1, 1, 1, 0, 1, 1, 0, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1,\n",
       "        1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 0, 0, 1, 0,\n",
       "        0, 0, 1, 1, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 1, 1, 0, 1, 1,\n",
       "        1, 1, 0, 0, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 1, 0, 1,\n",
       "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "        0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "        1, 1, 1, 1, 0, 0, 0, 0, 0, 1, 0, 1, 1, 1, 0, 0, 1, 0, 0, 1, 0, 0,\n",
       "        0, 0, 1, 0, 1, 1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "        0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1],\n",
       "       dtype=int32)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mmapped_array_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "2a058fe6-5c93-4b7a-b779-367320319ab4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.47516778523489933"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Compare predictions to truth - could it also be 53%? does it depend on the \"label\" - not assigned though\n",
    "# still essentially random guess\n",
    "from sklearn.metrics import accuracy_score\n",
    "accuracy = accuracy_score(mmapped_array_labels, prediction)\n",
    "accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4a04d2c-9581-4ec2-9ddc-a35b8ec69144",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50aac963-affa-45a2-97fd-f7940fcaabb3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d14710e-5460-4e9e-a052-7ae89054768b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9390e2f3-a3d1-48d2-b343-9127fb079db6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b2a7ce4-0cf9-4160-8264-2b5eff5caebc",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d9c9913-c2a9-432e-b0fb-ccdcb6f45c16",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea01c61e-3467-4de8-8c76-891b3fc3fe1a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22dd7591-e7cb-415e-a980-3324c284f0a3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "187f8a17-cf17-4c1b-bbad-00d059500a0c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d9eb4f2-8d28-4790-982a-02871314b4b6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6dd478b-f5f4-4836-a5fd-38d129fe5263",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d3ee0df-209d-49fa-b2d0-186ee49b6935",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f274b2f5-4110-41cf-aaa8-4e5ca0a01e22",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4b66286-1a93-4ba5-a28f-cb926408b177",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "015c5d3a-6843-435c-bbb3-ed03e3e0c5fb",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7716f09c-4ae3-4702-99c9-2797f4bef71d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b15625b-2d61-4955-ab1c-c9c8e8cab0b5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4a784a2-4fe7-4391-8671-8e07f5d5986d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aca35fe5-997b-429d-8ffc-ac04fa1d480d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c40fe935-a88c-4580-90ef-f3b248fb0ae4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import nilearn.image\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import pyarrow.parquet as pq\n",
    "import pyarrow as pa\n",
    "\n",
    "def flatten_images(folder_path, train_test):\n",
    "    # Ensure output directory exists\n",
    "    output_dir = \"outputs\"\n",
    "    os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "    # Define file paths based on train/test\n",
    "    if train_test == 'train':\n",
    "        flattened_images_path = os.path.join(output_dir, \"train_flattened_images.parquet\")\n",
    "        labels_path = os.path.join(output_dir, \"train_labels.parquet\")\n",
    "    elif train_test == 'test':\n",
    "        flattened_images_path = os.path.join(output_dir, \"test_flattened_images.parquet\")\n",
    "        labels_path = os.path.join(output_dir, \"test_labels.parquet\")\n",
    "    else:\n",
    "        print(\"Options for train_test include 'train' or 'test'\")\n",
    "        return\n",
    "\n",
    "    # Initialize Parquet writers\n",
    "    image_writer = None\n",
    "    label_writer = None\n",
    "\n",
    "    # Loop through each file in the folder\n",
    "    for file_name in os.listdir(folder_path):\n",
    "        if file_name.endswith('.mgz'):\n",
    "            # Construct full file path\n",
    "            file_path = os.path.join(folder_path, file_name)\n",
    "\n",
    "            # Load the image\n",
    "            img = nilearn.image.load_img(file_path)\n",
    "\n",
    "            # Get the tensor values\n",
    "            img_data = img.get_fdata()\n",
    "\n",
    "            # Flatten the image data\n",
    "            flattened_data = img_data.flatten().astype(np.float32)  # Reduce memory usage\n",
    "\n",
    "            # Extract the diagnosis_id\n",
    "            diagnosis_id = file_name.split('_')[-1].replace('.mgz', '')\n",
    "\n",
    "            # Convert to Pandas DataFrame\n",
    "            image_df = pd.DataFrame([flattened_data])  # Single-row DataFrame\n",
    "            label_df = pd.DataFrame([diagnosis_id], columns=[\"diagnosis\"])\n",
    "\n",
    "            # Append data to Parquet files\n",
    "            if image_writer is None:\n",
    "                image_writer = pq.ParquetWriter(flattened_images_path, pa.Table.from_pandas(image_df).schema)\n",
    "            image_writer.write_table(pa.Table.from_pandas(image_df))\n",
    "\n",
    "            if label_writer is None:\n",
    "                label_writer = pq.ParquetWriter(labels_path, pa.Table.from_pandas(label_df).schema)\n",
    "            label_writer.write_table(pa.Table.from_pandas(label_df))\n",
    "\n",
    "    # Close Parquet writers\n",
    "    if image_writer:\n",
    "        image_writer.close()\n",
    "    if label_writer:\n",
    "        label_writer.close()\n",
    "\n",
    "    print(f\"Data successfully saved to {flattened_images_path} and {labels_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c9bbc79d-64c2-4cc6-85d2-2f75ae2ff5db",
   "metadata": {},
   "outputs": [
    {
     "ename": "ArrowInvalid",
     "evalue": "Negative buffer resize: -1521843588",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mArrowInvalid\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[2], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# Flatten Test files\u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m \u001b[43mflatten_images\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m./data/test\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mtest\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[1], line 52\u001b[0m, in \u001b[0;36mflatten_images\u001b[0;34m(folder_path, train_test)\u001b[0m\n\u001b[1;32m     50\u001b[0m \u001b[38;5;66;03m# Append data to Parquet files\u001b[39;00m\n\u001b[1;32m     51\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m image_writer \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m---> 52\u001b[0m     image_writer \u001b[38;5;241m=\u001b[39m \u001b[43mpq\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mParquetWriter\u001b[49m\u001b[43m(\u001b[49m\u001b[43mflattened_images_path\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpa\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mTable\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfrom_pandas\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimage_df\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mschema\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     53\u001b[0m image_writer\u001b[38;5;241m.\u001b[39mwrite_table(pa\u001b[38;5;241m.\u001b[39mTable\u001b[38;5;241m.\u001b[39mfrom_pandas(image_df))\n\u001b[1;32m     55\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m label_writer \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "File \u001b[0;32m~/.local/lib/python3.11/site-packages/pyarrow/parquet/core.py:1021\u001b[0m, in \u001b[0;36mParquetWriter.__init__\u001b[0;34m(self, where, schema, filesystem, flavor, version, use_dictionary, compression, write_statistics, use_deprecated_int96_timestamps, compression_level, use_byte_stream_split, column_encoding, writer_engine_version, data_page_version, use_compliant_nested_type, encryption_properties, write_batch_size, dictionary_pagesize_limit, store_schema, write_page_index, write_page_checksum, sorting_columns, store_decimal_as_integer, **options)\u001b[0m\n\u001b[1;32m   1019\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_metadata_collector \u001b[38;5;241m=\u001b[39m options\u001b[38;5;241m.\u001b[39mpop(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmetadata_collector\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m)\n\u001b[1;32m   1020\u001b[0m engine_version \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mV2\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[0;32m-> 1021\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mwriter \u001b[38;5;241m=\u001b[39m \u001b[43m_parquet\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mParquetWriter\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1022\u001b[0m \u001b[43m    \u001b[49m\u001b[43msink\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mschema\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1023\u001b[0m \u001b[43m    \u001b[49m\u001b[43mversion\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mversion\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1024\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcompression\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcompression\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1025\u001b[0m \u001b[43m    \u001b[49m\u001b[43muse_dictionary\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43muse_dictionary\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1026\u001b[0m \u001b[43m    \u001b[49m\u001b[43mwrite_statistics\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mwrite_statistics\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1027\u001b[0m \u001b[43m    \u001b[49m\u001b[43muse_deprecated_int96_timestamps\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43muse_deprecated_int96_timestamps\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1028\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcompression_level\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcompression_level\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1029\u001b[0m \u001b[43m    \u001b[49m\u001b[43muse_byte_stream_split\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43muse_byte_stream_split\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1030\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcolumn_encoding\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcolumn_encoding\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1031\u001b[0m \u001b[43m    \u001b[49m\u001b[43mwriter_engine_version\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mengine_version\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1032\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdata_page_version\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdata_page_version\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1033\u001b[0m \u001b[43m    \u001b[49m\u001b[43muse_compliant_nested_type\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43muse_compliant_nested_type\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1034\u001b[0m \u001b[43m    \u001b[49m\u001b[43mencryption_properties\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mencryption_properties\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1035\u001b[0m \u001b[43m    \u001b[49m\u001b[43mwrite_batch_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mwrite_batch_size\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1036\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdictionary_pagesize_limit\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdictionary_pagesize_limit\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1037\u001b[0m \u001b[43m    \u001b[49m\u001b[43mstore_schema\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstore_schema\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1038\u001b[0m \u001b[43m    \u001b[49m\u001b[43mwrite_page_index\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mwrite_page_index\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1039\u001b[0m \u001b[43m    \u001b[49m\u001b[43mwrite_page_checksum\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mwrite_page_checksum\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1040\u001b[0m \u001b[43m    \u001b[49m\u001b[43msorting_columns\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msorting_columns\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1041\u001b[0m \u001b[43m    \u001b[49m\u001b[43mstore_decimal_as_integer\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstore_decimal_as_integer\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1042\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1043\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mis_open \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n",
      "File \u001b[0;32m~/.local/lib/python3.11/site-packages/pyarrow/_parquet.pyx:2206\u001b[0m, in \u001b[0;36mpyarrow._parquet.ParquetWriter.__cinit__\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32m~/.local/lib/python3.11/site-packages/pyarrow/error.pxi:155\u001b[0m, in \u001b[0;36mpyarrow.lib.pyarrow_internal_check_status\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32m~/.local/lib/python3.11/site-packages/pyarrow/error.pxi:92\u001b[0m, in \u001b[0;36mpyarrow.lib.check_status\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;31mArrowInvalid\u001b[0m: Negative buffer resize: -1521843588"
     ]
    }
   ],
   "source": [
    "# Flatten Test files\n",
    "flatten_images(\"./data/test\", 'test')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc0473b3-b000-4619-9e92-af4bf800b4f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Flatten Train files\n",
    "flatten_images(\"./data/train\", 'train')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3082a534-c795-435b-a525-2584171418a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load Train flattened images\n",
    "train_flattened_images = pd.read_parquet(\"outputs/train_flattened_images.parquet\")\n",
    "train_diagnosis_ids = pd.read_parquet(\"outputs/train_labels.parquet\")[\"diagnosis\"].tolist()\n",
    "\n",
    "print(f\"Train: Loaded {len(train_flattened_images)} images with shape {train_flattened_images.shape[1]} each\")\n",
    "print(f\"Train: Loaded {len(train_diagnosis_ids)} labels\")\n",
    "\n",
    "# Load Test flattened images\n",
    "test_flattened_images = pd.read_parquet(\"outputs/test_flattened_images.parquet\")\n",
    "test_diagnosis_ids = pd.read_parquet(\"outputs/test_labels.parquet\")[\"diagnosis\"].tolist()\n",
    "\n",
    "print(f\"Test: Loaded {len(test_flattened_images)} images with shape {test_flattened_images.shape[1]} each\")\n",
    "print(f\"Test: Loaded {len(test_diagnosis_ids)} labels\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "658481f9-499f-4d60-9be8-4d47b943894c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import nilearn.image\n",
    "import numpy as np\n",
    "import json\n",
    "\n",
    "from ExKMC.Tree import Tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2795458-1a88-48e9-a0db-d28d20b3c6ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "def flatten_images(folder_path, train_test):\n",
    "    # Ensure output directory exists\n",
    "    output_dir = \"outputs\"\n",
    "    os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "    if train_test == 'train':\n",
    "        # File paths\n",
    "        flattened_images_path = os.path.join(output_dir, \"train_flattened_images.txt\")\n",
    "        labels_path = os.path.join(output_dir, \"train_labels.txt\")\n",
    "    elif train_test == 'test':\n",
    "        # File paths\n",
    "        flattened_images_path = os.path.join(output_dir, \"test_flattened_images.txt\")\n",
    "        labels_path = os.path.join(output_dir, \"test_labels.txt\")\n",
    "    else:\n",
    "        print(\"options for train_test include 'train' or 'test'\")\n",
    "    \n",
    "    # Initialize empty files with opening brackets for JSON format\n",
    "    with open(flattened_images_path, \"w\") as img_file, open(labels_path, \"w\") as label_file:\n",
    "        img_file.write(\"[\\n\")  # Start list for images\n",
    "        label_file.write(\"[\\n\")  # Start list for labels\n",
    "    \n",
    "    first_entry = True  # To manage commas correctly\n",
    "    \n",
    "    # Loop through each file in the folder\n",
    "    for file_name in os.listdir(folder_path):\n",
    "        if file_name.endswith('.mgz'):\n",
    "            # Construct full file path\n",
    "            file_path = os.path.join(folder_path, file_name)\n",
    "    \n",
    "            # Load the image\n",
    "            img = nilearn.image.load_img(file_path)\n",
    "    \n",
    "            # Get the tensor values\n",
    "            img_data = img.get_fdata()\n",
    "    \n",
    "            # Flatten the image data\n",
    "            flattened_data = img_data.flatten().tolist()  # Convert NumPy array to list\n",
    "    \n",
    "            # Extract the diagnosis_id\n",
    "            diagnosis_id = file_name.split('_')[-1].replace('.mgz', '')\n",
    "    \n",
    "            # Append the data to files in JSON format\n",
    "            with open(flattened_images_path, \"a\") as img_file, open(labels_path, \"a\") as label_file:\n",
    "                if not first_entry:\n",
    "                    img_file.write(\",\\n\")  # Add comma before new list\n",
    "                    label_file.write(\",\\n\")  # Add comma before new label\n",
    "                json.dump(flattened_data, img_file)  # Save image as a JSON list\n",
    "                json.dump(diagnosis_id, label_file)  # Save label as JSON string\n",
    "                first_entry = False  # After first entry, ensure commas are added\n",
    "    \n",
    "    # Close the lists properly\n",
    "    with open(flattened_images_path, \"a\") as img_file, open(labels_path, \"a\") as label_file:\n",
    "        img_file.write(\"\\n]\")  # Close list for images\n",
    "        label_file.write(\"\\n]\")  # Close list for labels\n",
    "    \n",
    "    return print(\"Data successfully saved to outputs\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Flatten Train files\n",
    "folder_path = \"./data/train\"\n",
    "flatten_images(folder_path, 'train')\n",
    "\n",
    "# Flatten Test files\n",
    "folder_path = \"./data/test\"\n",
    "flatten_images(folder_path, 'test')\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Load Train flattened images\n",
    "with open(\"outputs/train_flattened_images.txt\", \"r\") as f:\n",
    "    train_flattened_images = json.load(f)\n",
    "\n",
    "# Load labels\n",
    "with open(\"outputs/train_labels.txt\", \"r\") as f:\n",
    "    train_diagnosis_ids = json.load(f)\n",
    "\n",
    "print(f\"Train: Loaded {len(train_flattened_images)} images with shape {len(train_flattened_images[0])} each\")\n",
    "print(f\"Train: Loaded {len(train_diagnosis_ids)} labels\")\n",
    "\n",
    "# Load Test flattened images\n",
    "with open(\"outputs/test_flattened_images.txt\", \"r\") as f:\n",
    "    test_flattened_images = json.load(f)\n",
    "\n",
    "# Load labels\n",
    "with open(\"outputs/test_labels.txt\", \"r\") as f:\n",
    "    test_diagnosis_ids = json.load(f)\n",
    "\n",
    "print(f\"Test: Loaded {len(test_flattened_images)} images with shape {len(test_flattened_images[0])} each\")\n",
    "print(f\"Test: Loaded {len(test_diagnosis_ids)} labels\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "553e0431-547e-4ad6-9571-26a5ad83d406",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Flatten Train files\n",
    "folder_path = \"./data/train\"\n",
    "flatten_images(folder_path, 'train')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0d89b3c-9d1f-49c4-b49d-5c3aa02b5272",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Flatten Test files\n",
    "folder_path = \"./data/test\"\n",
    "flatten_images(folder_path, 'test')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d75c4a13-13cf-4c07-b88b-582e27e868b9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 2 images with shape 16777216 each\n",
      "Loaded 2 labels\n"
     ]
    }
   ],
   "source": [
    "# Load Train flattened images\n",
    "with open(\"outputs/train_flattened_images.txt\", \"r\") as f:\n",
    "    train_flattened_images = json.load(f)\n",
    "\n",
    "# Load labels\n",
    "with open(\"outputs/train_labels.txt\", \"r\") as f:\n",
    "    train_diagnosis_ids = json.load(f)\n",
    "\n",
    "print(f\"Train: Loaded {len(train_flattened_images)} images with shape {len(train_flattened_images[0])} each\")\n",
    "print(f\"Train: Loaded {len(train_diagnosis_ids)} labels\")\n",
    "\n",
    "# Load Test flattened images\n",
    "with open(\"outputs/test_flattened_images.txt\", \"r\") as f:\n",
    "    test_flattened_images = json.load(f)\n",
    "\n",
    "# Load labels\n",
    "with open(\"outputs/test_labels.txt\", \"r\") as f:\n",
    "    test_diagnosis_ids = json.load(f)\n",
    "\n",
    "print(f\"Test: Loaded {len(test_flattened_images)} images with shape {len(test_flattened_images[0])} each\")\n",
    "print(f\"Test: Loaded {len(test_diagnosis_ids)} labels\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "541eec6d-7279-4ab9-bf16-205052c2e745",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train Tree (PCA first????) Seems terrifying without but would defeat the explinability part"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e028b820-299a-4eed-9e38-5eae97617e0b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/bin/xdg-open: line 862: www-browser: command not found\n",
      "/usr/bin/xdg-open: line 862: links2: command not found\n",
      "/usr/bin/xdg-open: line 862: elinks: command not found\n",
      "/usr/bin/xdg-open: line 862: links: command not found\n",
      "/usr/bin/xdg-open: line 862: lynx: command not found\n",
      "/usr/bin/xdg-open: line 862: w3m: command not found\n",
      "xdg-open: no method available for opening 'filename.gv.png'\n"
     ]
    }
   ],
   "source": [
    "# Initialize tree with up to 6 leaves, predicting 3 clusters\n",
    "tree = Tree(k=k, max_leaves=2*k) \n",
    "\n",
    "# Construct the tree, and return cluster labels\n",
    "prediction = tree.fit_predict(flattened_images) #predict on training set\n",
    "\n",
    "# Tree plot saved to filename\n",
    "tree.plot('outputs/xkmeans_tree')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "74596395-b7ee-424d-b86a-d9f450dfc1d0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0., 1., 0., 2., 2., 0., 0., 1., 2., 2., 0., 0., 0., 2., 2., 0., 0.,\n",
       "       1., 0., 2., 0., 0., 0., 0., 2., 0., 1., 1., 2., 1., 1., 2., 2., 2.,\n",
       "       0., 1., 0., 2., 0., 2., 2., 2., 0., 1., 2., 2., 2., 0., 2., 0., 1.,\n",
       "       1., 2., 1., 1., 1., 0., 2., 2., 2., 2., 2., 1., 0., 0., 0., 1., 2.,\n",
       "       0., 0., 0., 1., 2., 1., 2., 1., 1., 1., 0., 1., 1., 1., 1., 2., 1.,\n",
       "       1., 1., 2., 0., 1., 2., 0., 0., 1., 1., 2., 2., 1., 1., 0.])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prediction[10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1316d62-294b-4c35-9e60-544b92c27a03",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"outputs/predictions.txt\", \"w\") as file:\n",
    "    file.write(prediction)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53579fa2-3121-416e-af5c-d2fdfb4877a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Compare predictions to truth\n",
    "from sklearn.metrics import accuracy_score\n",
    "accuracy = accuracy_score(diagnosis_ids, prediction)\n",
    "accuracy"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (xkmeans2)",
   "language": "python",
   "name": "xkmeans2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
